# Using the RAG Engine with Fine-Tuned Qwen

This document explains how to use `rag_engine_finetuned.py`, which runs the same RAG pipeline as `rag_engine.py` but uses your **fine-tuned Qwen** model for the final answer generation step.

---

## What stays the same

- **Step 1 – Query routing:** The default LLM (e.g. GPT-4o from `db_connection`) still analyzes the user question and splits it into sub-queries.
- **Step 2 – Retrieval & reranking:** Neo4j vector search and the Cross-Encoder reranker are unchanged. Embeddings still use the existing embedding model.
- **Few-shot examples:** The same QA dataset and similarity-based few-shot selection are used.

Only the **final answer generation** (Step 3) is done by the fine-tuned Qwen model instead of the default LLM.

---

## Prerequisites

1. **Fine-tuned Qwen** saved under a run folder, for example:
   - `drive/MyDrive/aicompliance/outputs/qwen-0.5b-aicompliance/<run_name>/`
   - The folder must contain at least:
     - `adapter_config.json`
     - `adapter_model.safetensors` (or other adapter weights)
   - Optionally: `tokenizer_config.json` (if not present, the base Qwen tokenizer is used).

2. **Same environment as the base RAG:**
   - Neo4j (with your graph and chunk index)
   - `.env` with `NEO4J_URI`, `NEO4J_USERNAME`, `NEO4J_PASSWORD`, `OPENAI_API_KEY`
   - Python packages: `transformers`, `peft`, `langchain_*`, `sentence_transformers`, etc.

---

## How to use

### 1. Run from the RAG project directory

From the repo root or from the `rag` folder:

```bash
cd rag
python -m src.rag_engine_finetuned
```

Or from repo root (with `rag` on `PYTHONPATH` or run from `rag`):

```bash
python rag/src/rag_engine_finetuned.py
```

This runs the built-in test: it loads the RAG engine, calls `generate_answer_finetuned_qwen()` with a fixed Korean question, and prints the answer and context.

---

### 2. Use in your own code

```python
import sys
import os
sys.path.insert(0, "path/to/rag")  # or your project root if rag is a package

from src.rag_engine_finetuned import AIComplianceRAG

# Same constructor as rag_engine.py (DB, embeddings, default LLM for routing)
rag = AIComplianceRAG()

# Option A: Answer with fine-tuned Qwen (default path = Colab Drive)
answer, context = rag.generate_answer_finetuned_qwen(
    "유럽에서 채용 면접 분석 AI를 쓰려고 하는데, 고위험 AI에 해당하나요? 위반 시 벌칙은?"
)
print(answer)

# Option B: Point to a specific run folder
answer, context = rag.generate_answer_finetuned_qwen(
    "What are the key obligations for high-risk AI providers?",
    model_dir="drive/MyDrive/aicompliance/outputs/qwen-0.5b-aicompliance/20250225_123456",
    max_new_tokens=512,
    temperature=0.3,
)
print(answer)
```

---

### 3. API summary

| Method | Description |
|--------|-------------|
| `generate_answer(query)` | Same as `rag_engine.py`: full pipeline with the **default LLM** (e.g. GPT-4o) for the final answer. |
| `generate_answer_finetuned_qwen(query, model_dir=..., max_new_tokens=512, temperature=0.3)` | Same pipeline, but the **final answer** is generated by the fine-tuned Qwen. |

**Parameters for `generate_answer_finetuned_qwen`:**

- **`query`** (str): User question (e.g. Korean or English).
- **`model_dir`** (str, optional): Path to the fine-tuned Qwen run folder (or the parent folder containing multiple runs; the latest run is then chosen). Default: `drive/MyDrive/aicompliance/outputs/qwen-0.5b-aicompliance`.
- **`max_new_tokens`** (int, optional): Max tokens to generate. Default: 512.
- **`temperature`** (float, optional): Sampling temperature. Default: 0.3.

**Returns:** `(answer: str, context: str)` — same as `generate_answer()`.

---

### 4. Model path behavior

- If `model_dir` is a **run folder** (contains `adapter_config.json`), that run is used.
- If `model_dir` is a **parent folder** (e.g. `qwen-0.5b-aicompliance`) with several run subfolders, the **latest run** (by `adapter_config.json` modification time) is used.

Examples:

- Use default (Colab):  
  `generate_answer_finetuned_qwen("질문")`
- Use a specific run:  
  `generate_answer_finetuned_qwen("질문", model_dir="outputs/qwen-0.5b-aicompliance/exp_lr5e5")`

---

## File layout

- **`rag/src/rag_engine_finetuned.py`** – RAG engine class + `generate_answer_finetuned_qwen()` and Qwen loading helpers.
- **`rag/USAGE_FINETUNED_RAG.md`** – This usage guide.

All other RAG behavior (routing, retrieval, reranking, few-shot) is identical to `rag_engine.py`; only the final answer is produced by the fine-tuned Qwen.
