"""
RAG engine that uses the fine-tuned Qwen model for final answer generation.

Same as rag_engine.py (routing, retrieval, reranking, few-shot) except the final
answer is generated by the fine-tuned Qwen instead of the default LLM (e.g. GPT-4o).

See USAGE_FINETUNED_RAG.md for how to use this module.
"""

import sys
import os

os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["OBJC_DISABLE_INITIALIZE_FORK_SAFETY"] = "YES"
os.environ["USE_TF"] = "0"
os.environ["USE_TORCH"] = "1"

import json
import numpy as np
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pathlib import Path
from typing import Tuple, List, Optional
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from src.db_connection import GraphDBConnection
from sentence_transformers import CrossEncoder

# Fine-tuned Qwen default path (Colab Drive)
DEFAULT_FINETUNED_QWEN_DIR = "drive/MyDrive/aicompliance/outputs/qwen-0.5b-aicompliance"
BASE_MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"


def _resolve_qwen_run_dir(model_dir: str) -> Path:
    """Return the directory that contains adapter_config.json."""
    p = Path(model_dir).expanduser().resolve()
    if not p.exists():
        return p
    if (p / "adapter_config.json").exists():
        return p
    runs = [d for d in p.iterdir() if d.is_dir()]
    if not runs:
        return p
    latest = max(
        runs,
        key=lambda d: (d / "adapter_config.json").stat().st_mtime if (d / "adapter_config.json").exists() else 0,
    )
    return latest if (latest / "adapter_config.json").exists() else p


def _load_finetuned_qwen(model_dir: str, device_map: str = "auto"):
    """Load base Qwen + PEFT adapters and tokenizer. Returns (model, tokenizer)."""
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import PeftModel

    run_dir = _resolve_qwen_run_dir(model_dir)
    if not (run_dir / "adapter_config.json").exists():
        raise FileNotFoundError(
            f"No adapter found in {run_dir}. Use a run folder with adapter_config.json, or the parent of run folders."
        )

    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        device_map=device_map,
        trust_remote_code=True,
    )
    model = PeftModel.from_pretrained(model, str(run_dir))
    model.eval()

    tokenizer = (
        AutoTokenizer.from_pretrained(str(run_dir), trust_remote_code=True)
        if (run_dir / "tokenizer_config.json").exists()
        else AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True)
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return model, tokenizer


def _generate_with_qwen(
    model,
    tokenizer,
    prompt: str,
    max_new_tokens: int = 512,
    temperature: float = 0.3,
    do_sample: bool = True,
) -> str:
    """Generate completion for a single prompt string."""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        do_sample=do_sample,
        pad_token_id=tokenizer.pad_token_id,
    )
    generated = tokenizer.decode(
        outputs[0][inputs["input_ids"].shape[1] :],
        skip_special_tokens=True,
    )
    return generated.strip()


class AIComplianceRAG:
    """Same as rag_engine.AIComplianceRAG; add generate_answer_finetuned_qwen() for Qwen-backed answers."""

    def __init__(self):
        db_conn = GraphDBConnection()
        self.graph = db_conn.get_graph()
        self.embeddings = db_conn.get_embeddings()
        self.llm = db_conn.get_llm()

        print("⏳ Cross-Encoder 리랭킹 모델 로딩 중...")
        self.reranker = CrossEncoder("Dongjin-kr/ko-reranker")

        self.qa_dataset = self.load_qa_dataset()

    def load_qa_dataset(self) -> List[dict]:
        """JSON 파일로 저장된 50개의 QA 셋을 불러옵니다."""
        file_path = os.path.join(os.path.dirname(__file__), "qa_dataset.json")
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"⚠️ QA 데이터셋을 불러오지 못했습니다. 파일 위치를 확인하세요: {e}")
            return []

    def get_few_shot_examples(self, query: str, k: int = 2) -> str:
        """현재 질문과 가장 비슷한 QA 예시 2개를 찾아냅니다."""
        if not self.qa_dataset:
            return ""

        query_vector = self.embeddings.embed_query(query)
        scored_examples = []

        for qa in self.qa_dataset:
            qa_vector = self.embeddings.embed_query(qa["question"])
            similarity = np.dot(query_vector, qa_vector) / (
                np.linalg.norm(query_vector) * np.linalg.norm(qa_vector)
            )
            scored_examples.append((similarity, qa))

        top_examples = sorted(scored_examples, key=lambda x: x[0], reverse=True)[:k]
        few_shot_text = ""
        for i, (_, qa) in enumerate(top_examples):
            few_shot_text += f"\n[참고 예시 {i+1}]\nQ: {qa['question']}\nA: {qa['answer']}\n"
        return few_shot_text

    def analyze_and_route_query(self, query: str) -> List[str]:
        """💡 [Step 1] 질문의 복잡도를 판단하고 3가지 유형으로 라우팅/분할합니다."""
        router_prompt = ChatPromptTemplate.from_template("""
        당신은 기업의 AI 컴플라이언스 분석가입니다. 사용자의 질문을 분석하여 데이터베이스 검색에 최적화된 하위 질문(Sub-queries) 리스트로 변환하십시오.

        [분할 핵심 지침]
        - 사용자의 질문에 "벌칙", "벌금", "제재", "위반 시", "페널티", "리스크" 등의 단어가 포함되어 있다면, **반드시 그 제재 내용만을 묻는 명확한 하위 질문 1개를 리스트에 추가**해야 합니다. (예: "고위험 AI 시스템 위반 시 부과되는 벌칙과 과징금 규모")

        [분류 및 분할 기준]
        1. 단순 포괄형: "이거 문제 될까?" 형태의 일반적 질문 -> 핵심 키워드만 뽑아 1개의 질문으로 정리.
        2. 단일 심층형: 특정 조항이나 벌금을 깊게 묻는 질문 -> 깊게 파고들 수 있도록 1개의 구체적 질문으로 변환.
        3. 다중 분할형: 규제 대상 여부, 핵심 의무, 벌칙 등을 복합적으로 묻는 긴 질문 -> 2~3개의 개별 질문으로 분할. (예: ["채용 AI의 고위험 여부", "고위험 AI 위반 벌칙 금액"])

        반드시 파이썬 List 형태의 문자열(예: ["질문1", "질문2"])만 출력하십시오. 절대 다른 설명은 붙이지 마십시오.

        사용자 질문: {query}
        """)

        chain = router_prompt | self.llm | StrOutputParser()
        result_str = chain.invoke({"query": query}).strip()

        try:
            sub_queries = eval(result_str)
            if not isinstance(sub_queries, list):
                sub_queries = [query]
        except Exception:
            sub_queries = [query]

        print(f"\n[🔀 질문 라우팅 및 분할 결과]: {sub_queries}")
        return sub_queries

    def translate_query(self, query: str) -> str:
        trans_prompt = ChatPromptTemplate.from_template(
            "Translate the following Korean query into English legal terms (EU AI Act style). "
            "Just output the translated text without any explanation.\nQuery: {query}"
        )
        chain = trans_prompt | self.llm | StrOutputParser()
        return chain.invoke({"query": query})

    def retrieve_and_rerank_context(self, sub_queries: List[str], final_k: int = 5) -> str:
        """[Step 2] 벡터 검색 후 Cross-Encoder로 정밀하게 재정렬(Re-ranking)합니다."""
        cypher_query = """
        CALL db.index.vector.queryNodes('chunk_embedding', 15, $query_vector)
        YIELD node AS chunk, score
        MATCH (parent)-[:HAS_CHUNK]->(chunk)

        OPTIONAL MATCH (parent)-[r1]->(t1)
        WHERE type(r1) <> 'HAS_CHUNK' AND type(r1) <> 'INCLUDES'

        OPTIONAL MATCH (t1)-[r2]->(t2)
        WHERE type(r2) IN ['MANDATED_FOR', 'PERMITS', 'PENALIZES_WITH', 'APPLIES_TO', 'LEADS_TO', 'ENCOMPASSES', 'SUPPLEMENTS']

        WITH chunk, score, parent, r1, t1, r2, t2,
             coalesce(t1.name, t1.level, t1.id, labels(t1)[0]) AS t1_base,
             CASE
                WHEN 'Sanction' IN labels(t1) THEN ' [금액: ' + coalesce(t1['amount'], '명시안됨') + ', 유형: ' + coalesce(t1['type'], '분류안됨') + ', 상세: ' + coalesce(t1['description'], '없음') + ']'
                WHEN 'Requirement' IN labels(t1) OR 'Support' IN labels(t1) THEN ' [상세: ' + coalesce(t1['description'], '없음') + ']'
                WHEN 'TechCriterion' IN labels(t1) OR 'UsageCriterion' IN labels(t1) THEN ' [기준값: ' + coalesce(t1['threshold_value'], '') + ' ' + coalesce(t1['unit'], '') + ']'
                WHEN 'Concept' IN labels(t1) THEN ' [언어: ' + coalesce(t1['lang'], '') + ']'
                ELSE ''
             END AS t1_extra,
             CASE
                WHEN 'Concept' IN labels(t2) THEN ' [언어: ' + coalesce(t2['lang'], '') + ']'
                WHEN 'Requirement' IN labels(t2) OR 'Support' IN labels(t2) OR 'Sanction' IN labels(t2) THEN ' [상세: ' + coalesce(t2['description'], '없음') + ']'
                ELSE ''
             END AS t2_extra

        WITH chunk, score, parent,
             CASE
                WHEN t1 IS NOT NULL AND t2 IS NULL THEN type(r1) + " -> " + t1_base + t1_extra
                WHEN t1 IS NOT NULL AND t2 IS NOT NULL THEN type(r1) + " -> " + t1_base + t1_extra + " => [" + type(r2) + " -> " + coalesce(t2.name, t2.level, t2.id, labels(t2)[0]) + t2_extra + "]"
                ELSE null
             END AS path_info

        RETURN
            chunk.text AS chunk_text,
            labels(parent)[0] AS parent_type,
            coalesce(parent.id, parent.name, 'ID없음') AS parent_id,
            coalesce(parent.title, '') AS parent_title,
            collect(DISTINCT path_info) AS target_info
        """

        unique_chunks = {}
        for sq in sub_queries:
            translated_query = self.translate_query(sq)
            query_vector = self.embeddings.embed_query(translated_query)
            results = self.graph.query(cypher_query, params={"query_vector": query_vector})
            for res in results:
                chunk_text = res["chunk_text"]
                if chunk_text not in unique_chunks:
                    unique_chunks[chunk_text] = res

        if not unique_chunks:
            return "관련된 법적 근거를 찾을 수 없습니다."

        print(f"👉 1차 벡터 검색 완료: 총 {len(unique_chunks)}개의 조항 확보. 리랭킹 진행 중...")
        chunk_list = list(unique_chunks.values())
        pairs = [[sub_queries[0], item["chunk_text"]] for item in chunk_list]
        scores = self.reranker.predict(pairs)
        scored_results = sorted(zip(scores, chunk_list), key=lambda x: x[0], reverse=True)
        top_results = [item for score, item in scored_results[:final_k]]
        print(f"🎯 리랭킹 완료: 가장 적합한 상위 {final_k}개 문서를 추렸습니다.")

        formatted_context = []
        for res in top_results:
            p_type = res["parent_type"]
            p_id = res["parent_id"]
            p_title = res["parent_title"]
            valid_targets = [t for t in res["target_info"] if t]
            t_info = "\n  - " + "\n  - ".join(valid_targets) if valid_targets else "추가 연결된 규제 없음"
            title_str = f" ({p_title})" if p_title else ""
            context_piece = (
                f"--- 출처: [{p_type}] {p_id}{title_str} ---\n"
                f"내용(Chunk): {res['chunk_text']}\n"
                f"관련 구조(Graph): {t_info}\n"
            )
            formatted_context.append(context_piece)
        return "\n".join(formatted_context)

    def generate_answer(self, query: str) -> Tuple[str, str]:
        """[Step 3] Few-Shot이 적용된 최종 답변 생성 (기본 LLM 사용)."""
        sub_queries = self.analyze_and_route_query(query)
        context = self.retrieve_and_rerank_context(sub_queries, final_k=6)
        few_shot_examples = self.get_few_shot_examples(query, k=2)

        prompt = ChatPromptTemplate.from_template("""
        당신은 글로벌 기업의 최고 AI 컴플라이언스 책임자(CCO)입니다.
        아래 제공된 [Context]에 있는 실제 법안 정보만을 엄격하게 바탕으로 사용자의 [Question]에 답변해야 합니다.

        [데이터 분석 필수 지침 🚨]
        - [Context]의 '내용(Chunk)'뿐만 아니라, '관련 구조(Graph)'에 있는 텍스트를 최우선으로 분석하십시오.
        - 특히 Graph 구조 내의 `[상세: ...]`, `[금액: ...]`, `[유형: ...]` 부분에 있는 구체적인 벌금 액수, 제재 내용, 의무 사항을 절대 누락하지 마십시오.
        - 💡 중요: 만약 내용 중에 벌금 액수나 특정 제재 내용이 없다면 "제공된 데이터에 없다"는 식의 표현은 절대 사용하지 마십시오. 대신 **"관련 법률상 구체적인 금액이 명시되어 있지 않습니다"**라고 전문가답게 기재하십시오. ('데이터'라는 단어 사용 엄금)

        [답변 구조 및 포맷 지침 (절대 준수!)]
        당신의 답변은 반드시 아래의 '본문'과 '요약' 2단 구조로 출력되어야 합니다. 시작 부분에 어떤 안내 문구도 넣지 말고 곧바로 본문 내용부터 시작하십시오.

        1. 상세 본문 (줄글 작성)
           - 질문에 대한 종합적이고 구체적인 답변을 1~2개의 문단으로 상세히 작성하십시오.
           - 본문 안에는 1) 해당 AI 시스템의 규제 대상/등급 여부, 2) 사용자(기업)가 시장 출시 전이나 운영 중에 구체적으로 무엇을 준비하고 어떤 절차를 거쳐야 하는지(핵심 의무), 3) 위반 시 겪게 될 구체적인 법적 리스크가 자연스러운 문장으로 모두 녹아있어야 합니다.
           - 사용자의 신뢰성을 높이기 위해, 본문 설명 중 자연스럽게 "유럽 인공지능법(EU AI Act) 제O조에 따르면~"과 같이 법적 근거를 함께 언급하며 설명하십시오.

        ---
        [요약 및 참고]
        - 규제 대상: (짧게 요약)
        - 핵심 의무: (짧게 요약)
        - 위반 리스크: (법률 기준 짧게 요약)
        - 근거 조항: (참조한 문서와 조항 번호)

        [참고 예시 (어투와 논리 전개 방식을 참고하되, 본문은 위 지침에 따라 구체적인 의무를 넣어 작성하십시오)]
        {few_shot_examples}

        [실제 법안 데이터 (Context)]
        {context}

        [사용자 질문 (Question)]
        {query}
        """)

        chain = prompt | self.llm | StrOutputParser()
        answer = chain.invoke({
            "context": context,
            "query": query,
            "few_shot_examples": few_shot_examples if few_shot_examples else "별도 예시 없음.",
        })
        if answer.startswith("답변"):
            answer = answer.split("\n", 1)[-1].strip()
        return answer, context

    # -------------------------------------------------------------------------
    # Fine-tuned Qwen: same pipeline, final generation with local Qwen
    # -------------------------------------------------------------------------

    _qwen_model = None
    _qwen_tokenizer = None

    def _get_qwen(self, model_dir: str):
        """Lazy-load fine-tuned Qwen once per process."""
        if AIComplianceRAG._qwen_model is None:
            print("⏳ Fine-tuned Qwen 로딩 중...")
            AIComplianceRAG._qwen_model, AIComplianceRAG._qwen_tokenizer = _load_finetuned_qwen(model_dir)
            print("✅ Fine-tuned Qwen 로딩 완료.")
        return AIComplianceRAG._qwen_model, AIComplianceRAG._qwen_tokenizer

    def generate_answer_finetuned_qwen(
        self,
        query: str,
        model_dir: str = DEFAULT_FINETUNED_QWEN_DIR,
        max_new_tokens: int = 512,
        temperature: float = 0.3,
    ) -> Tuple[str, str]:
        """
        [Step 3] Few-Shot + 동일 Context를 사용하되, **최종 답변만** fine-tuned Qwen으로 생성합니다.

        - 라우팅·검색·리랭킹·Few-Shot은 기존과 동일 (기본 LLM/임베딩 사용).
        - 답변 생성만 Colab/로컬에 저장한 fine-tuned Qwen으로 수행합니다.
        """
        sub_queries = self.analyze_and_route_query(query)
        context = self.retrieve_and_rerank_context(sub_queries, final_k=6)
        few_shot_examples = self.get_few_shot_examples(query, k=2)

        prompt_body = """
당신은 글로벌 기업의 최고 AI 컴플라이언스 책임자(CCO)입니다.
아래 제공된 [Context]에 있는 실제 법안 정보만을 엄격하게 바탕으로 사용자의 [Question]에 답변해야 합니다.

[데이터 분석 필수 지침 🚨]
- [Context]의 '내용(Chunk)'뿐만 아니라, '관련 구조(Graph)'에 있는 텍스트를 최우선으로 분석하십시오.
- 특히 Graph 구조 내의 `[상세: ...]`, `[금액: ...]`, `[유형: ...]` 부분에 있는 구체적인 벌금 액수, 제재 내용, 의무 사항을 절대 누락하지 마십시오.
- 💡 중요: 만약 내용 중에 벌금 액수나 특정 제재 내용이 없다면 "제공된 데이터에 없다"는 식의 표현은 절대 사용하지 마십시오. 대신 **"관련 법률상 구체적인 금액이 명시되어 있지 않습니다"**라고 전문가답게 기재하십시오. ('데이터'라는 단어 사용 엄금)

[답변 구조 및 포맷 지침 (절대 준수!)]
당신의 답변은 반드시 아래의 '본문'과 '요약' 2단 구조로 출력되어야 합니다. 시작 부분에 어떤 안내 문구도 넣지 말고 곧바로 본문 내용부터 시작하십시오.

1. 상세 본문 (줄글 작성)
   - 질문에 대한 종합적이고 구체적인 답변을 1~2개의 문단으로 상세히 작성하십시오.
   - 본문 안에는 1) 해당 AI 시스템의 규제 대상/등급 여부, 2) 사용자(기업)가 시장 출시 전이나 운영 중에 구체적으로 무엇을 준비하고 어떤 절차를 거쳐야 하는지(핵심 의무), 3) 위반 시 겪게 될 구체적인 법적 리스크가 자연스러운 문장으로 모두 녹아있어야 합니다.
   - 사용자의 신뢰성을 높이기 위해, 본문 설명 중 자연스럽게 "유럽 인공지능법(EU AI Act) 제O조에 따르면~"과 같이 법적 근거를 함께 언급하며 설명하십시오.

---
[요약 및 참고]
- 규제 대상: (짧게 요약)
- 핵심 의무: (짧게 요약)
- 위반 리스크: (법률 기준 짧게 요약)
- 근거 조항: (참조한 문서와 조항 번호)

[참고 예시 (어투와 논리 전개 방식을 참고하되, 본문은 위 지침에 따라 구체적인 의무를 넣어 작성하십시오)]
{few_shot_examples}

[실제 법안 데이터 (Context)]
{context}

[사용자 질문 (Question)]
{query}
""".strip()

        filled = prompt_body.format(
            context=context,
            query=query,
            few_shot_examples=few_shot_examples if few_shot_examples else "별도 예시 없음.",
        )

        # Qwen2.5-Instruct chat format so the model knows it's answering
        chat_prompt = (
            "<|im_start|>system\n"
            "You are an AI compliance expert (CCO). Answer only based on the provided Context.<|im_end|>\n"
            "<|im_start|>user\n"
            + filled
            + "<|im_end|>\n"
            "<|im_start|>assistant\n"
        )

        model, tokenizer = self._get_qwen(model_dir)
        answer = _generate_with_qwen(
            model,
            tokenizer,
            chat_prompt,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=temperature > 0,
        )
        if answer.startswith("답변"):
            answer = answer.split("\n", 1)[-1].strip()
        return answer, context


if __name__ == "__main__":
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

    print("⚙️ AI Compliance RAG (Fine-tuned Qwen 답변) 엔진 가동 중...")
    rag = AIComplianceRAG()

    test_question = "유럽에서 신입사원 채용 면접을 분석하는 AI 시스템을 도입하려고 해. 이거 고위험 AI에 해당해? 그리고 위반하면 어떤 벌칙이 있어?"

    answer, context = rag.generate_answer_finetuned_qwen(test_question)

    print("\n[최종 답변 (Fine-tuned Qwen)]")
    print(answer)
