import sys
import os

# ê²½ë¡œ ì¶”ê°€ ì½”ë“œ: í˜„ì¬ í´ë”ì™€ ë¶€ëª¨ í´ë”ë¥¼ ëª¨ë‘ íŒŒì´ì¬ ê¸¸ì°¾ê¸°ì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json
import pandas as pd
from tqdm import tqdm
from datasets import Dataset
from ragas import evaluate

# 1. ì„í¬íŠ¸
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)

# 2. ê¸€ì ìˆ˜(max_tokens)ë¥¼ ëŠ˜ë¦¬ê¸° ìœ„í•´ ë­ì²´ì¸ ëª¨ë“ˆë§Œ ì¶”ê°€ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€ ë° RAG ì—”ì§„ ë¶ˆëŸ¬ì˜¤ê¸°
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from src.rag_engine import AIComplianceRAG

def run_evaluation():
    print("ğŸ§ª AI Compliance RAG í‰ê°€ íŒŒì´í”„ë¼ì¸ ê°€ë™ ì¤‘...")
    rag_engine = AIComplianceRAG()

    # 3. ì‹¬íŒê´€ ì»¤ìŠ¤í…€
    # í‰ê°€ ë„ì¤‘ ê¸€ìê°€ ì˜ë¦¬ì§€ ì•Šê²Œ max_tokensë¥¼ ë„‰ë„‰í•˜ê²Œ 8192ë¡œ ëŠ˜ë¦½ë‹ˆë‹¤.
    # ì„ë² ë”© ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ëª…ì‹œì ìœ¼ë¡œ ì„ë² ë”© ëª¨ë¸ë„ ì¥ì—¬ì¤ë‹ˆë‹¤.
    print("âš™ï¸ ì‹¬íŒê´€ LLMì˜ ê¸€ì ìˆ˜ ì œí•œì„ í•´ì œí•©ë‹ˆë‹¤...")
    my_llm = ChatOpenAI(model="gpt-4o-mini", max_tokens=8192, temperature=0.0)
    my_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # 1. 10ê°œ QA ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
    qa_path = os.path.join('src', 'qa_dataset.json')
    with open(qa_path, 'r', encoding='utf-8') as f:
        qa_data = json.load(f)

    # [ì¤‘ìš”] ìµœì´ˆ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 3ê°œë§Œ ë¨¼ì € ì‹¤í–‰í•´ ë´…ë‹ˆë‹¤.
    # qa_data = qa_data[:3] 

    print(f"ğŸš€ ì´ {len(qa_data)}ê°œì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì— ëŒ€í•´ ë‹µë³€ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # RAGASê°€ ìš”êµ¬í•˜ëŠ” ë°ì´í„° í˜•ì‹(Dictionary of Lists) ì¤€ë¹„
    data_samples = {
        "question": [],
        "answer": [],
        "contexts": [],
        "ground_truth": []
    }

    # 2. ìš°ë¦¬ RAG ì—”ì§„ìœ¼ë¡œ ë‹µë³€ ì¶”ì¶œ
    for item in tqdm(qa_data, desc="ë‹µë³€ ìƒì„± ì¤‘"):
        question = item['question']
        ground_truth = item['answer']
        
        answer, context_text = rag_engine.generate_answer(question)
        data_samples["question"].append(question)
        data_samples["answer"].append(answer)
        data_samples["contexts"].append([context_text]) 
        data_samples["ground_truth"].append(ground_truth)

    # 3. ë°ì´í„°ì…‹ ë³€í™˜ ë° í‰ê°€ ì‹¤í–‰
    print("\nâš–ï¸ RAGAS ì‹¬íŒê´€ ëª¨ë¸ì´ ì§€í‘œë¥¼ ì±„ì í•˜ê³  ìˆìŠµë‹ˆë‹¤ (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)...")
    dataset = Dataset.from_dict(data_samples)

    # 4. í‰ê°€ ìˆ˜í–‰: ì›ë³¸ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë˜, ë°©ê¸ˆ ë§Œë“  my_llmë§Œ ì˜µì…˜ìœ¼ë¡œ íˆ¬ì…
    score = evaluate(
        dataset,
        metrics=[
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall,
        ],
        llm=my_llm,                 # ê¸€ì ìˆ˜ ë„‰ë„‰í•œ LLM íˆ¬ì…
        embeddings=my_embeddings,   # ì—ëŸ¬ ì•ˆ ë‚˜ëŠ” ì„ë² ë”© íˆ¬ì…
        raise_exceptions=False      # ì¤‘ê°„ ì—ëŸ¬ ë¬´ì‹œ
    )

    # 4. ê²°ê³¼ ì¶œë ¥ ë° ì—‘ì…€(CSV) ì €ì¥
    print("\nğŸ“Š [ìµœì¢… í‰ê°€ í‰ê·  ì ìˆ˜]")
    print(score)

    df_score = score.to_pandas()
    result_filename = "rag_evaluation_results.csv"
    df_score.to_csv(result_filename, index=False, encoding='utf-8-sig')
    print(f"\nâœ… ìƒì„¸ ë¬¸í•­ë³„ í‰ê°€ ê²°ê³¼ê°€ '{result_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    run_evaluation()