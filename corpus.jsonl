{"meta": {"section_type": "article", "article_ref": "Article 6", "node_types": ["Sanction", "Requirement"]}, "text": "[ARTICLE] Article 6\n\nArticle 6\nClassification rules for high-risk AI systems\n1.   Irrespective of whether an AI system is placed on the market or put into service independently of the products referred to in points (a) and (b), that AI system shall be considered to be high-risk where both of the following conditions are fulfilled:\n(a) the AI system is intended to be used as a safety component of a product, or the AI system is itself a product, covered by the Union harmonisation legislation listed in Annex I;\n(b) the product whose safety component pursuant to point (a) is the AI system, or the AI system itself as a product, is required to undergo a third-party conformity assessment, with a view to the placing on the market or the putting into service of that product pursuant to the Union harmonisation legislati\n\nArticle 6\nClassification rules for high-risk AI systems\n1.   Irrespective of whether an AI system is placed on the market or put into service independently of the products referred to in points (a) and (b), that AI system shall be considered to be high-risk where both of the following conditions are fulfilled:\n(a) the AI system is intended to be used as a safety component of a product, or the AI system is itself a product, covered by the Union harmonisation legislation listed in Annex I;\n(b) the product whose safety component pursuant to point (a) is the AI system, or the AI system itself as a product, is required to undergo a third-party conformity assessment, with a view to the placing on the market or the putting into service of that product pursuant to the Union harmonisation legislati"}
{"meta": {"section_type": "article", "article_ref": "Article 7", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 7\n\nArticle 7\nAmendments to Annex III\n1.   The Commission is empowered to adopt delegated acts in accordance with Article 97 to amend Annex III by adding or modifying use-cases of high-risk AI systems where both of the following conditions are fulfilled:\n(a) the AI systems are intended to be used in any of the areas listed in Annex III;\n(b) the AI systems pose a risk of harm to health and safety, or an adverse impact on fundamental rights, and that risk is equivalent to, or greater than, the risk of harm or of adverse impact posed by the high-risk AI systems already referred to in Annex III.\n2.   When assessing the condition under paragraph 1, point (b), the Commission shall take into account the following criteria:\n(a) the intended purpose of the AI system;\n(b) the extent to which an AI syste"}
{"meta": {"section_type": "article", "article_ref": "Article 8", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 8\n\nArticle 8\nCompliance with the requirements\n1.   High-risk AI systems shall comply with the requirements laid down in this Section, taking into account their intended purpose as well as the generally acknowledged state of the art on AI and AI-related technologies. The risk management system referred to in Article 9 shall be taken into account when ensuring compliance with those requirements.\n2.   Where a product contains an AI system, to which the requirements of this Regulation as well as requirements of the Union harmonisation legislation listed in Section A of Annex I apply, providers shall be responsible for ensuring that their product is fully compliant with all applicable requirements under applicable Union harmonisation legislation. In ensuring the compliance of high-risk AI systems"}
{"meta": {"section_type": "article", "article_ref": "Article 9", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 9\n\nArticle 9\nRisk management system\n1.   A risk management system shall be established, implemented, documented and maintained in relation to high-risk AI systems.\n2.   The risk management system shall be understood as a continuous iterative process planned and run throughout the entire lifecycle of a high-risk AI system, requiring regular systematic review and updating. It shall comprise the following steps:\n(a) the identification and analysis of the known and the reasonably foreseeable risks that the high-risk AI system can pose to health, safety or fundamental rights when the high-risk AI system is used in accordance with its intended purpose;\n(b) the estimation and evaluation of the risks that may emerge when the high-risk AI system is used in accordance with its intended purpose, and und"}
{"meta": {"section_type": "article", "article_ref": "Article 10", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 10\n\nArticle 10\nData and data governance\n1.   High-risk AI systems which make use of techniques involving the training of AI models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5 whenever such data sets are used.\n2.   Training, validation and testing data sets shall be subject to data governance and management practices appropriate for the intended purpose of the high-risk AI system. Those practices shall concern in particular:\n(a) the relevant design choices;\n(b) data collection processes and the origin of data, and in the case of personal data, the original purpose of the data collection;\n(c) relevant data-preparation processing operations, such as annotation, labelling, cleaning, updating"}
{"meta": {"section_type": "article", "article_ref": "Article 11", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 11\n\nArticle 11\nTechnical documentation\n1.   The technical documentation of a high-risk AI system shall be drawn up before that system is placed on the market or put into service and shall be kept up-to date.\nThe technical documentation shall be drawn up in such a way as to demonstrate that the high-risk AI system complies with the requirements set out in this Section and to provide national competent authorities and notified bodies with the necessary information in a clear and comprehensive form to assess the compliance of the AI system with those requirements. It shall contain, at a minimum, the elements set out in Annex IV. SMEs, including start-ups, may provide the elements of the technical documentation specified in Annex IV in a simplified manner. To that end, the Commission shall establi"}
{"meta": {"section_type": "article", "article_ref": "Article 12", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 12\n\nArticle 12\nRecord-keeping\n1.   High-risk AI systems shall technically allow for the automatic recording of events (logs) over the lifetime of the system.\n2.   In order to ensure a level of traceability of the functioning of a high-risk AI system that is appropriate to the intended purpose of the system, logging capabilities shall enable the recording of events relevant for:\n(a) identifying situations that may result in the high-risk AI system presenting a risk within the meaning of Article 79(1) or in a substantial modification;\n(b) facilitating the post-market monitoring referred to in Article 72; and\n(c) monitoring the operation of high-risk AI systems referred to in Article 26(5).\n3.   For high-risk AI systems referred to in point 1 (a), of Annex III, the logging capabilities shall prov"}
{"meta": {"section_type": "article", "article_ref": "Article 13", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 13\n\nArticle 13\nTransparency and provision of information to deployers\n1.   High-risk AI systems shall be designed and developed in such a way as to ensure that their operation is sufficiently transparent to enable deployers to interpret a system’s output and use it appropriately. An appropriate type and degree of transparency shall be ensured with a view to achieving compliance with the relevant obligations of the provider and deployer set out in Section 3.\n2.   High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to deployers.\n3.   The instructions for use shall contain at least the following information:\n(a) the identity and"}
{"meta": {"section_type": "article", "article_ref": "Article 14", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 14\n\nArticle 14\nHuman oversight\n1.   High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which they are in use.\n2.   Human oversight shall aim to prevent or minimise the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular where such risks persist despite the application of other requirements set out in this Section.\n3.   The oversight measures shall be commensurate with the risks, level of autonomy and context of use of the high-risk AI system, and shall be ensured through either one or both of"}
{"meta": {"section_type": "article", "article_ref": "Article 15", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 15\n\nArticle 15\nAccuracy, robustness and cybersecurity\n1.   High-risk AI systems shall be designed and developed in such a way that they achieve an appropriate level of accuracy, robustness, and cybersecurity, and that they perform consistently in those respects throughout their lifecycle.\n2.   To address the technical aspects of how to measure the appropriate levels of accuracy and robustness set out in paragraph 1 and any other relevant performance metrics, the Commission shall, in cooperation with relevant stakeholders and organisations such as metrology and benchmarking authorities, encourage, as appropriate, the development of benchmarks and measurement methodologies.\n3.   The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be declared in the accompanying"}
{"meta": {"section_type": "article", "article_ref": "Article 16", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 16\n\nArticle 16\nObligations of providers of high-risk AI systems\nProviders of high-risk AI systems shall:\n(a) ensure that their high-risk AI systems are compliant with the requirements set out in Section 2;\n(b) indicate on the high-risk AI system or, where that is not possible, on its packaging or its accompanying documentation, as applicable, their name, registered trade name or registered trade mark, the address at which they can be contacted;\n(c) have a quality management system in place which complies with Article 17;\n(d) keep the documentation referred to in Article 18;\n(e) when under their control, keep the logs automatically generated by their high-risk AI systems as referred to in Article 19;\n(f) ensure that the high-risk AI system undergoes the relevant conformity assessment procedure"}
{"meta": {"section_type": "article", "article_ref": "Article 17", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 17\n\nArticle 17\nQuality management system\n1.   Providers of high-risk AI systems shall put a quality management system in place that ensures compliance with this Regulation. That system shall be documented in a systematic and orderly manner in the form of written policies, procedures and instructions, and shall include at least the following aspects:\n(a) a strategy for regulatory compliance, including compliance with conformity assessment procedures and procedures for the management of modifications to the high-risk AI system;\n(b) techniques, procedures and systematic actions to be used for the design, design control and design verification of the high-risk AI system;\n(c) techniques, procedures and systematic actions to be used for the development, quality control and quality assurance of the h"}
{"meta": {"section_type": "article", "article_ref": "Article 18", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 18\n\nArticle 18\nDocumentation keeping\n1.   The provider shall, for a period ending 10 years after the high-risk AI system has been placed on the market or put into service, keep at the disposal of the national competent authorities:\n(a) the technical documentation referred to in Article 11;\n(b) the documentation concerning the quality management system referred to in Article 17;\n(c) the documentation concerning the changes approved by notified bodies, where applicable;\n(d) the decisions and other documents issued by the notified bodies, where applicable;\n(e) the EU declaration of conformity referred to in Article 47.\n2.   Each Member State shall determine conditions under which the documentation referred to in paragraph 1 remains at the disposal of the national competent authorities for the per"}
{"meta": {"section_type": "article", "article_ref": "Article 19", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 19\n\nArticle 19\nAutomatically generated logs\n1.   Providers of high-risk AI systems shall keep the logs referred to in Article 12(1), automatically generated by their high-risk AI systems, to the extent such logs are under their control. Without prejudice to applicable Union or national law, the logs shall be kept for a period appropriate to the intended purpose of the high-risk AI system, of at least six months, unless provided otherwise in the applicable Union or national law, in particular in Union law on the protection of personal data.\n2.   Providers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law shall maintain the logs automatically generated by their high-risk AI systems as part of"}
{"meta": {"section_type": "article", "article_ref": "Article 20", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 20\n\nArticle 20\nCorrective actions and duty of information\n1.   Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system that they have placed on the market or put into service is not in conformity with this Regulation shall immediately take the necessary corrective actions to bring that system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They shall inform the distributors of the high-risk AI system concerned and, where applicable, the deployers, the authorised representative and importers accordingly.\n2.   Where the high-risk AI system presents a risk within the meaning of Article 79(1) and the provider becomes aware of that risk, it shall immediately investigate the causes, in collaboration with the reporting de"}
{"meta": {"section_type": "article", "article_ref": "Article 21", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 21\n\nArticle 21\nCooperation with competent authorities\n1.   Providers of high-risk AI systems shall, upon a reasoned request by a competent authority, provide that authority all the information and documentation necessary to demonstrate the conformity of the high-risk AI system with the requirements set out in Section 2, in a language which can be easily understood by the authority in one of the official languages of the institutions of the Union as indicated by the Member State concerned.\n2.   Upon a reasoned request by a competent authority, providers shall also give the requesting competent authority, as applicable, access to the automatically generated logs of the high-risk AI system referred to in Article 12(1), to the extent such logs are under their control.\n3.   Any information obtained"}
{"meta": {"section_type": "article", "article_ref": "Article 22", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 22\n\nArticle 22\nAuthorised representatives of providers of high-risk AI systems\n1.   Prior to making their high-risk AI systems available on the Union market, providers established in third countries shall, by written mandate, appoint an authorised representative which is established in the Union.\n2.   The provider shall enable its authorised representative to perform the tasks specified in the mandate received from the provider.\n3.   The authorised representative shall perform the tasks specified in the mandate received from the provider. It shall provide a copy of the mandate to the market surveillance authorities upon request, in one of the official languages of the institutions of the Union, as indicated by the competent authority. For the purposes of this Regulation, the mandate shall empo"}
{"meta": {"section_type": "article", "article_ref": "Article 23", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 23\n\nArticle 23\nObligations of importers\n1.   Before placing a high-risk AI system on the market, importers shall ensure that the system is in conformity with this Regulation by verifying that:\n(a) the relevant conformity assessment procedure referred to in Article 43 has been carried out by the provider of the high-risk AI system;\n(b) the provider has drawn up the technical documentation in accordance with Article 11 and Annex IV;\n(c) the system bears the required CE marking and is accompanied by the EU declaration of conformity referred to in Article 47 and instructions for use;\n(d) the provider has appointed an authorised representative in accordance with Article 22(1).\n2.   Where an importer has sufficient reason to consider that a high-risk AI system is not in conformity with this Regulati"}
{"meta": {"section_type": "article", "article_ref": "Article 24", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 24\n\nArticle 24\nObligations of distributors\n1.   Before making a high-risk AI system available on the market, distributors shall verify that it bears the required CE marking, that it is accompanied by a copy of the EU declaration of conformity referred to in Article 47 and instructions for use, and that the provider and the importer of that system, as applicable, have complied with their respective obligations as laid down in Article 16, points (b) and (c) and Article 23(3).\n2.   Where a distributor considers or has reason to consider, on the basis of the information in its possession, that a high-risk AI system is not in conformity with the requirements set out in Section 2, it shall not make the high-risk AI system available on the market until the system has been brought into conformity with"}
{"meta": {"section_type": "article", "article_ref": "Article 25", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 25\n\nArticle 25\nResponsibilities along the AI value chain\n1.   Any distributor, importer, deployer or other third-party shall be considered to be a provider of a high-risk AI system for the purposes of this Regulation and shall be subject to the obligations of the provider under Article 16, in any of the following circumstances:\n(a) they put their name or trademark on a high-risk AI system already placed on the market or put into service, without prejudice to contractual arrangements stipulating that the obligations are otherwise allocated;\n(b) they make a substantial modification to a high-risk AI system that has already been placed on the market or has already been put into service in such a way that it remains a high-risk AI system pursuant to Article 6;\n(c) they modify the intended purpose"}
{"meta": {"section_type": "article", "article_ref": "Article 26", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 26\n\nArticle 26\nObligations of deployers of high-risk AI systems\n1.   Deployers of high-risk AI systems shall take appropriate technical and organisational measures to ensure they use such systems in accordance with the instructions for use accompanying the systems, pursuant to paragraphs 3 and 6.\n2.   Deployers shall assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support.\n3.   The obligations set out in paragraphs 1 and 2, are without prejudice to other deployer obligations under Union or national law and to the deployer’s freedom to organise its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider.\n4.   Without prejudice to paragraphs 1 and 2, to the ex"}
{"meta": {"section_type": "article", "article_ref": "Article 27", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 27\n\nArticle 27\nFundamental rights impact assessment for high-risk AI systems\n1.   Prior to deploying a high-risk AI system referred to in Article 6(2), with the exception of high-risk AI systems intended to be used in the area listed in point 2 of Annex III, deployers that are bodies governed by public law, or are private entities providing public services, and deployers of high-risk AI systems referred to in points 5 (b) and (c) of Annex III, shall perform an assessment of the impact on fundamental rights that the use of such system may produce. For that purpose, deployers shall perform an assessment consisting of:\n(a) a description of the deployer’s processes in which the high-risk AI system will be used in line with its intended purpose;\n(b) a description of the period of time within which,"}
{"meta": {"section_type": "article", "article_ref": "Article 31", "node_types": ["Sanction", "Requirement"]}, "text": "[ARTICLE] Article 31\n\nArticle 31\nRequirements relating to notified bodies\n1.   A notified body shall be established under the national law of a Member State and shall have legal personality.\n2.   Notified bodies shall satisfy the organisational, quality management, resources and process requirements that are necessary to fulfil their tasks, as well as suitable cybersecurity requirements.\n3.   The organisational structure, allocation of responsibilities, reporting lines and operation of notified bodies shall ensure confidence in their performance, and in the results of the conformity assessment activities that the notified bodies conduct.\n4.   Notified bodies shall be independent of the provider of a high-risk AI system in relation to which they perform conformity assessment activities. Notified bodies shall als\n\nArticle 31\nRequirements relating to notified bodies\n1.   A notified body shall be established under the national law of a Member State and shall have legal personality.\n2.   Notified bodies shall satisfy the organisational, quality management, resources and process requirements that are necessary to fulfil their tasks, as well as suitable cybersecurity requirements.\n3.   The organisational structure, allocation of responsibilities, reporting lines and operation of notified bodies shall ensure confidence in their performance, and in the results of the conformity assessment activities that the notified bodies conduct.\n4.   Notified bodies shall be independent of the provider of a high-risk AI system in relation to which they perform conformity assessment activities. Notified bodies shall als"}
{"meta": {"section_type": "article", "article_ref": "Article 34", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 34\n\nArticle 34\nOperational obligations of notified bodies\n1.   Notified bodies shall verify the conformity of high-risk AI systems in accordance with the conformity assessment procedures set out in Article 43.\n2.   Notified bodies shall avoid unnecessary burdens for providers when performing their activities, and take due account of the size of the provider, the sector in which it operates, its structure and the degree of complexity of the high-risk AI system concerned, in particular in view of minimising administrative burdens and compliance costs for micro- and small enterprises within the meaning of Recommendation 2003/361/EC. The notified body shall, nevertheless, respect the degree of rigour and the level of protection required for the compliance of the high-risk AI system with the requir"}
{"meta": {"section_type": "article", "article_ref": "Article 38", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 38\n\nArticle 38\nCoordination of notified bodies\n1.   The Commission shall ensure that, with regard to high-risk AI systems, appropriate coordination and cooperation between notified bodies active in the conformity assessment procedures pursuant to this Regulation are put in place and properly operated in the form of a sectoral group of notified bodies.\n2.   Each notifying authority shall ensure that the bodies notified by it participate in the work of a group referred to in paragraph 1, directly or through designated representatives.\n3.   The Commission shall provide for the exchange of knowledge and best practices between notifying authorities."}
{"meta": {"section_type": "article", "article_ref": "Article 40", "node_types": ["Sanction", "Requirement"]}, "text": "[ARTICLE] Article 40\n\nArticle 40\nHarmonised standards and standardisation deliverables\n1.   High-risk AI systems or general-purpose AI models which are in conformity with harmonised standards or parts thereof the references of which have been published in the\nOfficial Journal of the European Union in accordance with Regulation (EU) No 1025/2012 shall be presumed to be in conformity with the requirements set out in Section 2 of this Chapter or, as applicable, with the obligations set out in of Chapter V, Sections 2 and 3, of this Regulation, to the extent that those standards cover those requirements or obligations.\n2.   In accordance with Article 10 of Regulation (EU) No 1025/2012, the Commission shall issue, without undue delay, standardisation requests covering all requirements set out in Section 2 of this Ch\n\nArticle 40\nHarmonised standards and standardisation deliverables\n1.   High-risk AI systems or general-purpose AI models which are in conformity with harmonised standards or parts thereof the references of which have been published in the\nOfficial Journal of the European Union in accordance with Regulation (EU) No 1025/2012 shall be presumed to be in conformity with the requirements set out in Section 2 of this Chapter or, as applicable, with the obligations set out in of Chapter V, Sections 2 and 3, of this Regulation, to the extent that those standards cover those requirements or obligations.\n2.   In accordance with Article 10 of Regulation (EU) No 1025/2012, the Commission shall issue, without undue delay, standardisation requests covering all requirements set out in Section 2 of this Ch [Default Stakeholder: no Stakeholder could be extracted from the text; Provider assigned as default per §3.1.]\n\nArticle 40\nHarmonised standards and standardisation deliverables\n1.   High-risk AI systems or general-purpose AI models which are in conformity with harmonised standards or parts thereof the references of which have been published in the\nOfficial Journal of the European Union in accordance with Regulation (EU) No 1025/2012 shall be presumed to be in conformity with the requirements set out in Section 2 of this Chapter or, as applicable, with the obligations set out in of Chapter V, Sections 2 and 3, of this Regulation, to the extent that those standards cover those requirements or obligations.\n2.   In accordance with Article 10 of Regulation (EU) No 1025/2012, the Commission shall issue, without undue delay, standardisation requests covering all requirements set out in Section 2 of this Ch"}
{"meta": {"section_type": "article", "article_ref": "Article 42", "node_types": ["Sanction", "Requirement"]}, "text": "[ARTICLE] Article 42\n\nArticle 42\nPresumption of conformity with certain requirements\n1.   High-risk AI systems that have been trained and tested on data reflecting the specific geographical, behavioural, contextual or functional setting within which they are intended to be used shall be presumed to comply with the relevant requirements laid down in Article 10(4).\n2.   High-risk AI systems that have been certified or for which a statement of conformity has been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019/881 and the references of which have been published in the\nOfficial Journal of the European Union shall be presumed to comply with the cybersecurity requirements set out in Article 15 of this Regulation in so far as the cybersecurity certificate or statement of conformity or parts thereo\n\nArticle 42\nPresumption of conformity with certain requirements\n1.   High-risk AI systems that have been trained and tested on data reflecting the specific geographical, behavioural, contextual or functional setting within which they are intended to be used shall be presumed to comply with the relevant requirements laid down in Article 10(4).\n2.   High-risk AI systems that have been certified or for which a statement of conformity has been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019/881 and the references of which have been published in the\nOfficial Journal of the European Union shall be presumed to comply with the cybersecurity requirements set out in Article 15 of this Regulation in so far as the cybersecurity certificate or statement of conformity or parts thereo [Default Stakeholder: no Stakeholder could be extracted from the text; Provider assigned as default per §3.1.]\n\nArticle 42\nPresumption of conformity with certain requirements\n1.   High-risk AI systems that have been trained and tested on data reflecting the specific geographical, behavioural, contextual or functional setting within which they are intended to be used shall be presumed to comply with the relevant requirements laid down in Article 10(4).\n2.   High-risk AI systems that have been certified or for which a statement of conformity has been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019/881 and the references of which have been published in the\nOfficial Journal of the European Union shall be presumed to comply with the cybersecurity requirements set out in Article 15 of this Regulation in so far as the cybersecurity certificate or statement of conformity or parts thereo"}
{"meta": {"section_type": "article", "article_ref": "Article 43", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 43\n\nArticle 43\nConformity assessment\n1.   For high-risk AI systems listed in point 1 of Annex III, where, in demonstrating the compliance of a high-risk AI system with the requirements set out in Section 2, the provider has applied harmonised standards referred to in Article 40, or, where applicable, common specifications referred to in Article 41, the provider shall opt for one of the following conformity assessment procedures based on:\n(a) the internal control referred to in Annex VI; or\n(b) the assessment of the quality management system and the assessment of the technical documentation, with the involvement of a notified body, referred to in Annex VII.\nIn demonstrating the compliance of a high-risk AI system with the requirements set out in Section 2, the provider shall follow the conformi"}
{"meta": {"section_type": "article", "article_ref": "Article 46", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 46\n\nArticle 46\nDerogation from conformity assessment procedure\n1.   By way of derogation from Article 43 and upon a duly justified request, any market surveillance authority may authorise the placing on the market or the putting into service of specific high-risk AI systems within the territory of the Member State concerned, for exceptional reasons of public security or the protection of life and health of persons, environmental protection or the protection of key industrial and infrastructural assets. That authorisation shall be for a limited period while the necessary conformity assessment procedures are being carried out, taking into account the exceptional reasons justifying the derogation. The completion of those procedures shall be undertaken without undue delay.\n2.   In a duly justified"}
{"meta": {"section_type": "article", "article_ref": "Article 47", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 47\n\nArticle 47\nEU declaration of conformity\n1.   The provider shall draw up a written machine readable, physical or electronically signed EU declaration of conformity for each high-risk AI system, and keep it at the disposal of the national competent authorities for 10 years after the high-risk AI system has been placed on the market or put into service. The EU declaration of conformity shall identify the high-risk AI system for which it has been drawn up. A copy of the EU declaration of conformity shall be submitted to the relevant national competent authorities upon request.\n2.   The EU declaration of conformity shall state that the high-risk AI system concerned meets the requirements set out in Section 2. The EU declaration of conformity shall contain the information set out in Annex V, and"}
{"meta": {"section_type": "article", "article_ref": "Article 48", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 48\n\nArticle 48\nCE marking\n1.   The CE marking shall be subject to the general principles set out in Article 30 of Regulation (EC) No 765/2008.\n2.   For high-risk AI systems provided digitally, a digital CE marking shall be used, only if it can easily be accessed via the interface from which that system is accessed or via an easily accessible machine-readable code or other electronic means.\n3.   The CE marking shall be affixed visibly, legibly and indelibly for high-risk AI systems. Where that is not possible or not warranted on account of the nature of the high-risk AI system, it shall be affixed to the packaging or to the accompanying documentation, as appropriate.\n4.   Where applicable, the CE marking shall be followed by the identification number of the notified body responsible for the con"}
{"meta": {"section_type": "article", "article_ref": "Article 49", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 49\n\nArticle 49\nRegistration\n1.   Before placing on the market or putting into service a high-risk AI system listed in Annex III, with the exception of high-risk AI systems referred to in point 2 of Annex III, the provider or, where applicable, the authorised representative shall register themselves and their system in the EU database referred to in Article 71.\n2.   Before placing on the market or putting into service an AI system for which the provider has concluded that it is not high-risk according to Article 6(3), that provider or, where applicable, the authorised representative shall register themselves and that system in the EU database referred to in Article 71.\n3.   Before putting into service or using a high-risk AI system listed in Annex III, with the exception of high-risk AI systems"}
{"meta": {"section_type": "article", "article_ref": "Article 60", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 60\n\nArticle 60\nTesting of high-risk AI systems in real world conditions outside AI regulatory sandboxes\n1.   Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes may be conducted by providers or prospective providers of high-risk AI systems listed in Annex III, in accordance with this Article and the real-world testing plan referred to in this Article, without prejudice to the prohibitions under Article 5.\nThe Commission shall, by means of implementing acts, specify the detailed elements of the real-world testing plan. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 98(2).\nThis paragraph shall be without prejudice to Union or national law on the testing in real world conditions of high-risk AI syst"}
{"meta": {"section_type": "article", "article_ref": "Article 63", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 63\n\nArticle 63\nDerogations for specific operators\n1.   Microenterprises within the meaning of Recommendation 2003/361/EC may comply with certain elements of the quality management system required by Article 17 of this Regulation in a simplified manner, provided that they do not have partner enterprises or linked enterprises within the meaning of that Recommendation. For that purpose, the Commission shall develop guidelines on the elements of the quality management system which may be complied with in a simplified manner considering the needs of microenterprises, without affecting the level of protection or the need for compliance with the requirements in respect of high-risk AI systems.\n2.   Paragraph 1 of this Article shall not be interpreted as exempting those operators from fulfilling any o"}
{"meta": {"section_type": "article", "article_ref": "Article 71", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 71\n\nArticle 71\nEU database for high-risk AI systems listed in Annex III\n1.   The Commission shall, in collaboration with the Member States, set up and maintain an EU database containing information referred to in paragraphs 2 and 3 of this Article concerning high-risk AI systems referred to in Article 6(2) which are registered in accordance with Articles 49 and 60 and AI systems that are not considered as high-risk pursuant to Article 6(3) and which are registered in accordance with Article 6(4) and Article 49. When setting the functional specifications of such database, the Commission shall consult the relevant experts, and when updating the functional specifications of such database, the Commission shall consult the Board.\n2.   The data listed in Sections A and B of Annex VIII shall be enter"}
{"meta": {"section_type": "article", "article_ref": "Article 72", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 72\n\nArticle 72\nPost-market monitoring by providers and post-market monitoring plan for high-risk AI systems\n1.   Providers shall establish and document a post-market monitoring system in a manner that is proportionate to the nature of the AI technologies and the risks of the high-risk AI system.\n2.   The post-market monitoring system shall actively and systematically collect, document and analyse relevant data which may be provided by deployers or which may be collected through other sources on the performance of high-risk AI systems throughout their lifetime, and which allow the provider to evaluate the continuous compliance of AI systems with the requirements set out in Chapter III, Section 2. Where relevant, post-market monitoring shall include an analysis of the interaction with other AI s"}
{"meta": {"section_type": "article", "article_ref": "Article 73", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 73\n\nArticle 73\nReporting of serious incidents\n1.   Providers of high-risk AI systems placed on the Union market shall report any serious incident to the market surveillance authorities of the Member States where that incident occurred.\n2.   The report referred to in paragraph 1 shall be made immediately after the provider has established a causal link between the AI system and the serious incident or the reasonable likelihood of such a link, and, in any event, not later than 15 days after the provider or, where applicable, the deployer, becomes aware of the serious incident.\nThe period for the reporting referred to in the first subparagraph shall take account of the severity of the serious incident.\n3.   Notwithstanding paragraph 2 of this Article, in the event of a widespread infringement or"}
{"meta": {"section_type": "article", "article_ref": "Article 77", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 77\n\nArticle 77\nPowers of authorities protecting fundamental rights\n1.   National public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights, including the right to non-discrimination, in relation to the use of high-risk AI systems referred to in Annex III shall have the power to request and access any documentation created or maintained under this Regulation in accessible language and format when access to that documentation is necessary for effectively fulfilling their mandates within the limits of their jurisdiction. The relevant public authority or body shall inform the market surveillance authority of the Member State concerned of any such request.\n2.   By 2 November 2024, each Member State shall identify the public auth"}
{"meta": {"section_type": "article", "article_ref": "Article 80", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 80\n\nArticle 80\nProcedure for dealing with AI systems classified by the provider as non-high-risk in application of Annex III\n1.   Where a market surveillance authority has sufficient reason to consider that an AI system classified by the provider as non-high-risk pursuant to Article 6(3) is indeed high-risk, the market surveillance authority shall carry out an evaluation of the AI system concerned in respect of its classification as a high-risk AI system based on the conditions set out in Article 6(3) and the Commission guidelines.\n2.   Where, in the course of that evaluation, the market surveillance authority finds that the AI system concerned is high-risk, it shall without undue delay require the relevant provider to take all necessary actions to bring the AI system into compliance with the"}
{"meta": {"section_type": "article", "article_ref": "Article 82", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 82\n\nArticle 82\nCompliant AI systems which present a risk\n1.   Where, having performed an evaluation under Article 79, after consulting the relevant national public authority referred to in Article 77(1), the market surveillance authority of a Member State finds that although a high-risk AI system complies with this Regulation, it nevertheless presents a risk to the health or safety of persons, to fundamental rights, or to other aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure that the AI system concerned, when placed on the market or put into service, no longer presents that risk without undue delay, within a period it may prescribe.\n2.   The provider or other relevant operator shall ensure that corrective action is taken"}
{"meta": {"section_type": "article", "article_ref": "Article 86", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 86\n\nArticle 86\nRight to explanation of individual decision-making\n1.   Any affected person subject to a decision which is taken by the deployer on the basis of the output from a high-risk AI system listed in Annex III, with the exception of systems listed under point 2 thereof, and which produces legal effects or similarly significantly affects that person in a way that they consider to have an adverse impact on their health, safety or fundamental rights shall have the right to obtain from the deployer clear and meaningful explanations of the role of the AI system in the decision-making procedure and the main elements of the decision taken.\n2.   Paragraph 1 shall not apply to the use of AI systems for which exceptions from, or restrictions to, the obligation under that paragraph follow from Un"}
{"meta": {"section_type": "article", "article_ref": "Article 95", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 95\n\nArticle 95\nCodes of conduct for voluntary application of specific requirements\n1.   The AI Office and the Member States shall encourage and facilitate the drawing up of codes of conduct, including related governance mechanisms, intended to foster the voluntary application to AI systems, other than high-risk AI systems, of some or all of the requirements set out in Chapter III, Section 2 taking into account the available technical solutions and industry best practices allowing for the application of such requirements.\n2.   The AI Office and the Member States shall facilitate the drawing up of codes of conduct concerning the voluntary application, including by deployers, of specific requirements to all AI systems, on the basis of clear objectives and key performance indicators to measure the"}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Article-specific]\n\n2 Its aim is to promote innovation in and the uptake of \nAI, while ensuring a high level of protection of health, safety, and fundamental rights in \nthe Union, including democracy and the rule of law. \n \n(2) The AI Act does not apply to all systems, but only to those systems that fulfil the \ndefinition of an ‘AI system’ within the meaning of Article 3(1) AI Act. The definition of \nan AI system is therefore key to understanding the scope of application of the AI Act. \n \n(3) Article 96(1)(f) AI Act requires the Commission to develop guidelines on the application \nof the definition o f an AI system as set out in Article 3(1) of that Act. By issuing these \nGuidelines, the Commission aims to assist providers and other relevant persons, \nincluding market and institutional stakeholders, in determining whether a system \nconstitutes an AI syste m within the meaning of the AI Act, thereby facilitating the \neffective application and enforcement of that Act.  \n \n(4) The definition of an AI system entered into application on 2 February 2025 3, together \nwith other provisions set out in Chapters I and II AI A ct, notably Article 5 AI Act on \nprohibited AI practices."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nCONTENTS \n1. Background and objectives ............................................................................................................. 1 \n2. Overview of prohibited AI practices ............................................................................................... 2 \n2.1. Prohibitions listed in Article 5 AI Act .................................................................................... 2 \n2.2. Legal basis of the prohibitions ................................................................................................ 3 \n2.3. Material scope: practices related to the ‘placing on the market’, ‘putting into service’ or \n‘use’ of an AI system .......................................................................................................................... 4 \n2.4. Personal scope: responsible actors .......................................................................................... 5 \n2.5. Exclusion from the scope of the AI Act .................................................................................. 7 \n2.5.1. National security, defence and military purposes ........................................................... 7 \n2.5.2. Judicial and law enforcement cooperation with third countries ...................................... 9 \n2.5.3. Research & Development ............................................................................................... 9 \n2.5.4. Personal non-professional activity ................................................................................ 10 \n2.5.5. AI systems released under free and open source licences ............................................. 11 \n2.6. Interplay of the prohibitions with the requirements for high-risk AI systems ...................... 12 \n2.7."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nApplication of the prohibitions to general-purpose AI systems and systems with intended \npurposes ............................................................................................................................................ 12 \n2.8. Interplay between the prohibitions and other Union law ...................................................... 14 \n2.9. Enforcement of Article 5 AI Act ........................................................................................... 17 \n2.9.1. Market Surveillance Authorities ................................................................................... 17 \n2.9.2. Penalties ........................................................................................................................ 17 \n3. Article 5(1)(a) and (b) AI Act – harmful manipulation, deception and exploitation .................... 18 \n3.1. Rationale and objectives ....................................................................................................... 18 \n3.2. Main components of the prohibition in Article 5(1)(a) AI Act – harmful manipulation ...... 19 \n3.2.1. Subliminal, purposefully manipulative or deceptive techniques .................................. 19 \n3.2.2. With the objective or the effect of materially distorting the behaviour of a person or a \ngroup of persons ............................................................................................................................ 24 \n3.2.3. (Reasonably likely to) cause significant harm .............................................................. 28 \n3.3. Main components of the prohibition in Article 5(1)(b) AI Act – harmful exploitation of \nvulnerabilities .................................................................................................................................... 33 \n3.3.1. Exploitation of vulnerabilities due to age, disability, or a specific socio-economic \nsituation 33 \n3.3.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nWith the objective or the effect of materially distorting behaviour .............................. 38 \n3.3.3. (Reasonably likely to) cause significant harm .............................................................. 38 \n3.4. Interplay between the prohibitions in Article 5(1)(a) and (b) AI Act ................................... 42"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 5]\n\n6.2.2. Through untargeted scraping of facial images .............................................................. 78 \n6.2.3. From the Internet and CCTV footage ........................................................................... 79 \n6.3. Out of scope .......................................................................................................................... 79 \n6.4. Interplay with other Union legal acts .................................................................................... 80 \n7. Article 5(1)(f) AI Act emotion recognition ................................................................................... 80 \n7.1. Rationale and objectives ....................................................................................................... 80 \n7.2. Main concepts and components of the prohibition ............................................................... 81 \n7.2.1. AI systems to infer emotions ........................................................................................ 82 \n7.2.2. Limitation of the prohibition to workplace and educational institutions ...................... 84 \n7.2.3. Exceptions for medical and safety reasons ................................................................... 87 \n7.3. More favourable Member State law ...................................................................................... 88 \n7.4. Out of scope .......................................................................................................................... 89 \n8. Article 5(1)(g) AI Act: Biometric categorisation for certain ‘sensitive’ characteristics ............... 90 \n8.1. Rationale and objectives ....................................................................................................... 90 \n8.2. Main concepts and components of the prohibition ............................................................... 90 \n8.2.1."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n6.2.2. Through untargeted scraping of facial images .............................................................. 78 \n6.2.3. From the Internet and CCTV footage ........................................................................... 79 \n6.3. Out of scope .......................................................................................................................... 79 \n6.4. Interplay with other Union legal acts .................................................................................... 80 \n7. Article 5(1)(f) AI Act emotion recognition ................................................................................... 80 \n7.1. Rationale and objectives ....................................................................................................... 80 \n7.2. Main concepts and components of the prohibition ............................................................... 81 \n7.2.1. AI systems to infer emotions ........................................................................................ 82 \n7.2.2. Limitation of the prohibition to workplace and educational institutions ...................... 84 \n7.2.3. Exceptions for medical and safety reasons ................................................................... 87 \n7.3. More favourable Member State law ...................................................................................... 88 \n7.4. Out of scope .......................................................................................................................... 89 \n8. Article 5(1)(g) AI Act: Biometric categorisation for certain ‘sensitive’ characteristics ............... 90 \n8.1. Rationale and objectives ....................................................................................................... 90 \n8.2. Main concepts and components of the prohibition ............................................................... 90 \n8.2.1."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 5]\n\nBiometric categorisation system ................................................................................... 91 \n8.2.2. Persons are individually categorised based on their biometric data .............................. 93 \n8.2.3. To deduce or infer their race, political opinions, trade union membership, religious or \nphilosophical beliefs, sex-life or sexual orientation ...................................................................... 93 \n8.3. Out of scope .......................................................................................................................... 94 \n8.4. Interplay with other Union law ............................................................................................. 95 \n9. Article 5(1)(h) AI Act - Real-time Remote Biometric Identification (RBI) Systems for Law \nEnforcement Purposes .......................................................................................................................... 95 \n9.1. Rationale and objectives ....................................................................................................... 96 \n9.2. Main concepts and components of the prohibition ............................................................... 97 \n9.2.1. The Notion of Remote Biometric Identification ........................................................... 97 \n9.2.2. Real-time ..................................................................................................................... 100 \n9.2.3. In publicly accessible spaces . ..................................................................................... 101 \n9.2.4. For law enforcement purposes .................................................................................... 103 \n9.3. Exceptions to the prohibition .............................................................................................. 104 \n9.3.1."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nBiometric categorisation system ................................................................................... 91 \n8.2.2. Persons are individually categorised based on their biometric data .............................. 93 \n8.2.3. To deduce or infer their race, political opinions, trade union membership, religious or \nphilosophical beliefs, sex-life or sexual orientation ...................................................................... 93 \n8.3. Out of scope .......................................................................................................................... 94 \n8.4. Interplay with other Union law ............................................................................................. 95 \n9. Article 5(1)(h) AI Act - Real-time Remote Biometric Identification (RBI) Systems for Law \nEnforcement Purposes .......................................................................................................................... 95 \n9.1. Rationale and objectives ....................................................................................................... 96 \n9.2. Main concepts and components of the prohibition ............................................................... 97 \n9.2.1. The Notion of Remote Biometric Identification ........................................................... 97 \n9.2.2. Real-time ..................................................................................................................... 100 \n9.2.3. In publicly accessible spaces . ..................................................................................... 101 \n9.2.4. For law enforcement purposes .................................................................................... 103 \n9.3. Exceptions to the prohibition .............................................................................................. 104 \n9.3.1."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education]\n\nRationale and objectives ............................................................................................. 105 \n9.3.2. Targeted search for the victims of three serious crimes and missing persons ............ 105 \n9.3.3. Prevention of imminent threats to life or terrorist attacks ........................................... 107 \n9.3.4. Localisation and identification of suspects of certain crimes ..................................... 109"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\nRationale and objectives ............................................................................................. 105 \n9.3.2. Targeted search for the victims of three serious crimes and missing persons ............ 105 \n9.3.3. Prevention of imminent threats to life or terrorist attacks ........................................... 107 \n9.3.4. Localisation and identification of suspects of certain crimes ..................................... 109"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n10. Safeguards and Conditions for the exceptions (Article 5(2)-(7) AI Act) ................................ 112 \n10.1. Targeted individual and safeguards (Article 5(2) AI Act) .............................................. 112 \n10.1.1. Fundamental Rights Impact Assessment ................................................................ 114 \n10.1.2. Registration of the authorized RBI systems ............................................................ 118 \n10.2. Need for prior authorisation ............................................................................................ 119 \n10.2.1. Objective ................................................................................................................. 120 \n10.2.2. The main principle: Prior authorisation by a judicial authority or an independent \nadministrative authority .............................................................................................................. 120 \n10.3. Notification to the authorities of each use of ‘real-time’ remote biometric identification \nsystems in publicly accessible spaces for law enforcement ............................................................ 126 \n10.4. Need for national laws within the limits of the AI Act exceptions ................................. 127 \n10.4.1. Principle: national law required to provide the legal basis for the authorisation for all \nor some of the exceptions............................................................................................................ 127 \n10.4.2. National law shall respect the limits and conditions of Article 5(1)(h) AI Act ...... 127 \n10.4.3. Detailed national law on the authorisation request, the issuance and the exercise . 128 \n10.4.4. Detailed national law on the supervision and the reporting relating to the \nauthorisation ................................................................................................................................ 130 \n10.5."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\nAnnual reports by the national market surveillance authorities and the national data \nprotection authorities of Member States ......................................................................................... 130 \n10.6. Annual reports by the Commission ................................................................................. 131 \n10.7. Out-of-Scope ................................................................................................................... 131 \n10.8. Examples of uses ............................................................................................................. 132 \n11. Entry into application .............................................................................................................. 135 \n12. Review and update of the Commission guidelines ................................................................. 135"}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article \n5]\n\n3 \nsocial score leading to detrimental or unfavourable \ntreatment when data comes from unrelated social \ncontexts or such treatment is unjustified or \ndisproportionate to the social behaviour \nArticle \n5(1)(d) \nIndividual criminal \noffence risk assessment \nand prediction \nAI systems that assess or predict the risk of people \ncommitting a criminal offence based solely on \nprofiling or personality traits and characteristics; \nexcept to support a human assessment based on \nobjective and verifiable facts directly linked to a \ncriminal activity \nArticle \n5(1)(e) \nUntargeted scraping to \ndevelop facial \nrecognition databases \nAI systems that create or expand facial recognition \ndatabases through untargeted scraping of facial \nimages from the internet or closed-circuit television \n(‘CCTV’) footage \nArticle \n5(1)(f) \nEmotion recognition AI systems that infer emotions at the workplace or \nin education institutions; except for medical or \nsafety reasons \nArticle \n5(1)(g) \nBiometric categorisation AI systems that categorise people based on their \nbiometric data to deduce or infer their race, political \nopinions, trade union membership, religious or \nphilosophical beliefs, sex-life or sexual orientation; \nexcept for labelling or filtering of lawfully acquired \nbiometric datasets, including in the area of law \nenforcement \nArticle \n5(1)(h) \nReal-time remote \nbiometric identification \n(‘RBI’) \nAI systems for real-time remote biometric \nidentification in publicly accessible spaces for the \npurposes of law enforcement; except if necessary \nfor the targeted search of specific victims, the \nprevention of specific threats including terrorist \nattacks, or the search of suspects of specific \noffences (further procedural requirements, \nincluding for authorisation, outlined in Article 5(2-\n7) AI Act). 2.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article \n5]\n\n3 \nsocial score leading to detrimental or unfavourable \ntreatment when data comes from unrelated social \ncontexts or such treatment is unjustified or \ndisproportionate to the social behaviour \nArticle \n5(1)(d) \nIndividual criminal \noffence risk assessment \nand prediction \nAI systems that assess or predict the risk of people \ncommitting a criminal offence based solely on \nprofiling or personality traits and characteristics; \nexcept to support a human assessment based on \nobjective and verifiable facts directly linked to a \ncriminal activity \nArticle \n5(1)(e) \nUntargeted scraping to \ndevelop facial \nrecognition databases \nAI systems that create or expand facial recognition \ndatabases through untargeted scraping of facial \nimages from the internet or closed-circuit television \n(‘CCTV’) footage \nArticle \n5(1)(f) \nEmotion recognition AI systems that infer emotions at the workplace or \nin education institutions; except for medical or \nsafety reasons \nArticle \n5(1)(g) \nBiometric categorisation AI systems that categorise people based on their \nbiometric data to deduce or infer their race, political \nopinions, trade union membership, religious or \nphilosophical beliefs, sex-life or sexual orientation; \nexcept for labelling or filtering of lawfully acquired \nbiometric datasets, including in the area of law \nenforcement \nArticle \n5(1)(h) \nReal-time remote \nbiometric identification \n(‘RBI’) \nAI systems for real-time remote biometric \nidentification in publicly accessible spaces for the \npurposes of law enforcement; except if necessary \nfor the targeted search of specific victims, the \nprevention of specific threats including terrorist \nattacks, or the search of suspects of specific \noffences (further procedural requirements, \nincluding for authorisation, outlined in Article 5(2-\n7) AI Act). 2.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 114", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 114]\n\nLegal basis of the prohibitions  \n(10) The AI Act is supported by two legal bases: Article 114 of the Treaty on the Functioning \nof the European Union (‘TFEU’) (the internal market legal basis) and Article 16 TFEU \n(the data protection legal basis). Article 16 TFEU serves as a legal basis for the specific \nrules on  the processing of personal data in relation to the prohibition on the use of  \nremote biometric identification (‘RBI’) systems for law enforcement purposes, \nbiometric categorisation systems for law enforcement purposes , and individual risk \nassessments for law enforcement purposes.4 All other prohibitions listed in Article 5 AI \nAct find their legal basis in Article 114 TFEU."}
{"meta": {"section_type": "guideline", "article_ref": "Article 114", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 114]\n\nLegal basis of the prohibitions  \n(10) The AI Act is supported by two legal bases: Article 114 of the Treaty on the Functioning \nof the European Union (‘TFEU’) (the internal market legal basis) and Article 16 TFEU \n(the data protection legal basis). Article 16 TFEU serves as a legal basis for the specific \nrules on  the processing of personal data in relation to the prohibition on the use of  \nremote biometric identification (‘RBI’) systems for law enforcement purposes, \nbiometric categorisation systems for law enforcement purposes , and individual risk \nassessments for law enforcement purposes.4 All other prohibitions listed in Article 5 AI \nAct find their legal basis in Article 114 TFEU."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n4 \n2.3. Material scope: practices related to  the ‘placing on the market’, ‘putting \ninto service’ or ‘use’ of an AI system \n(11) The practices prohibited by  Article 5 AI Act relate to the placing on the market, the \nputting into service , or the use of specific AI systems.5 As regards real -time remote \nbiometric identification (‘RBI’) systems, the prohibition in Article 5(1)(h) AI Act only \napplies to their use. Article 3(1) AI Act defines what constitutes an A I system. The \nGuidelines on the Definition of an AI system provide the Commission’s interpretation \nof that definition. (12) According to Article 3(9) AI Act,  the placing on the market  of an AI system is ‘the \nfirst making available of an AI system […] on the Union market’. ‘Making available’ \nis defined as the supply of the system ‘for distribution or use on the Union market in \nthe course of a commercial activity, whether in return for payment or free of charge.’ 6 \nThe making available  of an AI system  is covered regardless of the means  of supply, \nsuch as access to the system  and its service  through an application programm ing \ninterface ( ‘API’), via cloud, direct downloads, as physical copies , or embedded in \nphysical products. For example, a RBI system developed outside the Union by a third-country provider is \nplaced on the Union market for the first time when it is offered in return for payment or \nfree of charge in one or more Member States. Such placing on the market may occur by \nproviding access to the system online through an API or other user interface. (13) Article 3(11) AI Act defines putting into service  as ‘the supply of an AI system for \nfirst use to the deployer or for own use in the Union for its intended purpose’, therefore \ncovering both supply for first use to third parties, as well as in-house development and \ndeployment."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\nThe intended purpose of the system is the ‘use for which an AI system is \nintended by the provider, including the specific context and condition s of use, as \nspecified in the information supplied by the provider in the instructions of use, \npromotional or sales materials and statements, as well as in the technical \ndocumentation.’7 \nFor example, a provider builds a RBI system outside the Union and supplies that system \nto a law enforcement authority or to a private company established in a Member State \nto be used for the first time, thereby putting it into service. For example, a public authority develops a scoring system  in-house and deploys it to \npredict the risk of fraud of household allowance beneficiaries, thereby putting it into \nservice."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article \n5]\n\nsecurity and justice (AFSJ) annexed to the TEU and TFEU, Ireland may decide not to apply the rules concerning the prohibition of \nreal-time use of RBIs in publi c spaces for a law enforcement purpose as well as the procedural rules linked to that article (Article \n5(2) to (6) AI Act) (see Recital 40). Denmark benefits from opt-out agreements when applying Protocol No. 22 to the TEU and TFEU \nand may decide not to fully apply the prohibitions based on Article 16 of the TFEU (see Recital 41). 5 See for definitions of these terms also the Commission Notice – The ‘Blue Guide’ on the implementation of EU product rules 2022, \n2022/C 247/01, Section 2. 6 Article 3(10) AI Act. 7 Article 3(12) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n6 \nbe a public authority that develops the system in -house and puts it into service for its \nown use. (17) Deployers are natural or legal persons, public authorities, agencies or other bodies \nusing AI systems under their authority, unless the use is for a personal non-professional \nactivity.14 ‘Authority’ over an AI system  should be understood as assuming \nresponsibility over the decision to deploy the system and over the manner of its actual \nuse. Deployers fall within the scope of the AI Act , if their place of establishment or \nlocation is within the Union15 or, if they are located in a third country, the output of the \nAI system is used in the Union.16 \n(18) Where the deployer of an AI system is a legal person under whose authority the system \nis used, i.e. a law enforcement authority or a private security company,  the individual \nemployees that act within the procedures and under the control of that person should \nnot be considered to be the deployer . A legal person also remains  a deployer if it \ninvolves third parties (e.g. contractors, external staff) in the operation of the system on \nits behalf and under its responsibility and control. (19) Operators may fulfil more than one role concurrently in relation to an AI system. For \nexample, if an operator develops its own AI system that it use s afterwards, it will be \nconsidered both the provider and the deployer of that system, even if that system is also \nused by other deployers to whom the system has been provided in return for payment \nor free of charge. (20) Continuous compliance with the AI Act is required during all phases of the AI lifecycle. This necessitates ongoing monitoring of and updates to AI systems placed on the market \nor put into service in the Union to ensure that an AI system remains compliant with the \nAI Act throughout its lifecycle and that it does not result in a practice prohibited under \nArticle 5 AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 2]\n\nProviders and deployers of AI systems have different responsibilities \ndepending on their roles and control over the design , the development and the actual \nuse of the system to avoid a prohibited practice. For each of the prohibitions, these roles \nand responsibilities should be interpreted in a proportionate manner, taking into account \nwho in the value chain is best placed to adopt specific preventive and mitigating \nmeasures and ensure compliant development and use  of AI systems  in line with the  \nobjectives and the approach of the AI Act. 2.5. Exclusion from the scope of the AI Act  \n(21) Article 2 AI Act provides for a number of  general exclusions from scope which are \nrelevant for a complete understanding of the practical application of the prohibitions  \nlisted in Article 5 AI Act. 2.5.1. National security, defence and military purposes \n(22) According to Article 2(3) AI Act, the AI Act does not apply to areas outside the scope \nof Union law, and should not, in any event, affect the competences of the Member States"}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 2]\n\n7 \nconcerning national security, regardless of the type of entity entrusted by the Member \nStates with carrying out tasks in relat ion to those competences. The AI Act expressly \nexcludes from its scope AI systems that are ‘placed on the market, put into service, or \nused with or without modification exclusively for military, defence or national security \npurposes, regardless of the type  of entity carrying out those activities.’ Whether that \nexclusion applies therefore depends on the purposes or the uses of the AI system, not \nthe entities carrying out the activities with that system, which may also cover private \noperators entrusted by the  Member States with carrying out tasks in relation to those \ncompetences. (23) According to the CJEU, the term ‘national security’ refers to ‘the primary interest in \nprotecting the essential functions of the State and the fundamental interests of society \nand encompasses the prevention and punishment of activities capable of ser iously \ndestabilising the fundamental constitutional, political, economic or social structures of \na country and, in particular, of directly threatening society, the population or the Stat e \nitself, such as terrorist activities .’17 National security  does not cover , for example \nactivities relating to road safety,18 or the organisation or administration of justice.19 As \nstated by the CJEU, ‘it is for the Member States to define their essential security \ninterests and to adopt appropriate measures to ensure their internal and external security, \n[…] a national measure […] taken for the purpose of protecting national security cannot \nrender EU law inapplicable and exempt the Member States from their obligation to \ncomply with that law.’20 \n(24) For the exclusion in Article 2(3), second subparagraph, AI Act to apply, the AI system \nmust be placed on the market, put into service or used exclusively for military, defence \nor national security purposes."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Administration of Justice", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Administration of Justice] [Article 2]\n\n7 \nconcerning national security, regardless of the type of entity entrusted by the Member \nStates with carrying out tasks in relat ion to those competences. The AI Act expressly \nexcludes from its scope AI systems that are ‘placed on the market, put into service, or \nused with or without modification exclusively for military, defence or national security \npurposes, regardless of the type  of entity carrying out those activities.’ Whether that \nexclusion applies therefore depends on the purposes or the uses of the AI system, not \nthe entities carrying out the activities with that system, which may also cover private \noperators entrusted by the  Member States with carrying out tasks in relation to those \ncompetences. (23) According to the CJEU, the term ‘national security’ refers to ‘the primary interest in \nprotecting the essential functions of the State and the fundamental interests of society \nand encompasses the prevention and punishment of activities capable of ser iously \ndestabilising the fundamental constitutional, political, economic or social structures of \na country and, in particular, of directly threatening society, the population or the Stat e \nitself, such as terrorist activities .’17 National security  does not cover , for example \nactivities relating to road safety,18 or the organisation or administration of justice.19 As \nstated by the CJEU, ‘it is for the Member States to define their essential security \ninterests and to adopt appropriate measures to ensure their internal and external security, \n[…] a national measure […] taken for the purpose of protecting national security cannot \nrender EU law inapplicable and exempt the Member States from their obligation to \ncomply with that law.’20 \n(24) For the exclusion in Article 2(3), second subparagraph, AI Act to apply, the AI system \nmust be placed on the market, put into service or used exclusively for military, defence \nor national security purposes."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\nRecital 2 4 AI Act  further clarifies  how the notion  \n‘exclusively’ should be interpreted and when an AI system used for such purposes may \nnevertheless fall within the scope of the AI Act. For example, if an AI system placed on the market, put into service or used for military, \ndefence or national security purposes is used (temporarily or permanently) for other \npurposes, such as for civilian or humanitarian purposes, law enforcement or publi c \nsecurity purposes, that system will fall within the scope of the AI Act. In that case, the \nentity using the AI system for the other purposes should ensure compliance of the AI \nsystem with the AI Act, unless the system already complies with that act, which has to \nbe verified before such use. (25) Furthermore, recital 24 AI Act clarifies that AI systems placed on the market or put into \nservice for an excluded purpose, namely military, defence or national security, and for \none or more non-excluded purposes, such as civilian or law enforcement purposes (so"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Administration of Justice", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Administration of Justice]\n\nRecital 2 4 AI Act  further clarifies  how the notion  \n‘exclusively’ should be interpreted and when an AI system used for such purposes may \nnevertheless fall within the scope of the AI Act. For example, if an AI system placed on the market, put into service or used for military, \ndefence or national security purposes is used (temporarily or permanently) for other \npurposes, such as for civilian or humanitarian purposes, law enforcement or publi c \nsecurity purposes, that system will fall within the scope of the AI Act. In that case, the \nentity using the AI system for the other purposes should ensure compliance of the AI \nsystem with the AI Act, unless the system already complies with that act, which has to \nbe verified before such use. (25) Furthermore, recital 24 AI Act clarifies that AI systems placed on the market or put into \nservice for an excluded purpose, namely military, defence or national security, and for \none or more non-excluded purposes, such as civilian or law enforcement purposes (so"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n8 \ncalled ‘ dual use ’ systems), fall within the scope of the AI Act. Providers of those \nsystems should ensure that they comply with the requirements in the AI Act. For example, if a company offers a RBI system for v arious purposes, including law \nenforcement and national security, that company is the provider of that ‘dual use’ \nsystem and must ensure its compliance with the requirements in the AI Act. (26) However, the fact that an AI system may fall within the scope of the AI Act should not \naffect the ability of entities carrying out national security, defence and military activities \nto use that system for national security, military and defence purposes, regardless of the \ntype of entity carrying out those activities.21 \nFor example, if a national security agency or a private operator is tasked by a national \nintelligence agency to use real -time RBI systems for national security purposes (such \nas to gather intelligence), such use would be excluded from the scope of the AI Act. (27) The clear delineation of the national security exclusion is particularly important where \nAI systems are placed on the market, put into service or used for law enforcement \npurposes that fall within the scope of the AI Act. This is relevant for the prohibitions \nregarding individual crime predictions and assessments and regarding the use of real -\ntime RBI systems for law enforcement purposes laid down in Article 5(1)(d) and (h) AI \nAct respectively. Police and other law enforcement authorities are taske d with the \nprevention, detection, investigation and prosecution of criminal offences or the \nexecution of criminal penalties, including safeguarding against and preventing threats \nto public security .22 Whenever AI systems are used for such purposes, they wi ll fall \nwithin the scope of the AI Act. (28) The activities of Europol and other Union security agencies, such as Frontex, fall within \nthe scope of the AI Act. 2.5.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 2]\n\nJudicial and law enforcement cooperation with third countries \n(29) According to Article 2(4)  AI Act, the AI Act does not apply to public authorities  in a \nthird country or international organisations, where those authorities or organisations use \nAI systems in the framework of international cooperation or agreements for law \nenforcement and judicial cooperation with the Union or with one or more Member \nStates, provided that such a third country or international organisation provides \nadequate safeguards with respect to the protection of fundamental rights and freedoms \nof individuals. Where relevant, this exclusion may cover the activities of private entities \nentrusted by the third country in question to carry out specific tasks in support of such \nlaw enforcement and judicial cooperation. 23 At the same time, for the exclusion to \napply, these frameworks for cooperation or international agreements must include  \nadequate safeguards with respect to the protection of  the fundamental rights and \nfreedoms of individuals, to be assessed by the market surveillance authorities competent"}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 2]\n\n9 \nfor the supervision of AI systems used in the area of  law enforcement and ju stice.24 \nRecital 22 AI Act clarifies that the recipient national authorities and Union institutions, \nbodies, offices and agencies making use of such AI outputs in the Union remain \naccountable to ensure their use complies with Union law. When those international \nagreements are revised or new ones are concluded in the future, the contracting parties \nshould make utmost efforts to align those agreements with the requirements of th e AI \nAct. 2.5.3. Research & Development  \n(30) According to Article 2(8) AI Act, the AI Act does not apply ‘to any research, testing or \ndevelopment activity regarding AI systems or AI models prior to their being placed on \nthe market or put into service’. This exclusion is in line with the market-based logic of \nthe AI Act, which applies to AI systems once they are placed on the market or put into \nservice. For example, during the research and development (R&D) phase, AI developers have \nthe freedom to experiment and test new functionalities which might involve techniques \nthat could be seen as manipulative and covered by Article 5(1) (a) AI Act, if used in \nconsumer-facing applications. The AI Act allows for such experimentation by \nrecognising that early-stage R&D is essential for refining AI technologies and ensuring \nthat they meet safety and ethical standards prior to their placing on the market. (31) As clarified in recital 25 AI Act, the AI Act aims to support innovation and recognises \nthe importance of scientific research in advancing AI technologies and contributing to \nscientific progress and innovation. Article 2(6) AI Act therefore provides an exclusion \nfor ‘AI systems or AI models, including their outputs, specifically developed and put \ninto service for the sole purpose of scientific research and development’."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nFor example, research into cognitive and behavioural responses to AI-driven subliminal \nor deceptive stimuli can provide valuable insights into human -AI interactions, \ninforming safer and more effective AI applications in the future. Such research is \npermitted, since it is excluded from the scope of the AI Act, notwithstanding the \nprohibition in Article 5(1)(a) AI Act. (32) The exclusion in Article 2(8) AI Act is, however, without prejudice to the obligation to \ncomply with the AI Act where an AI system is placed on the market or put into service \nas a result of such research and development activity.25 Testing in real-world conditions \nwithin the meaning of the AI Act26 is also not covered by that exclusion. For example, a municipality wishing to test facial recognition software using a RBI \nsystem in the streets during carn ival recruits volunteers to be identified by the system"}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 2]\n\n10 \nin real-world conditions. Because real-world testing does not fall within the exclusion \nof Article 2(8) AI Act, the planned testing  must be  fully compliant with  the \nrequirements for RBI systems in  the AI Act , unless the system is tested in an AI \nregulatory sandbox or in accordance with the special regime for testing in real world \nconditions outside the sandbox, as provided for in Articles 60 and 61 AI Act.27 \n(33) In any event, any research and development activity (including when excluded from the \nscope of the AI Act)  should be carried out in accordance with recognised ethical and \nprofessional standards for scientific research and should be conducted in accordance \nwith applicable Union law28 (e.g., data protection law that remains applicable). 2.5.4. Personal non-professional activity \n(34) Article 2(10) AI Act provides that the AI Act ‘does not apply to obligations of deployers \nwho are natural persons using systems in the course of a purely personal non-\nprofessional activity’. The definition of deployer also excludes users engaged in such \nactivities (see section 2.4. above). Any activity through which a natural person gains an \neconomic benefit on a regular basis or is otherwise involved in a professional, business, \ntrade, occupational or freelance activity should be considered  as a  ‘professional’ \nactivity. The specification of ‘personal’ is a qualifier of non-professional, meaning that \nthe person should act in both a personal and a non-professional capacity. The exclusion \nshould therefore, for example, not encompass criminal activities since these should not \nbe considered purely personal. For example, an individual using a facial recognition system at home (e.g."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 2]\n\nto control \naccess and to monitor for safety the entrance to the home) would fall under the exclusion \nof Article 2(10) AI Act and, hence, would not be subject to the obligations for deployers \nunder the AI Act, even in cases where it is required to transmit (parts of) the footage to \nlaw enforcement authorities. By contrast , a natural person using an AI system for professional activities such as \nfreelancers, journalists, doctors , etc. would need to comply with the obligations  for \ndeployers of facial recognition systems  under the AI Act. Any use by natural persons \nwhere they are acting on behalf or under the authority of a deployer acting in a \nprofessional capacity will also fall within the scope of the AI Act. Furthermore, criminal activities cannot be considered purely personal activities, even if \nno economic benefit is sought or attained . For other unlawful activities, such as non -\ncompliance with consumer protection or data protection law and national administrative \nlegislation, the exclusion in the AI Act applies, but the other relevant legal frameworks \ncontinue to apply). (35) The exclusion in Article 2(10)  AI Act  applies only as regards the  obligations of \ndeployers when using the system for purely personal non-professional activities. The \nsystem as such remains within the scope of  the AI Act as regards the obligat ions of"}
{"meta": {"section_type": "guideline", "article_ref": "Article 6", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Healthcare] [Article 6]\n\n11 \nproviders placing the system on the market or putting it into service, other professional \ndeployers, and other responsible actors, such as importers and distributors. For example, an emotion recognition system, if intended to be used by natural persons \nfor purely personal non -professional activities, remain s a high -risk AI system as \nclassified in Article 6 AI Act and must be fully in compliance with the AI Act . At the \nsame time the deployer that uses it for purely personal non-professional use (e.g., an \nautistic person) will not be covered by specific obligations  for deployers under the AI \nAct and the use would be out of scope. 2.5.5. AI systems released under free and open source licences \n(36) According to Article 2(12) AI Act, th e AI Act does not apply to AI systems released \nunder free and open-source licences,29 unless they are placed on the market or put into \nservice as high-risk AI systems or as an AI system that falls under Article 5 (prohibited \nAI practices) or Article 50 AI Act (transparency obligations for certain AI systems) . This means that providers of AI systems cannot benefit from this exclusion if the AI \nsystem they place on the market or put into service constitutes a prohibited practice \nunder Articles 5 AI Act. 2.6. Interplay of the prohibitions with the requirements for high -risk AI \nsystems \n(37) The AI practices prohibited by Article 5 AI Act should be considered in relation to the \nAI systems classified as high -risk in accordance with Article 6  AI Act, in particular \nthose listed in Annex III.30 That is because the use of AI systems classified as high-risk \nmay in some cases qualify as a prohibited practice in specific instances if all conditions \nunder one or more of the prohibitions in Article 5 AI Act are fulfilled. Conversely, most \nAI systems that fall under an exception from a prohibition listed in Article 5 AI Act will \nqualify as high-risk."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Healthcare] [Article 5]\n\nFor example, emotion recognition systems, where they do not fulfil the conditions for \nthe prohibition in Article 5(1)(f) AI Act, classify as high-risk AI systems according to \nArticle 6(2) and Annex III, point (1)(c) AI Act. Similarly, certain AI-based scoring \nsystem, such as those used for credit -scoring or assessing risk in health and life \ninsurance, will be  considered high -risk AI systems  where they do not fulfil the \nconditions for the prohibition listed in Article 5(1)(c) AI Act.31 Another example are AI \nsystems evaluating persons and determining if they are entitled to receive essential \npublic assistance benefits and services, such as healthcare services and social security \nbenefits that are classified as high -risk.32 If such systems involve unacceptable social \nscoring and fulfil the conditions of Article 5(1)(c) AI Act, their placing on the market, \nputting into service and use will be prohibited in the Union."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Biometrics", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Biometrics]\n\n29 Recital 102 AI Act describes that a release of software and data under free and open-source licence ‘allows them to be openly shared \nand where users can freely access, use, modify and redistribute them or modified versions thereto’. 30 In this list, AI systems based on biometrics are covered, as well as AI systems used for specific purposes in certain domains such as \nemployment, education, access to public and private services, law enforcement etc. 31 This is expressly mentioned in Recital 58 and in Annex III AI Act. 32 Recital 58 AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education]\n\n29 Recital 102 AI Act describes that a release of software and data under free and open-source licence ‘allows them to be openly shared \nand where users can freely access, use, modify and redistribute them or modified versions thereto’. 30 In this list, AI systems based on biometrics are covered, as well as AI systems used for specific purposes in certain domains such as \nemployment, education, access to public and private services, law enforcement etc. 31 This is expressly mentioned in Recital 58 and in Annex III AI Act. 32 Recital 58 AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment]\n\n29 Recital 102 AI Act describes that a release of software and data under free and open-source licence ‘allows them to be openly shared \nand where users can freely access, use, modify and redistribute them or modified versions thereto’. 30 In this list, AI systems based on biometrics are covered, as well as AI systems used for specific purposes in certain domains such as \nemployment, education, access to public and private services, law enforcement etc. 31 This is expressly mentioned in Recital 58 and in Annex III AI Act. 32 Recital 58 AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\n29 Recital 102 AI Act describes that a release of software and data under free and open-source licence ‘allows them to be openly shared \nand where users can freely access, use, modify and redistribute them or modified versions thereto’. 30 In this list, AI systems based on biometrics are covered, as well as AI systems used for specific purposes in certain domains such as \nemployment, education, access to public and private services, law enforcement etc. 31 This is expressly mentioned in Recital 58 and in Annex III AI Act. 32 Recital 58 AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 5]\n\n13 \ndeceive and cause significant harms  under Article 5(1) (a) AI Act , the provider  is \nexpected to  take appropriate and proportionate measures (e.g. , appropriate safe and \nethical design, integration of technical and other safeguards, restrictions of use , \ntransparency and user control, appropriate information in the instructions of use) before \nthe AI system  is placed on the market (Article 5(1) (a) AI Act) to ensure the chat bot \ndoes not cause significant harm to users or other persons or groups of persons (see also \nsection 3.2.3.(c)). (41) In certain cases, in particular where the prohibitions are linked to a very specific purpose \nof the system,36 providers may have limited possibilities to integrate other preventive \nand mitigating measures  and will have to rely on  primarily providing appropriate \ninstructions and information to the deployers and the required human oversight and \nrestricting prohibited use of the system. Where appropriate, such measures may also \ninclude monitoring for compliance with that restriction, depending on  the means \nthrough which the AI system is supplied and the information at the provider’s disposal \nfor possible misuse . Any possible monitoring measures to detect misuse should not \namount to a general monitoring of the activities of the deployers and should be in line \nwith Union law. For example, a general-purpose AI system that can recognise or infer emotions should \nnot be used by deployers at the workplaces or in education institutions , unless an \nexception for medical or safety reasons applies. However, the provider may not be in a \nposition to know the specific context in which the emotion recognition functionality of \nthe system will be used and whether an exception  to the prohibition in  Article 5(1)(f) \nAI Act may apply. Such providers may nevertheless explicitly exclude such prohibited \nuse in their terms of use and include appropriate information in the instructions of use \nto guide deployers."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment] [Article 5]\n\n13 \ndeceive and cause significant harms  under Article 5(1) (a) AI Act , the provider  is \nexpected to  take appropriate and proportionate measures (e.g. , appropriate safe and \nethical design, integration of technical and other safeguards, restrictions of use , \ntransparency and user control, appropriate information in the instructions of use) before \nthe AI system  is placed on the market (Article 5(1) (a) AI Act) to ensure the chat bot \ndoes not cause significant harm to users or other persons or groups of persons (see also \nsection 3.2.3.(c)). (41) In certain cases, in particular where the prohibitions are linked to a very specific purpose \nof the system,36 providers may have limited possibilities to integrate other preventive \nand mitigating measures  and will have to rely on  primarily providing appropriate \ninstructions and information to the deployers and the required human oversight and \nrestricting prohibited use of the system. Where appropriate, such measures may also \ninclude monitoring for compliance with that restriction, depending on  the means \nthrough which the AI system is supplied and the information at the provider’s disposal \nfor possible misuse . Any possible monitoring measures to detect misuse should not \namount to a general monitoring of the activities of the deployers and should be in line \nwith Union law. For example, a general-purpose AI system that can recognise or infer emotions should \nnot be used by deployers at the workplaces or in education institutions , unless an \nexception for medical or safety reasons applies. However, the provider may not be in a \nposition to know the specific context in which the emotion recognition functionality of \nthe system will be used and whether an exception  to the prohibition in  Article 5(1)(f) \nAI Act may apply. Such providers may nevertheless explicitly exclude such prohibited \nuse in their terms of use and include appropriate information in the instructions of use \nto guide deployers."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education]\n\nThey are also expected to take appropriate measures if they become \naware that  the system is misused for this specific prohibited purpose by specific \ndeployers, for example, if such misuse is reported or  the provider becomes otherwise \naware, which may be the case if the system is directly operated through a platform under \nthe control of the provider and the provider performs checks. 2.8. Interplay between the prohibitions and other Union law  \n(42) The AI Act is a regulation that applies horizontally across all sectors without prejudice \nto other Union legislation, in particular on the protection of fundamental rights, \nconsumer protection, employment, the protection of workers, and product safety.37 The \nAI Act complements such legislation through its preventative and safety logic  (AI \nsystems may not be placed on the market  or used  in a certain way ) and  provides \nadditional protection by addressing specific harmful AI practices which may not be \nprohibited by other laws . Furthermore, by addressing the earlier stages of the AI \nsystems’ lifecycle  (i.e. the placing on the market and putting into service) and"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment]\n\nThey are also expected to take appropriate measures if they become \naware that  the system is misused for this specific prohibited purpose by specific \ndeployers, for example, if such misuse is reported or  the provider becomes otherwise \naware, which may be the case if the system is directly operated through a platform under \nthe control of the provider and the provider performs checks. 2.8. Interplay between the prohibitions and other Union law  \n(42) The AI Act is a regulation that applies horizontally across all sectors without prejudice \nto other Union legislation, in particular on the protection of fundamental rights, \nconsumer protection, employment, the protection of workers, and product safety.37 The \nAI Act complements such legislation through its preventative and safety logic  (AI \nsystems may not be placed on the market  or used  in a certain way ) and  provides \nadditional protection by addressing specific harmful AI practices which may not be \nprohibited by other laws . Furthermore, by addressing the earlier stages of the AI \nsystems’ lifecycle  (i.e. the placing on the market and putting into service) and"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment] [Article 5]\n\n14 \ndeployment (i.e. the use), the AI Act’s prohibitions enable action to be taken against \nharmful practices involving AI at various points in the AI value chain. (43) At the same time, the AI Act does not affect prohibitions that apply where an AI practice \nfalls within other Union law.38 Thus, even where an AI system is not prohibited by the \nAI Act, its use could still be prohibited or unlawful based on other primary or secondary \nUnion law (e.g., because of the failure to respect fundamental rights in a given case , \nsuch as the lack of a legal basis for the processing of personal data required under data \nprotection law , discrimination prohibited by Union law , etc.). The respect of the \nprohibitions in the AI Act are therefore not a sufficient condition for compliance with \nother Union legislation that remains applicable to providers and deployers of AI \nsystems. For example, AI-enabled emotion recognition systems used in the workplace that are \nexempted from the prohibition in Article 5(1)(f) AI Act, because they are used for \nmedical or safety reasons, remain subject to data protection law and Union and national \nlaw on employment and working conditions, including health and safety at work, which \nmay foresee other restrictions and safeguards in relation to the use of such systems.39 \n(44) When specific activities related to the placing on the market or use of AI systems are \nalso covered under other Union legislation, the AI Act aims to ensure the consistent \nimplementation of the different provisions. Moreover, it enables effective cooperation \nbetween the competent authorities responsible for the enforcement of the AI Act and \nthe authorities protecting fundamental rights pursuant to  Article 77 AI Act and other \nprovisions of the AI Act. More generally, in accordance with Article 4(3) TEU, the \nvarious authorities concerned are bound to cooperate sincerely when giving effect to \ntheir respective tasks under Union law."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n14 \ndeployment (i.e. the use), the AI Act’s prohibitions enable action to be taken against \nharmful practices involving AI at various points in the AI value chain. (43) At the same time, the AI Act does not affect prohibitions that apply where an AI practice \nfalls within other Union law.38 Thus, even where an AI system is not prohibited by the \nAI Act, its use could still be prohibited or unlawful based on other primary or secondary \nUnion law (e.g., because of the failure to respect fundamental rights in a given case , \nsuch as the lack of a legal basis for the processing of personal data required under data \nprotection law , discrimination prohibited by Union law , etc.). The respect of the \nprohibitions in the AI Act are therefore not a sufficient condition for compliance with \nother Union legislation that remains applicable to providers and deployers of AI \nsystems. For example, AI-enabled emotion recognition systems used in the workplace that are \nexempted from the prohibition in Article 5(1)(f) AI Act, because they are used for \nmedical or safety reasons, remain subject to data protection law and Union and national \nlaw on employment and working conditions, including health and safety at work, which \nmay foresee other restrictions and safeguards in relation to the use of such systems.39 \n(44) When specific activities related to the placing on the market or use of AI systems are \nalso covered under other Union legislation, the AI Act aims to ensure the consistent \nimplementation of the different provisions. Moreover, it enables effective cooperation \nbetween the competent authorities responsible for the enforcement of the AI Act and \nthe authorities protecting fundamental rights pursuant to  Article 77 AI Act and other \nprovisions of the AI Act. More generally, in accordance with Article 4(3) TEU, the \nvarious authorities concerned are bound to cooperate sincerely when giving effect to \ntheir respective tasks under Union law."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment] [Article 2]\n\n(45) In the context of the prohibitions , the interplay between the AI Act  and Union data \nprotection law is particularly relevant , since AI systems often process information \nrelating to identified or identifiable natural persons (‘personal data’) .40 Depending on \nthe prohibition and the context, the most relevant legal acts in relation to such systems \nare Regulation (EU) 2016/679 on the protection of natural persons with regard to the \nprocessing of personal data and on the free movement of such data  (General Data \nProtection Regulation, hereinafter ‘GDPR’), Directive (EU) 2016/680 on the protection \nof natural persons with regard to the pro cessing of personal data by competent \nauthorities for the purposes of the prevention, investigation, detection or prosecution of \ncriminal offences or the execution of criminal penalties, and on the free movement of \nsuch data  (Law Enforcement Directive, hereinafter ‘ LED’), and Regulation (EU) \n2018/1725 which lays down data protection rules for the EU Institutions, bodies, offices \nand agencies (hereinafter ‘EUDPR’). In accordance with Article 2(7) AI Act, these acts \nremain unaffected and will continue to apply alongside the AI Act, which is consistent \nand complementary to the EU data protection acquis. Several aspects of these EU data"}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 2]\n\n(45) In the context of the prohibitions , the interplay between the AI Act  and Union data \nprotection law is particularly relevant , since AI systems often process information \nrelating to identified or identifiable natural persons (‘personal data’) .40 Depending on \nthe prohibition and the context, the most relevant legal acts in relation to such systems \nare Regulation (EU) 2016/679 on the protection of natural persons with regard to the \nprocessing of personal data and on the free movement of such data  (General Data \nProtection Regulation, hereinafter ‘GDPR’), Directive (EU) 2016/680 on the protection \nof natural persons with regard to the pro cessing of personal data by competent \nauthorities for the purposes of the prevention, investigation, detection or prosecution of \ncriminal offences or the execution of criminal penalties, and on the free movement of \nsuch data  (Law Enforcement Directive, hereinafter ‘ LED’), and Regulation (EU) \n2018/1725 which lays down data protection rules for the EU Institutions, bodies, offices \nand agencies (hereinafter ‘EUDPR’). In accordance with Article 2(7) AI Act, these acts \nremain unaffected and will continue to apply alongside the AI Act, which is consistent \nand complementary to the EU data protection acquis. Several aspects of these EU data"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n15 \nprotection rules have been clarified by the CJEU and the European Data Protection \nBoard has adopted a series o f guidelines (e.g. on the noti on of ‘profiling’,41 which is \nparticularly relevant for the prohibition in Article 5(1)(d) AI Act, since it uses the same \nnotion). (46) Concerning the prohibitions/restrictions on the use of biometric categorisation systems \nand real-time RBI systems for law enforcement purposes, the AI Act applies as lex \nspecialis to Article 10 LED, thus regulating such use and the processing of biometric \ndata involved in an exhaustive manner.42 In that context, the AI Act is not intended to \nprovide the legal basis for the processing of personal data under Article  8 of Directive \n(EU) 2016/680. All other provisions of that Directive apply in addition to the conditions \nset out in  the AI Act , in particular for the use of real-time (RBI) systems for law \nenforcement purposes  when permitted , subject to the limited exceptions  in Article \n5(1)(h) AI Act. More generally, the LED must also be complied with for any processing \nof personal data by competent law enforcement authorities (i.e. competent authorities \nunder Article 3(7) LED) when they process the data for law enforcement purposes. (47) In accordance with Article 2(9) AI Act, EU consumer protection and safety legislation \nalso remain fully applicable to AI systems falling within scope of those acts. For example, \n- Social scoring practices by traders (including natural persons acting in a professional \ncapacity in business -to-consumer relations), subject to case -by-case assessment, may \nalso be considered  ‘unfair’ and therefore in breach of consumer law ( i.e. Directive \n2005/29/EC); \n- The use of an AI system to infer emotions may also have to comply with Regulation \n(EU) 2017/745  (Medical Device Regulation ) if the AI system is used for medical \ndiagnosis or medical treatment purposes."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 2]\n\n(48) Furthermore, the AI Act applies in conjunction with relevant obligations for providers \nof intermediary services that embed AI systems or models into their services regulated \nby Regulation (EU) 2022/2065 ( ‘the Digital Services Act ’). Specifically, Article 2(5) \nAI Act indicates that the AI Act does not affect the application of the provisions on the \nliability of such providers as set out in Chapter II of the Digital Services Act. (49) In addition, the prohibitions in the AI Act are without prejudice to any liability that the \nprovider or deployer might incur for the harm caused according to applicable Union or \nnational liability laws.43"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 5]\n\n23 \nBy contrast,  a generative AI system that incidentally presents false or misleading \ninformation and hallucinates66 may not be considered  to deploy deceptive techniques \nwithin the meaning of Article 5(1)(a) AI Act , taking into account the limitations and \nthe state of the art of generative AI . In particular , this may be the case where the \nprovider of the system has properly informed users about the system’s limitations and \nintegrated appropriate safeguards into the system to minimise such outcomes  and \nprovided that the system is not intended for, nor deployed in, sensitive contexts (e.g., \nhealth, education, elections) where serious harmful consequences are likely to occur \n(see also considerations in section 3.2.3.c) below). d) Combination of techniques \n(74) Article 5(1)(a) AI Act applies to subliminal, purposefully manipulative , or deceptive \ntechniques, or to combination s of such techniques that can have a compound impact. As stated above, purposefully manipulative techniques may be also subliminal in \nnature, if they operate beyond the threshold of conscious awareness. (75) Furthermore, when purposefully manipulative and deceptive techniques are applied in \ncombination, this may significantly  influence the behaviour of individuals , leading \nthem to make decisions based on unconscious manipulations and false beliefs . This \ncombination may create a feedback loop where individuals are less likely to question \nor critically evaluate the information received, since the manipulative elements have \nalready primed their cognitive biases and emotional responses. 3.2.2. With the objective or the effect of materially distorting the behaviour of \na person or a group of persons \n(76) A third condition for the prohibition in Article 5(1) (a) AI Act to apply is that the \ndeployed subliminal, purposefully manipulative, or deceptive technique must have ‘the \nobjective, or the effect of materially distorting the behaviour  of a person or a group of \npersons’."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 5]\n\nThis implies a substantial impact on the behaviour where a person’s autonomy \nand free choices are undermined, rather than a minor influence. However, intent is not \na necessary requirement , since Article 5(1)(a) AI Act also covers practices that may \nonly have the ‘effect’ of causing material distortion. There should be a \nplausible/reasonably likely causal link between the potential material distortion of the \nbehaviour and the subliminal, purposefully manipulative or deceptive technique \ndeployed by the AI system. a) The concept of ‘material distortion of the behaviour’ \n(77) The concept of ‘material distortion of the behaviour’ of a person or a group of persons \nis central to Article 5(1)(a)  AI Act . It involves the deployment of subliminal, \npurposefully manipulative or deceptive techniques that are capable of influencing \npeople’s behaviour in a manner that  appreciably impairs their ability to make an"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education]\n\n82 See in particular Articles 24, 25 and 26 of the Charter. See also United Nations Educational, Scientific and Cultural Organiz ation \n(UNESCO) Recommendation on the Ethics of Artificial Intelligence (2021) which emphasises incl usivity and fairness in AI \ndevelopment and deployment. It calls for special attention to vulnerable groups, including children, older people, and people  with \ndisabilities."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment] [Article 5]\n\n35 \n- A therapeutic chatbot aimed to provide mental health support and coping strategies to \npersons with mental disabilities can exploit their limited intellectual capacities to \ninfluence them to buy expensive medical products or nudge them to behave in ways \nthat are harmful to them or other persons. - AI systems can identify women and young girls with disabilities online with sexually \nabusive content and targets them with more effective grooming practices, thus \nexploiting their impairments and vulnerabilities that make them more susceptible to \nmanipulation and abuse and less capable of protecting themselves. By contrast, AI applications that are not designed in an accessible manner should not \nbe regarded to exploit vulnerabilities of persons with disabilities since they do not \nspecifically target those vulnerabilities, but are simply inaccessible to the persons with \ndisabilities. c) Specific social or economic situation  \n(109) The third category of vulnerabilities which the prohibition in Article 5(1) (b) AI Act \nseeks to protect are those due to a specific social or economic situation that is likely to \nmake the persons concerned more vulnerable to exploitation . ‘Specific’ should not be \ninterpreted in this context as a unique individual characteristic, but rather a legal status \nor membership to a specific vulnerable social or economic group. Recital 29 AI Act \ncontains a non-exhaustive list of examples of such situations, such as persons living in \nextreme poverty and ethnic or religious minorities. The category aims to cover , in \nprinciple, relatively stable and long-term characteristics, but transient circumstances, \nsuch as temporary unemployment, over-indebtedness or migration status, may also be \ncovered as a specific soci al or -economic situa tion."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Migration] [Article 5]\n\n35 \n- A therapeutic chatbot aimed to provide mental health support and coping strategies to \npersons with mental disabilities can exploit their limited intellectual capacities to \ninfluence them to buy expensive medical products or nudge them to behave in ways \nthat are harmful to them or other persons. - AI systems can identify women and young girls with disabilities online with sexually \nabusive content and targets them with more effective grooming practices, thus \nexploiting their impairments and vulnerabilities that make them more susceptible to \nmanipulation and abuse and less capable of protecting themselves. By contrast, AI applications that are not designed in an accessible manner should not \nbe regarded to exploit vulnerabilities of persons with disabilities since they do not \nspecifically target those vulnerabilities, but are simply inaccessible to the persons with \ndisabilities. c) Specific social or economic situation  \n(109) The third category of vulnerabilities which the prohibition in Article 5(1) (b) AI Act \nseeks to protect are those due to a specific social or economic situation that is likely to \nmake the persons concerned more vulnerable to exploitation . ‘Specific’ should not be \ninterpreted in this context as a unique individual characteristic, but rather a legal status \nor membership to a specific vulnerable social or economic group. Recital 29 AI Act \ncontains a non-exhaustive list of examples of such situations, such as persons living in \nextreme poverty and ethnic or religious minorities. The category aims to cover , in \nprinciple, relatively stable and long-term characteristics, but transient circumstances, \nsuch as temporary unemployment, over-indebtedness or migration status, may also be \ncovered as a specific soci al or -economic situa tion."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment] [Article 5]\n\nHowever, situations such as \ngrievances or loneliness that may be experienced by any person are not covered, since \nthey are not s pecific from a socio-economic perspective  (their exploitation may be \ncovered though under Article 5(1)(a) AI Act). (110) Persons in disadvantaged social or economic situations are usually more vulnerable and \nhave fewer resources  and lower digital literacy  than the general population , which \nmakes it harder for them to discern or counteract exploitative AI practices. Article \n5(1)(b) AI Act  aims to ensure that AI technologies do not perpetuate or exacerbate \nexisting financial and other social inequalities and injustices  by exploiting  the \nvulnerabilities of those people. For example, an AI-predictive algorithm can be used to target with advertisements for \npredatory financial products people who live in low-income post-codes and are in a dire \nfinancial situation, thus exploiting their susceptibility to such advertisements because \nof possible despair and causing them significant financial harm. By contrast, an AI system that is inadvertently biased and disproportionately impacts \nsocially disadvantaged persons (indirect discrimination)  due to biased training data \nshould not automatically be considered to exploit persons’ socio-economic \nvulnerabilities, since the y are not specifically targeted  as in the case of direct \ndiscrimination when such targeting is a deliberate feature of the system’s design of the"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Migration] [Article 5]\n\nHowever, situations such as \ngrievances or loneliness that may be experienced by any person are not covered, since \nthey are not s pecific from a socio-economic perspective  (their exploitation may be \ncovered though under Article 5(1)(a) AI Act). (110) Persons in disadvantaged social or economic situations are usually more vulnerable and \nhave fewer resources  and lower digital literacy  than the general population , which \nmakes it harder for them to discern or counteract exploitative AI practices. Article \n5(1)(b) AI Act  aims to ensure that AI technologies do not perpetuate or exacerbate \nexisting financial and other social inequalities and injustices  by exploiting  the \nvulnerabilities of those people. For example, an AI-predictive algorithm can be used to target with advertisements for \npredatory financial products people who live in low-income post-codes and are in a dire \nfinancial situation, thus exploiting their susceptibility to such advertisements because \nof possible despair and causing them significant financial harm. By contrast, an AI system that is inadvertently biased and disproportionately impacts \nsocially disadvantaged persons (indirect discrimination)  due to biased training data \nshould not automatically be considered to exploit persons’ socio-economic \nvulnerabilities, since the y are not specifically targeted  as in the case of direct \ndiscrimination when such targeting is a deliberate feature of the system’s design of the"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Healthcare] [Article 5]\n\n39 \nCertain AI-enabled differential pricing practices in key services such as insurance that  \nexploit the specific soci al or economic situation and provide higher prices to lower \nincome consumers can lead to a significant financial burden to pay more for the same \ncoverage, leaving them vulnerable to shocks.92 \n(118) Persons with disabilities  also represent a vulnerable group that exploitative and \nmanipulative AI systems may significantly harm. For instance, an AI system that uses emotion recognition to support mentally disabled \nindividuals in their daily life may also manipulate them into making harmful decisions, \nlike purchasing products promising unrealistic mental health benefits . This is likely to \nworsen their mental health condition and financially exploit them through the purchase \nof i neffective and expensive products , which is  likely to cause them significant \npsychological and financial harms. (119) Socially or economically disadvantaged individuals are particularly susceptible to AI \nsystems exploiting their financial desperation  and precarious social situation and are \noften less informed and digitally literate. For instance, an AI chatbot could target specific socially or economically disadvantaged \ngroups inciting them to commit acts of violence or injuries of other persons, by \nidentifying their heightened susceptibility to certain types of content, fear -based \nnarratives, or exploitative offers. The system’s targeted approach exacerbates the \nexisting vulnerabilities of these socio -economically disadvantaged individuals, \ndeepening their challenges . In certain cases this may  lead to increased anxiety, \ndepression, feelings of helplessness, social isolation, or self-harm and radicalisation to \na point that reaches the threshold of significant harm under Article 5(1)(b) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Healthcare] [Article 5]\n\n(120) Unlike Article 5(1)(a) AI Act, Article 5(1)(b) AI Act does not explicitly refer to group \nharms, while recital 29 AI Act refers for both pr ohibitions to harms suffered by both \nspecific persons and groups of individuals . The two prohibitions should  thus be \ninterpreted in a consistent manner aligned also with the safety logic of the AI Act and \nthe objective of the prohibition in Article 5(1)(b) to protect all individuals belonging to \nthe specific vulnerable groups  due to age, disability and  specific social or -economic \nsituation. Harms that can be extern alised and affect other persons, even if not directly \naffected by the system, should therefore also be taken into account in the assessment of \nthe significance of the harm under Article 5(1)(b) AI Act. For instance, \n- The AI-enabled exploitation of children’s vulnerabilities may have long-term societal \nimpacts, including increased prevalence of mental health concerns, healthcare costs, \nand reduced productivity due to chronic health issues. - An AI system exploiting the financial vulnerabilities of economically disadvantaged \npeople may lead to financial exclusion and creat e a downward spiral of socio -"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment]\n\n95 Particularly relevant in this respect is the Judgment of the Court (Grand Chamber) of 4 July 2023, Case C-252/21 Meta Platforms Inc \nand Others v Bundeskartellamt. Although the CJEU finds, inter alia, that the processing of off-service users’ personal data for direct \nmarketing purposes by a large social network platform may be regarded as carried out for a legitimate interest of the controller, this \ncannot be done without consent from a user as a legal basis due to the interests and fundamental rights of such a user, which under \nthe circumstances of that case, in particular the extensive processing, override the interest of that operator in such personalised \nadvertising through which social platforms finance their activities (see the Meta Platforms judgment, paragraphs 115 to 118). 96 E.g. Council Directive 2000/43/EC of 29 June 2000 implementing the principle of equal treatment between persons irrespective of \nracial or ethnic origin OJ L 180, 19.7.2000, p. 22 –26; Council Directive 2000/78/EC of 27 November 2000 establishing a general \nframework for equal treatment in employment and occupation OJ L 303, 2.12.2000, p. 16–22; Directive 2006/54/EC of the European \nParliament and of the Council of 5 July 2006 on the implementation of the principle of equal opportunities and equal treatmen t of \nmen and women in matters of employment and occupation (recast), OJ L 204, 26.7.2006, p. 23–36; Council Directive 2004/113/EC \nof 13 December 2004 implementing the principle of equal treatment between men and women in the access to and supply of goods \nand services, OJ L 373, 21.12.2004, p. 37–43."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment]\n\n52 \n(158) The second scenario is where the scoring is based on personal or personality \ncharacteristics, which may or may not involve specific social behavioural aspects. ‘Personal characteristics’ may include a variety of information relating to a person, for \nexample sex, sexual orientation or sexual characteristics, gender, gender identity, race, \nethnicity, family situation, address, income, household  members, profession, \nemployment or other legal status, performance at work, economic situation,  financial \nliquidity, health, personal preferences, interests, reliability, behaviour, location or \nmovement, level of debt , type of car  etc.117 ‘Personality characteristics’ should be in \nprinciple interpreted as synonymous with personal characteristics, but may also imply \nthe creation of  specific profiles of individuals as personalities . Personality \ncharacteristics may be also based on a number of factors and imply a judgement, which \nmay be made by the individuals themselves, other persons, or generated by AI systems. In the AI Act, personality characteristics are sometimes referred to as personality traits \nand characteristics;118 those concepts should be interpreted consistently. (159) ‘Known, inferred or predicted’ personal or personality characteristics are different types \nof information and personal data that need to be distinguished. ‘Known \ncharacteristics’ are based on information which has been provided to the AI system as \nan input, and which is  in most cases verifiable information . By contrast,  ‘inferred \ncharacteristics’ are based on information which has been inferred from other \ninformation, with the inference usually made by an AI system. ‘Predicted \ncharacteristics’ are those which are estimated based on patterns with less than 100% \naccuracy."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment] [Article 5]\n\nThe concepts of ‘inferred’ (or derived) data are also used  in the context of \nprofiling in Union data protection law and may therefore be a source of inspiration for \ninterpreting those concepts used in Article 5(1)(c) AI Act.119 The use of these different \ntypes of data may have different implications for the accuracy and the fairness of the \nscoring practices and therefore may be taken into account, in particular where the \nprocessing is opaque or relies on data points whose accuracy is more difficult to be \nverified. 4.2.2. The social score must lead to detrimental or unfavourable treatment in \nunrelated social context s and/or unjustified or disproportionate  \ntreatment to the gravity of the social behaviour \na) Causal link between the social score and the treatment  \n(160) For the prohibition in Article 5(1)(c) AI Act to apply, the social score created by or with \nthe assistance of an AI system must lead to a detrimental or unfavourable treatment \nfor the evaluated person or group of persons. In other words, the treatment must be the \nconsequence of the score, and the score the cause of the treatmen t. Such a plausible \ncausal link may also exist in cases where the harmful consequences have not yet \nmaterialised but the AI system is intended to or capable of producing such an adverse"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 5]\n\n55 \nworkplace, etc.122 By contrast, data that is relevant for the allocation of the benefits \nand lawfully collected could be used to determine the risk of fraud , since public \nauthorities pursue a legitimate aim in  verifying if social benefits are  correctly \nallocated. - A public labour agency uses an AI system to score unemployed individuals based on \nan interview and a n AI-based assessment for determining whether an individual \nshould benefit from state support for employment . That score is  based on relevant \npersonal characteristics, such as age  and education, but also variables collected or \ninferred from data and contexts with no apparent connection to the purpose  of \nevaluation, such as marital status, health data for chronic diseases, addiction, etc.123  \nThese unacceptable scoring practices may be distinguished from l awful practices that \nevaluate persons for  specific purpose in compliance with Union and national law , in \nparticular when such laws, in compliance with EU law, specify the data considered as \nrelevant and necessary for the purposes of evaluation (see section 4.3. out of scope). Scenario 2: Unfavourable or detrimental treatment disproportionate to the social behaviour \n(167) Another alternative scenario under Article 5(1)(c)(ii) AI Act  where an AI scoring \nsystem may be prohibited  is if the treatment resulting from the score  is unjustified or \ndisproportionate to the gravity of the social behaviour. The severity of the impact and \nthe interference with the fundamental rights of the person concerned resulting from the \nsocial scoring compared to the gravity of the social behaviour of the person should \ndetermine whether such treatment is disproportionate  for the legitimate aim pursued , \ntaking into account the general principle of proportionality."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment] [Article 5]\n\n55 \nworkplace, etc.122 By contrast, data that is relevant for the allocation of the benefits \nand lawfully collected could be used to determine the risk of fraud , since public \nauthorities pursue a legitimate aim in  verifying if social benefits are  correctly \nallocated. - A public labour agency uses an AI system to score unemployed individuals based on \nan interview and a n AI-based assessment for determining whether an individual \nshould benefit from state support for employment . That score is  based on relevant \npersonal characteristics, such as age  and education, but also variables collected or \ninferred from data and contexts with no apparent connection to the purpose  of \nevaluation, such as marital status, health data for chronic diseases, addiction, etc.123  \nThese unacceptable scoring practices may be distinguished from l awful practices that \nevaluate persons for  specific purpose in compliance with Union and national law , in \nparticular when such laws, in compliance with EU law, specify the data considered as \nrelevant and necessary for the purposes of evaluation (see section 4.3. out of scope). Scenario 2: Unfavourable or detrimental treatment disproportionate to the social behaviour \n(167) Another alternative scenario under Article 5(1)(c)(ii) AI Act  where an AI scoring \nsystem may be prohibited  is if the treatment resulting from the score  is unjustified or \ndisproportionate to the gravity of the social behaviour. The severity of the impact and \nthe interference with the fundamental rights of the person concerned resulting from the \nsocial scoring compared to the gravity of the social behaviour of the person should \ndetermine whether such treatment is disproportionate  for the legitimate aim pursued , \ntaking into account the general principle of proportionality."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 5]\n\nThis requires a case-by-\ncase assessment, which should consider all relevant circumstances of the case, as well \nas general ethical considerations and principles for fairness and social justice related to \nthe assessment of the social behaviour and the proportionality of the detrimental \ntreatment. The treatment may also be ‘unjustified’, such as lacking a legitimate aim. Sectoral Union or national legislation setting specific criteria and procedures that \nregulate such potential detrimental or unfavourable treatment may also be relevant as \npart of this assessment. Examples of unjustified or disproportionate treatment compared to the social behaviour \nprohibited under Article 5(1)(c) ii) AI Act \n- A public agency uses an AI system to profile families for early detection of children \nat risk based on criteria such as parental mental health  and unemployment, but also  \ninformation on parents’ social behaviour derived from multiple contexts. Based on the \nresulting score, families are singled out for inspection and children considered ‘at risk’"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment] [Article 5]\n\nThis requires a case-by-\ncase assessment, which should consider all relevant circumstances of the case, as well \nas general ethical considerations and principles for fairness and social justice related to \nthe assessment of the social behaviour and the proportionality of the detrimental \ntreatment. The treatment may also be ‘unjustified’, such as lacking a legitimate aim. Sectoral Union or national legislation setting specific criteria and procedures that \nregulate such potential detrimental or unfavourable treatment may also be relevant as \npart of this assessment. Examples of unjustified or disproportionate treatment compared to the social behaviour \nprohibited under Article 5(1)(c) ii) AI Act \n- A public agency uses an AI system to profile families for early detection of children \nat risk based on criteria such as parental mental health  and unemployment, but also  \ninformation on parents’ social behaviour derived from multiple contexts. Based on the \nresulting score, families are singled out for inspection and children considered ‘at risk’"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 5]\n\n56 \nare taken from their families, including in cases of minor transgressions by the parents, \nsuch as occasionally missing doctors’ appointments or receiving traffic fines. - A municipality use s an AI system to score trustworthiness  of residents  based on \nmultiple data points related to their social behaviour in a variety of contexts. The \ngenerated score for residents considered ‘less trustworthy’ is used for blacklisting, i.e. withdrawal of public benefits, other serious punitive measures, and increased control \nor surveillance. Among the factors  considered in the assessment are insufficient \nvolunteering and minor misbehaviour, such as not returning books to the library on \ntime, leaving rubbish on the street outside the day of collection , and a delay in the \npayment of local taxes. These unacceptable social scoring practices may be distinguished from lawful practices \nthat evaluate persons for a legitimate specific purpose in compliance with Union and \nnational law, in particular where those laws ensure that detrimental or unfavourable \ntreatment is justified and proportionate to the social behaviour (see section 4.3. out of \nscope). (168) Both alternatives  under Article 5(1) (c)(i) and (ii) AI Act  may also be fulfilled \nsimultaneously. Examples of unjustified or disproportionate treatment under Article 5(1)(c)) i) and ii) \nAI Act \n- A tax authorit y uses an AI system to detect child benefit fraud by profiling and \nassigning beneficiaries suspected of fraud to categories such as ‘deliberate intent/gross \nnegligence’ using criteria such as low income, dual nationality, social behaviour, etc. Based on the risk score, a beneficiary’s file is inspected and, in many cases,  their \nchildcare benefit ceased, they receive notice to repay the received benefits, and no \nlonger qualify for standard debt collection arrangements."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education]\n\nSuch scoring causes many \nfamilies to be heavily indebted and lead s to unjust, discriminatory and detrimental \ntreatment of individuals and groups of individuals ,124 driving many families into \nsevere financial hardship. - A public authority uses an AI system  to control fraud in the student housing grant \nprocess that considers among the indicators the internet connections, family status or \nlevel of education  of beneficiaries as distinguishing factor s for fraud risk, which do \nnot seem relevant, nor justified. - A government introduces a comprehensive AI -based system that monitors and rates \ncitizens based on their behaviour in various aspects of life, such as social interactions, \nonline activit ies, purchasing habits , and punctuality in paying bills . People with a \nlower score face restricted access to public services, higher interest rates on loans, and \ndifficulty in traveling, renting apartments, and even finding jobs. The system leads to \nexcessive surveillance of individuals and detrimental treatment in contexts unrelated"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment] [Article 5]\n\n57 \nto the social behaviour used to determine the social score  (e.g., job opportunities are \ninfluenced by social media activity), while also imposing excessive penalties for \nminor infractions ( e.g., social and financial disadvantages for relatively minor \noffences). These unacceptable social scoring practices may be distinguished from lawful practices \nevaluating persons for legitimate specific purposes that do not fulfil th ese conditions \nand are in compliance with Union and national law, in particular when those laws ensure \nthat detrimental or unfavourable treatment is justified and proportionate and data from \nrelated social contexts is used (see section 4.3. out of scope). (169) The prohibition under Article 5(1)(c) AI Act may also cover cases where awards or \npreferential treatment are given to certain individuals  or groups of persons , since this \nimplies less favourable treatment of other individuals (e.g., in cases of support \nemployment programmes, (de-)prioritisation for housing or resettlement). 4.2.3. Regardless of whether provided or used by public or private persons \n(170) As already noted , Article 5(1)(c) AI Act prohibits unacceptable AI -enabled social \nscoring practices regardless of whether the AI system or the score are provided or used \nby public or private persons. While scoring in the public sector may have very \nsignificant consequences for people due to an imbalance of power and a dependence on \npublic services, simil arly harmful consequences may also occur in the private sector, \nwhere scoring practices are also increasingly undertaken by companies and other \nentities. For example, \n- An insurance company collects spending and other financial information from a bank \nwhich is unrelated to the determination of eligibility of candidates for life insurance \nand which is used to determine the price of the premium to be paid for such insurance."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment]\n\nAn AI system analyses this information  and recommends, on that basis, whether to \nrefuse a contract or set higher life insurance premiums for a particular individual or a \ngroup of customers. - A private credit agency uses an AI system to determine the creditworthiness of people \nand decide whether an individual should obtain a loan for housing based on unrelated \npersonal characteristics. These unacceptable social scoring practices may be distinguished from lawful practices \nevaluating persons for spec ific legitimate purposes that do not fulfil these conditions \nand are in compliance with Union and national law, in particular when those laws ensure \nthat detrimental or unfavourable treatment is justified and proportionate and data from \nrelated social contexts is used (see section 4.3. out of scope). (171) In the case of checks by competent market surveillance authorities, it is on the provider \nand the deployer of the AI system, each within their responsibilities, to demonstrate that \nthe AI practice is legitimate and justified, including by providing transparency of the \nfunctioning of the AI system, and information about the types of data and data sources,"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Migration] [Article 5]\n\n58 \nensuring that only data related to the social context in which the score is used  are \nprocessed for the purpose of the evaluation or classification, the system is performing \nas intended, and any resulting detrimental or unfavourable treatment is justified and \nproportionate to the social behaviour. Compliance with applicable legislation and \nappropriate and proportionate safeguards built in the system and applied during its \noperation will help to avoid the prohibition from applying, while enabling the use of AI \nsystems for the evaluation or classification  of persons for legitimate and beneficial \npurposes (e.g., to improve the effectiveness of processes, quality of service, safety, etc.) \n(see section 4.3. out of scope). (172) Compliance with the requirements for high-risk AI systems (e.g. in the area of essential \npublic services and benefits, credit-scoring and creditworthiness assessment, migration \netc.) may also help to ensure that AI systems used for evaluation and classification  \npurposes in those high-risk areas do not constitute unacceptable social scoring practices \nthat providers and deployers should consider when implementi ng their respective \nobligations (e.g. on risk management, transparency, data governance, fundamental \nrights impact assessment, human oversight, monitoring, etc.). 4.3. Out of scope  \n(173) The prohibition in Article 5(1)(c) AI Act only applies to the scoring of natural persons \nor groups of persons, thus excluding in principle scoring of legal entities where the \nevaluation is not based on personal or personality characteristics or social behaviour of \nindividuals, even if in some cases individuals may be indirectly impacted  by the score \n(e.g., all citizens in a municipality in case of allocation of budget)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Migration] [Article 5]\n\nHowever, if legal \nentities have been evaluated based on an overall score that aggregates the evaluation or \nclassification of a group of natural persons based on their social behaviour or personal \nor personality characteristics and this score directly affects th ose persons (e.g. all \nemployees in a company, students in a specific school whose behaviour has been \nevaluated), the practice may fall within the scope of Article 5(1) (c) AI Act if all other  \nconditions are fulfilled. This will depend on a case-by-case assessment. (174) AI-based social scoring  as a ‘probabilistic value ’ and prognosis should also be \ndistinguished from individual ratings by users which assess the quality of a service \n(such as a driver in an online car-sharing platform or a host in an online platform for \naccommodation). Such ratings are the mere aggregation of individual human scores that \ndo not necessarily involve AI, unless the data are combined with other information and \nanalysed by  the AI system  for evaluating or classifying individuals fulfilling all \nconditions in Article 5(1)(c) AI Act. (175) Furthermore, the scoring of natural persons is not at all times prohibited, but only in the \nlimited cases where all of the conditions of Article 5(1)(c) AI Act  are cumulatively \nfulfilled, as analysed above . Recital 31  AI Act , in particular mentions that the \nprohibition ‘should not affect lawful evaluation practices of natural persons that are \ncarried out for a spec ific purpose in accordance wi th Union and national law’. For \nexample, credit scoring and risk scoring  and underwriting are essential aspects of the \nservices of financial and insurance businesses. Such practices , as well as other"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n59 \nlegitimate practices (i.e. to improve the quality and efficiency of services , to ensure \nmore efficient claims handling , to perform specific employee evaluations , fraud \nprevention and detection,  law enforcement or scoring of users’ behaviour on online \nplatforms), are not per se prohibited, if lawful and undertaken in line with the AI Act \nand other applicable Union law and national law, which must comply with Union law. (176) In other words, AI systems which evaluate or classify individuals for the purposes of \ngenerating a social score in a lawful manner and for a specific purpose in the related \ncontext as that in which the personal data used for the score  were collected are not \nprohibited, provided that any detrimental or unfavourable  treatment from using the \nscore is justified and proportionate to the gravity of the social behaviour.125 \n(177) Compliance with sectoral Union legislation, such as in the field of credit-scoring, anti-\nmoney laundering, etc., that specifies the type of data that can be used as relevant and \nnecessary for the specific legitimate purpose of evaluation and ensures that the \ntreatment is justified and proportionate to the social behaviour may thus ensure that the \nAI practice falls outside the scope of the prohibition in Article 5(1)(c) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nExamples of legitimate scoring practices in line with Union and national law  that are \noutside the scope of Article 5(1)(c) AI Act: \n- Financial credit scoring systems used by creditors or credit information agencies to \nassess a customer’s financial creditworthiness or outstanding debts, providing a credit \nscore or determining their creditworthiness assessment , which are based on the \ncustomer’s income and expenses and other financial and economic circumstances, are \nout of scope of Article 5(1)(c) AI Act if they are relevant for the legitimate purpose of \nthe credit scoring and if they comply with consumer protection laws126 specifying the \ntype of data and the necessary safeguards to ensure the fair treatment of consumers in \ncreditworthiness assessments. - Companies have a legitimate interest to evaluate customers for financial fraud and \nthose practices are not affected by the prohibition, if the evaluation is based on relevant \ndata such as transactional behaviour and metadata in the context of the services, past \nhistory and other factors from sources that are objectively relevant to determine the \nrisk of fraud and if the detrimental treatment is justified and proportionate as a \nconsequence of the fraudulent behaviour. - Information collected through telematic devices that show that a driver is speeding or \nnot maintaining safe driving practices used by an insurer that offers telematics-based \ntariffs in relation to  a policyholder ’s high -risk driving behaviour  may be used  to \nincrease the premium of that policyholder due to the higher risk of an accident caused \nby that driving behaviour, provided the increase in the premium is proportionate to the \nrisky behaviour of the driver."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment] [Article 5]\n\n60 \n- The collection and processing of data that is relevant and necessary for the intended \nlegitimate purpose of the AI systems (e.g. hea lth and schizophrenic data collected \nfrom various sources to diagnose patients) is out of scope of Article 5(1)(c) AI Act, in \nparticular because it process relevant and necessary data and typically does not entail \nunjustified detrimental or unfavourable treatment of certain natural persons. - Online platforms profiling users for safety reasons on their services based on data \nwhich is relevant for the context and purpose of assessment is out of scope of Article \n5(1)(c) AI Act , when the evaluation does not result in detrimental treatment that is \ndisproportionate to the gravity of the user’s misbehaviour. - AI-enabled targeted commercial advertising is out of scope , where it is  based on \nrelevant data (e.g., users’ preferences), it is done in line with Union law on consumer \nprotection, data protection and digital services, and it does not result in detrimental or \nunfavourable treatment disproportionate to the gravity of the user’s social behaviour  \n(e.g., exploitative and unfair differential pricing). - AI systems using data collected in refugee camps ( e.g., behavioural compliance) for \ndecisions about resettlement or employment is not affected by the prohibition , given \nthat this data is relevant for the purpose of assessment and provided that the procedures \nunder applicable Union migration law are fulfilled to ensure the treatment is justified \nand proportionate. - AI-enabled scoring by an online shopping platform which offers privileges to users \nwith a strong purchase history and a low rate of product returns, such as a faster returns \napplication process or returnless refunds  are out of scope of Article 5(1)(c) AI Act , \ngiven that the advantages are justified and proportionate to reward positive behaviour \nand other users continue to have access to the standard return process."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n60 \n- The collection and processing of data that is relevant and necessary for the intended \nlegitimate purpose of the AI systems (e.g. hea lth and schizophrenic data collected \nfrom various sources to diagnose patients) is out of scope of Article 5(1)(c) AI Act, in \nparticular because it process relevant and necessary data and typically does not entail \nunjustified detrimental or unfavourable treatment of certain natural persons. - Online platforms profiling users for safety reasons on their services based on data \nwhich is relevant for the context and purpose of assessment is out of scope of Article \n5(1)(c) AI Act , when the evaluation does not result in detrimental treatment that is \ndisproportionate to the gravity of the user’s misbehaviour. - AI-enabled targeted commercial advertising is out of scope , where it is  based on \nrelevant data (e.g., users’ preferences), it is done in line with Union law on consumer \nprotection, data protection and digital services, and it does not result in detrimental or \nunfavourable treatment disproportionate to the gravity of the user’s social behaviour  \n(e.g., exploitative and unfair differential pricing). - AI systems using data collected in refugee camps ( e.g., behavioural compliance) for \ndecisions about resettlement or employment is not affected by the prohibition , given \nthat this data is relevant for the purpose of assessment and provided that the procedures \nunder applicable Union migration law are fulfilled to ensure the treatment is justified \nand proportionate. - AI-enabled scoring by an online shopping platform which offers privileges to users \nwith a strong purchase history and a low rate of product returns, such as a faster returns \napplication process or returnless refunds  are out of scope of Article 5(1)(c) AI Act , \ngiven that the advantages are justified and proportionate to reward positive behaviour \nand other users continue to have access to the standard return process."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Migration] [Article 5]\n\n60 \n- The collection and processing of data that is relevant and necessary for the intended \nlegitimate purpose of the AI systems (e.g. hea lth and schizophrenic data collected \nfrom various sources to diagnose patients) is out of scope of Article 5(1)(c) AI Act, in \nparticular because it process relevant and necessary data and typically does not entail \nunjustified detrimental or unfavourable treatment of certain natural persons. - Online platforms profiling users for safety reasons on their services based on data \nwhich is relevant for the context and purpose of assessment is out of scope of Article \n5(1)(c) AI Act , when the evaluation does not result in detrimental treatment that is \ndisproportionate to the gravity of the user’s misbehaviour. - AI-enabled targeted commercial advertising is out of scope , where it is  based on \nrelevant data (e.g., users’ preferences), it is done in line with Union law on consumer \nprotection, data protection and digital services, and it does not result in detrimental or \nunfavourable treatment disproportionate to the gravity of the user’s social behaviour  \n(e.g., exploitative and unfair differential pricing). - AI systems using data collected in refugee camps ( e.g., behavioural compliance) for \ndecisions about resettlement or employment is not affected by the prohibition , given \nthat this data is relevant for the purpose of assessment and provided that the procedures \nunder applicable Union migration law are fulfilled to ensure the treatment is justified \nand proportionate. - AI-enabled scoring by an online shopping platform which offers privileges to users \nwith a strong purchase history and a low rate of product returns, such as a faster returns \napplication process or returnless refunds  are out of scope of Article 5(1)(c) AI Act , \ngiven that the advantages are justified and proportionate to reward positive behaviour \nand other users continue to have access to the standard return process."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment] [Article 5]\n\n- AI-evaluation and scoring of individuals by police and other law enforcement \nauthorities that collect data about individuals’ social behaviour from multiple contexts \nare out of scope of Article 5(1)(c) AI Act where those data are relevant for the specific \npurposes of the prevention, detection, prosecution and punishment of criminal \noffences, and where the detrimental treatment is justified and proportionate in \naccordance with substantive and procedural Union and national criminal and police \nlaw. It is also relevant to consider in this context the prohibition in Article 5(1)(d) AI \nAct, which  imposes additional and more specific conditions for AI -enabled risk \nassessments and prediction s of the likelihood of a person committing a criminal \noffence which must not be solely based on profiling or the assessment of personality \ntraits (see section 5). 4.4. Interplay with other Union legal acts \n(178) Providers and deployers should carefully assess whether other applicable Union and \nnational legislation applies to any particular AI scoring system used in their activities, \nin particular if there is more specific legislation that strictly regulates the types of data \nthat can be used as relevant and necessary for specific evaluation purposes and if there \nare more specific rules and procedures to ensure justified and fair treatment."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n- AI-evaluation and scoring of individuals by police and other law enforcement \nauthorities that collect data about individuals’ social behaviour from multiple contexts \nare out of scope of Article 5(1)(c) AI Act where those data are relevant for the specific \npurposes of the prevention, detection, prosecution and punishment of criminal \noffences, and where the detrimental treatment is justified and proportionate in \naccordance with substantive and procedural Union and national criminal and police \nlaw. It is also relevant to consider in this context the prohibition in Article 5(1)(d) AI \nAct, which  imposes additional and more specific conditions for AI -enabled risk \nassessments and prediction s of the likelihood of a person committing a criminal \noffence which must not be solely based on profiling or the assessment of personality \ntraits (see section 5). 4.4. Interplay with other Union legal acts \n(178) Providers and deployers should carefully assess whether other applicable Union and \nnational legislation applies to any particular AI scoring system used in their activities, \nin particular if there is more specific legislation that strictly regulates the types of data \nthat can be used as relevant and necessary for specific evaluation purposes and if there \nare more specific rules and procedures to ensure justified and fair treatment."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Migration] [Article 5]\n\n- AI-evaluation and scoring of individuals by police and other law enforcement \nauthorities that collect data about individuals’ social behaviour from multiple contexts \nare out of scope of Article 5(1)(c) AI Act where those data are relevant for the specific \npurposes of the prevention, detection, prosecution and punishment of criminal \noffences, and where the detrimental treatment is justified and proportionate in \naccordance with substantive and procedural Union and national criminal and police \nlaw. It is also relevant to consider in this context the prohibition in Article 5(1)(d) AI \nAct, which  imposes additional and more specific conditions for AI -enabled risk \nassessments and prediction s of the likelihood of a person committing a criminal \noffence which must not be solely based on profiling or the assessment of personality \ntraits (see section 5). 4.4. Interplay with other Union legal acts \n(178) Providers and deployers should carefully assess whether other applicable Union and \nnational legislation applies to any particular AI scoring system used in their activities, \nin particular if there is more specific legislation that strictly regulates the types of data \nthat can be used as relevant and necessary for specific evaluation purposes and if there \nare more specific rules and procedures to ensure justified and fair treatment."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n62 \n(184) Article 5(1)(d) AI Act prohibits AI systems assessing or predicting the risk of a natural \nperson committing a criminal offence based solely on profiling or assessing personality \ntraits and characteristics. (185) The provision indicates, in its last phrase, that the  prohibition does not apply if the AI \nsystem is used to support the human assessment of the involvement of a person in a \ncriminal activity, which is already based on objective and verifiable facts directly linked \nto that activity. Such AI systems that fall outside the scope of the prohibition intended \nto be used by law enforcement authorities, or on their behalf, or by Union institutions, \nbodies, offices or agencies in support of law enforcement authorities, for assessing the \nrisk of a natural person offending or re-offending not solely on the basis of profiling, or \nthe assessment of personality traits and characteristics or past criminal behaviour are \nclassified as ‘high -risk’ AI systems (Annex III, point 6, letter (d) AI Act) and must \ncomply with all relevant requirements and obligations under the AI Act. 5.1. Rationale and objectives \n(186) Recital 42 AI Act explains the background and rationale of the prohibition in Article \n5(1)(d) AI Act, namely, that natural persons should be judged on their actual behaviour \nand not on AI-predicted behaviour based solely on their profiling, personality traits or \ncharacteristics. 5.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nMain concepts and components of the prohibition \nArticle 5(1)(d) AI Act provides \nThe following AI practices shall be prohibited: \nd) the placing on the market, the putting into service for this specific purpose, or the \nuse of an AI system for making risk assessments of natural persons in order to assess \nor predict the risk of a natural person committing a criminal offence , based solely on \nthe profiling of a natural person or on assessing their personality traits and \ncharacteristics; this prohibition shall not apply to AI systems used to support the human \nassessment of the involvement of a person in a criminal activity, which is already based \non objective and verifiable facts directly linked to a criminal activity; \n(187) Several cumulative conditions must be fulfilled for the prohibition in Article 5(1)(d) AI \nAct to apply:  \n(i) The practice must constitute the ‘placing on the market’, ‘the putting into service \nfor this specific purpose’ or the ‘use’ of an AI system. (ii) The AI system must make risk assessments that assess or predict the risk of a \nnatural person committing a criminal offence. (iii)The risk assessment or the prediction must be based solely on either, or both, of \nthe following:  \n(a) the profiling of a natural person,  \n(b) assessing a natural person’s personality traits and characteristics."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\n63 \n(188) For the prohibition to apply all three conditions must be simultaneously fulfilled. The \nfirst condition, i.e. the  placing on the market, putting into service or use of the AI \nsystem, has been already analysed in section 2.3. The prohibition, therefore, applies to \nboth providers and deployers of AI systems, each within their respective responsibilities \nnot to place on the market, put into service , or use such AI systems  for this specific \npurpose. The other two conditions for the prohibition to apply are analysed below. 5.2.1. Assessing the risk or predicting the likelihood of a person committing a \ncrime \n(189) Risk assessments to assess or predict  the risk of an individual committing a criminal \noffence are often referred to as  individual ‘crime prediction’ or ‘crime forecasting’. While there is  no general ly agreed definition of ‘crime prediction’  or ‘crime \nforecasting’,129 these terms refer in general to a v ariety of advanced AI technologies \nand analytical methods applied to large amount of often historical data (including socio-\neconomic data, but also police records, etc .) which, in combination with criminology \ntheories, are used to forecast crime as a basis to inform police and law enforcement \nstrategies and action to combat, control, and prevent crime.130  \n(190) Crime prediction  AI systems identify patterns within historical data, associating \nindicators with the likelihood of a crime occurring, and then generate risk scores as \npredictive outputs. For example, such systems may be used for planning police task \nforces, for monitoring high -risk situatio ns, and for conducting  controls of persons \npredicted as likely (re-)offenders."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\nSuch systems bring opportunities for law \nenforcement authorities, especially those with scarce resources, increasing efficiency, \nand enabling a proactive approach for detecting, deterring , and anticipating criminal \noffences.131 However, such use of historical data on crimes committed to predict other \npersons’ future behaviour may perpetuate or even reinforce bias es, and may result in \ncrucial individual circumstances being ‘overlooked’ when these circumstances are not \npart of the data set or considered in the algorithms on which the particular AI system \noperates. This  may also  undermine public trust in law enforcement  and the justice \nsystem in general.132 \n(191) Such risk assessments and predictions are, in principle, forward-looking and concern \nfuture criminal offences (not yet committed ) or crimes that are assessed as a risk of \nbeing committed  at the moment , including in cases of  an attempt or preparatory \nactivities undertaken to commit a criminal offence.133 They can be made at any stage of \nthe law enforcement activities, such as during prevention and detection of crimes, but"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\n129 For example, see systems mentioned in the EU Fundamental Rights Agency handbook, such as the Criminality Awareness System \n(CAS) in the Netherlands and Precobs in Germany and Switzerland, Handbook, 2018, p.138. Preventing unlawful profiling today \nand in the future: a guide, Handbook, 2018, p.138. 130 See Europol, AI and policing The benefits and challenges of artificial intelligence for law enforcement, An Observatory Report from \nthe Europol Innovation Lab, 23 September 2024 . See also F. Yang, ‘ Predictive Policing ’ in Oxford Research Encyclopedia, \nCriminology and Criminal Justice, Oxford University Press, 2019. 131 For example, OxRec (Dutch Probation Office, ‘Reclassering Nederland ’) Prediction of violent reoffending in p risoners and \nindividuals on probation: a Dutch validation study (OxRec) - PMC (nih.gov) \n132 See for instance EU Fundamental Rights Agency (8 December 2022) Bias in algorithms - Artificial intelligence and discrimination | \nEuropean Union Agency for Fundamental Rights. 133 See in this respect Recital 42 AI Act that refers in this respect to the ‘likelihood of their offending’ and the ‘occurrence of an actual \nor potential criminal offence’ which are used in present, but not past tense."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n66 \nparticular, the existence of certain pre -established objective and verifiable facts may  \njustify that conclusion. For example,  \n- A law enforcement authority uses an AI system to predict criminal behaviour for \ncrimes such as terrorism solely based on individuals’ age, nationality, address, type of \ncar, and marital status . With that system, individuals are deemed more likely to \ncommit future offences that they have not yet committed solely based on their personal \ncharacteristics. Such a system may be assumed to be prohibited under Article 5(1)(d) \nAI Act. - National tax authorities use an AI predictive tool to review all taxpayers’ tax returns \nto predict potential criminal tax offences  to identify cases requiring  further \ninvestigation. This is done solely on  the basis of the profile built by the AI system, \nwhich uses for its assessment personal ity traits, such as double nationality, place of \nbirth, number of children, and opaque variables, especially inferred information that \nis predictive and therefore non -objective and hard to verify. Such a system  will \nnormally fall under the prohibition of Arti cle 5(1)(d) AI Act, since there is no \nreasonable suspicion of the involvement of a particular person in a criminal activity \nor other objective and verifiable facts linking that to that criminal activity. This is also \nan example that falls within the scope of social scoring prohibited under Article 5(1)(c) \nAI Act involving unfavourable treatment with data from unrelated social contexts. - A police department uses AI -based risk assessment tool to assess the risk of young \nchildren and adolescents being involv ed in ‘future violent and property offending ’. The system assesses children based on their relationships with other people and their \nsupposed risk levels, meaning that children may be deemed at a higher risk of \noffending simply by being linked to another individual with a high -risk assessment, \nsuch as a sibling or a friend."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nThe parents’ risk levels may also impact a child’s risk \nlevel. The risk assessments result in police ‘registering’ these children in their systems, \nmonitoring them with addition al inspections, and referring them to youth ‘care’ \nservices. Such a system is also likely to fall under the prohibition of Article 5(1)(d) AI \nAct. 5.2.3. Exclusion of AI systems to support the human assessment based on  \nobjective and verifiable facts directly linked to a criminal activity \n(203) Article 5(1)(d) AI Act provides, in its last phrase, that the prohibition does not apply to \nAI systems used to support the human assessment of the involvement of a  person in \na criminal activity, which is already based on objective and verifiable facts directly \nlinked to a criminal activity. Although, as noted, the situation described in this express \nexclusion is not necessarily the only one in which the prohibition does not apply , \nincluding that situation expressly in that provision offers legal certainty by delineating \nthe scope of the prohibition and by making it clear that, where that situation is at issue, \nthe prohibition does not in any event apply."}
{"meta": {"section_type": "guideline", "article_ref": "Article 14", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 14]\n\n67 \n(204) Where the system falls within the scope of the exclusion and is therefore not prohibited, \nit will be classified as a high-risk AI system (as referred to in Annex III, point 6(d), AI \nAct) if intended to be used by law enforcement authorities or on their behalf and \ntherefore subject to the requirements and  safeguards, including human oversight  \n(Article 14 and Article 26 AI Act). These requirements include that the human oversight \nmust be assigned to persons with the necessary competence, training and authority who \nshould be able to properly understand the capabilities and limitations of the AI system, \ncorrectly interpret its output and address the risk of automation bias. Those persons \nshould have clear procedures, training and the necessary competence and authority to \nmeaningfully assess the outputs of the AI system . In this specific case, their human \nassessment should ensure that any AI prediction or assessment of the risk of a person \ncommitting a crime is based on objective and verifiable facts linked to a criminal \nactivity. Those persons should also intervene in order to avoid negative consequences \nor risks, or stop the use of the AI system if it does not perform as intended. (205) Furthermore, the concept of ‘human intervention’ has been subject to CJEU case -law, \nin particular in the context of solely automated decision-making predicting the risk of \nair passengers being involved in serious crimes. That case-law may also be relevant for \nthe application of the concept of ‘human assessment’ as used in Article 5(1)(d) AI Act. In the Ligue des droits humains case,138 the CJEU examined the legality of the use of \nan advanced AI system for the systematic processing of passenger name record (PNR) \ndata of air travellers to assess their likelihood of being involved in terrorism and other \nserious crimes."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\nThe CJEU interpreted the rule in Directive (EU) 2016/681 ( ‘PNR Directive ’) that \nprohibits adverse legal decisions based solely on automated processing  and required \nindividual human assessment and review for any positive matches by non-automated \nmeans to identify false positives and ensure non-discriminatory results. According to the CJEU, that human assessment , subject to which any results of \nautomated processing of PNR data  must be based , must rely  on objective criteria to \nevaluate whether a positive match concerns someone who might be involved in this \nspecific case in terrorist offenses or serious crime, and to ensure the non-discriminatory \nnature of automated processing. (206) As to the content of the excl usion, one of its central elements is that the AI system is \nused to support human assessment, rather than involving the AI system itself making \nthe risk assessment as occurs in the situations covered by the prohibition. However, for \nthe exclusion to apply,  that human assessment must, in addition, already be based on \nobjective and verifiable facts directly linked to a criminal activity. 5.2.4. Extent to which private actors’ activities may fall within scope  \n(207) Besides law enforcement authorities that are in principle the main deployers of AI crime \npredictive systems, the activities of private entities may also be covered by the"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n68 \nprohibition in Article 5(1)(d) AI Act in some cases . That follows from the fact that, \nbased on its wording, the prohibition does not apply exclusively to law enforcement \nauthorities. Moreover, otherwise the prohibition might be easily circumvented, which \nwould call into question its effectiveness. (208) That being so, the prohibition may be assumed to apply, in particula r, when private  \nactors are entrusted by law to exercise public authority and public powers for the  \nprevention, investigation, detection or prosecution of criminal offences or the execution \nof criminal penalties.139 Private actors may be also explicitly requested on a case-by-\ncase basis to act on behalf of law enforcement authorities and carry out individual crime \nrisk predictions. In those cases,  the activities of those private actors could also fall \nwithin the scop e of the prohibition , if the applicable conditions are fulfilled and the \nexclusion does not apply. For example, a private company providing advanced AI-based crime analytic software \nmay be asked by a law enforcement authority to analyse a large amount of data from \nmultiple sources and databases , such as national registers, banking transactions, \ncommunication data, geo-spatial data, etc., to predict or assess the risk of individuals as \npotential offenders of human trafficking offences. If all the criteria for Art icle 5(1)(d) \nare met, such a use case could be prohibited. (209) Furthermore, the prohibition may apply to private entities assessing or predicting the \nrisk of a person of committing a crime where this is objectively necessary for \ncompliance with a legal obligation to which that private operator is subject to assess or \npredict the risk of persons committing specific criminal offences ( e.g., in case of anti-\nmoney laundering, terrorism financing)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nFor example,  a banking institution has an obligation under Union anti -money \nlaundering legislation140 to screen and profile customers for money -laundering \noffences. If the bank uses an AI system to fulfil its obligations, that should be done \nbased only on the data a s specified in that law which are objective and verifiable to \nensure that the person s singled out as suspect are reasonably likely to commit anti -\nmoney laundering offences. The predictions must also be subject to human assessment \nand verification in compliance with that legislation 141 in order to ensure the accuracy \nand appropriateness of such assessments. Compliance with that legislation will ensure \nthat the use of individual crime prediction AI system  for anti -money laundering \npurposes fall outside the scope of the prohibition in Article 5(1)(d) AI Act. (210) However, having regard to the focus on risk assessments relating specifically and \nexclusively to the commission of criminal offences that is evident from the wording of \nthe prohibition as well as to the purpose of the prohibition as explained in Recital 42, if \na private entity  profiles customers for its regular business operations and safety or to \nprotect its financial interests (e.g., detecting financial irregularities) without the purpose"}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 3]\n\n139  See definition of law enforcement authorities in Article 3(45) AI Act. 140  Anti-Money Laundering Regulation (EU) 2024/1624 of 31 May 2024. 141  Article 20 of Regulation (EU) 2024/1624."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n69 \nof assessing or predicting the risk of the customer committing a specific criminal \noffence, the activities of the private entities should not be considered to fall under the \nscope of the prohibition of Article 5(1)(d) AI Act. (211) In other words, in the absence of private parties having been entrusted by law certain \nspecific law enforcement tasks, acting on behalf of law enforcement authorities or being \nsubject to specific legal obligations as described above, the use of AI systems for \nmaking risk assessments in the context of private entities’ ordinary course of business \nand with the aim of protecting their own private interests, whilst the fact that those risk \nassessments may relate to the risk of criminal offences being committed merely as a \npurely accidental and secondary circumstance, is not deemed to be covered by the \nprohibition. 5.3. Out of scope \n5.3.1. Location-based or geospatial predictive or place -based crime \npredictions \n(212) Location-based or geospatial or place-based crime predictions is based on the place or \nlocation of crime or the likelihood that in those areas a  crime would be committed. In \nprinciple, such policing does not involve an assessment of a specific individual . They \ntherefore fall outside the scope of the prohibition. Examples of location-based or geospatial predictive or place-based crime predictions \n- AI-based predictive policing system provides a score of the likelihood of criminality \nin different areas in a city based on previous criminality rates by area and other \nsupporting information such as street maps, to highlight elevated risk of specific types \nof criminalities e.g., burglaries, knife crime etc. and help law enforcement authorities \ndetermine where to deploy less or more police patrols/presence to carry out \ncommunity policing to disrupt and deter criminal activity."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n- A customs authority uses AI risk analytic tools to predict the likelihood of the location \nof narcotics or illicit goods, for example on the basis of known trafficking routes. - A police department uses AI-driven systems to detect and locate gunshots in real time. The system employs acoustic sensors in urban areas to identify gunfire sounds and \ntriangulate their location, providing officers with actionable data to aid detection and \ninvestigation of crimes. (213) It may , however, not always be evident how to distinguish  location-based crime \npredictive systems from individual predictive systems assessing the risk of a person \ncommitting a crime . To the extent that an AI system  carries out location -based \npredictive policing and then considers the risk score of the location as an aspect in the \nprofiling of a person , that system should be considered person -based and in principle \ncovered by Article 5(1) (d) AI Act , although it may f all outside the scope of the \nprohibition on other grounds."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment]\n\n71 \ntrained on data including past criminal history of individuals in similar cases, as well \nas factors such as age group, social behaviour, income, and employment status. - AI system is used to support the assessment of a human officer to assess the risk of an \nindividual serving a non-custodial sentence violating release conditions or absconding \nbased on past criminal behaviours and objective facts that give grounds for suspicion \nsuch as adherence to conditions of release, psychological ass essment outcomes and \nrecommendations from other community services the individual may be using. Based \non this information,  the officer decides whether  to maintain the status quo or revise \nthe conditions of release. - AI systems used by custom authorities to assess the risk of goods entering the EU not \ncomplying with the legislation applicable at the border (e.g., which may include bans \non import of illicit drugs, export sanctions contravention or other illegal activity)  to \nidentify situations w here a customs control should be carried out. The AI system \nassesses objective and verifiable information  provided to the customs  related to the \ngoods and their supply chains (e.g., nature and value of the goods, container number, \nmeans of transport for co ncealment of other goods, prior knowledge relating to the \ncompliance of goods of the particular description and origin with requirements for \ntheir importation to or exportation from the Union). In certain cases, it may also \nprocess information about the pr ior involvement of the importer or exporter in \nirregularities related to import of goods, their affiliation to criminal organizations or a \ncriminal record for drug trafficking."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment] [Article 5]\n\nSuch systems are out of scope of the prohibition \nbecause any prediction of a likelihood of a natural person to be involved in an import \nor export of illicit goods is not solely based on profiling, but on objective and \nverifiable information related to the goods and the importer or exporter’s prior \ninvolvement in criminal activity and subject to a human review to determine whether \nor not the situation requires a customs control or risk mitigation action. 5.3.3. AI systems used for crime predictions and assessments in relation to  \nlegal entities \n(215) The prohibition in Article 5(1)(d) AI Act applies only to individual predictions and risk \nassessments of natural persons, thus typically excluding crime predictive systems that \nprofile legal entities such as companies or non-governmental organisations. For example,  \n- A tax or customs authority is using an AI system to analyse large amounts of data on \ntransactions and tax declarations and customs data of companies for assessing the risk \nof a company committing tax or customs fraud constituting a criminal offence. - AI systems used to assist customs authorities to help identify situations where an \ninstruction not to send illicit goods to the EU should be issued to legal entities. (216) At the same time, there may be borderline cases where a natural person acts via a legal \nentity as a ‘sole tr ader’ or as an independent profession al (e.g., a lawyer) . In such \ncircumstances, the prohibition in Article 5(1)(d) AI Act may apply provided that all"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n72 \nconditions are fulfilled , since  the AI system profiles a specific natural person and \nassesses or predicts their risk of committing a criminal offence, even if this is done for \npurposes in relation to the commercial activity undertaken by the natural person. 5.3.4. AI systems used for individual predictions of administrative offences \n(217) The prohibition in Article 5(1)(d) AI Act applies only for the prediction of criminal \noffences, thus excluding administrative offences from its scope , the prosecution of \nwhich is, in principle, less intrusive for the fundamental rights and freedoms of people. For example, a public authority using AI in the context of an administrative \ninvestigation to assess the risk of potential offenders of committing minor offences \n(such as petty traffic offences ) or irregularities  in tax, procurement or expenditure \nprocesses would not fall within the scope of the prohibition in Article 5(1)(d) AI Act, \neven in cases where information might be gathered for possible involvement of the \nnatural persons in criminal offences as a result of the administrative investigations and \nchecks. (218) Whether an offence is administrative or criminal in nature may depend on  Union or \nnational law. For offences that are not directly regulated by Union law , the national \nqualification of the offence is subject to scrutiny by CJEU since ‘criminal offence’ is a \nconcept that has autonomous meaning within the EU law and should be interpreted \nconsistently across Member States . The CJEU has conclu ded, in a different context,  \nthat the classification of the offences by the Member States is not conclusive  in that \nrespect.142 Relevant criteria used to assess the nature of the offence  (criminal or not)  \nmay be found in relevant case -law of the CJEU and the European Court of Human \nRights (ECtHR).143 \n5.4."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nInterplay with other Union legal acts \n(219) The interplay of the prohibition in Article 5(1)(d) AI Act  with the LED and GDPR is \nrelevant when assessing the lawfulness of personal data processing under Union data \nprotection law, such as the GDPR and the LED. In particular, Article 5(1)(d) AI Act \nimposes a specific prohibition for law enforcement authorities, other public authorities \nand private entities falling within the scope of the prohibition to assess or predict the \nrisk of a natural person committing a criminal offence, based solely on the profiling of \na natural person or on assessing their personality traits and characteristics. With regard"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n73 \nto LED, Article 5(1)(d) AI Act  is without prejudice to Article 11(3)  LED, which \nprohibits profiling resulting in (direct or indirect) discrimination. (220) The interplay of the prohibition in Article 5(1)(d) AI Act with Directive (EU) 2016/343 \non the presumption of innocence is also relevant, since both acts are concerned - directly \nin the case of the Directive and indirectly in the case of the AI Act (see its Recital 42), \nwith the fundamental right to be presumed innocent until proven guilty according to \nlaw.144 While the Directive applies from the moment that a person is suspected or \naccused of having committed a criminal offence,145 the AI Act has a broader scope of \napplication and applies already at the stage of prediction and crime prevention before a \nformal criminal investigation is opened against a particular person  and even in cases \nwhen such predictions and risk assessments are made by private actors falling within \nthe scope of Article 5(1)d) AI Act and not by competent law enforcement authorities, \nincluding judicial authorities. (221) Even in cases where the prohibition in Article 5(1) (d) AI Act does not apply, it is \nimportant to emphasize that applicable Union and national law remains fully applicable, \nincluding in particular data protection, criminal procedural and police law and \nsafeguards that may further restrict  or impose additional conditions on  the use of \nindividual crime predictive AI systems. 6. ARTICLE 5(1)(E) AI ACT - UNTARGETED SCRAPING OF FACIAL IMAGES  \n(222) Article 5(1)(e) AI Act prohibits the placing on the market, putting into service for this \nspecific purpose, or the use of AI systems that create o r expand facial recognition \ndatabases through the untargeted scraping of facial images from the Internet or CCTV \nfootage. 6.1."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nRationale and objectives \n(223) The untargeted scraping of facial images from the  internet and from CCTV footage \nseriously interferes with  individuals’ rights to privacy and data protection and deny \nthose individuals the right to remain anonymous. Recital 43 AI Act therefore justifies \nthe prohibition established in  Article 5(1)(e) AI Act based on the ‘feeling of mass \nsurveillance’ and the risks of ‘gross violations of fundamental rights, including the right \nto privacy’. 6.2. Main concepts and components of the prohibition  \nArticle 5(1)(e) AI Act provides \nThe following AI practices shall be prohibited: \n(e) the placing on the market, the putting into service for this specific purpose, or the \nuse of AI systems that create or expand facial recognition databases through the \nuntargeted scraping of facial images from the internet or CCTV footage;"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 5]\n\n76 \n(234) The prohibition in Article 5(1)(e) AI Act  does not apply to the untargeted scraping of \nbiometric data other than facial images (such as voice samples). The prohibition does \nalso not apply where no AI systems are involved in the scraping. Facial image databases \nthat are not used for the recognition of persons  are also out of scope , such as facial \nimage databases used for AI model training or testing purposes, where the persons are \nnot identified. (235) The prohibition in Article 5(1)(e) AI Act does not apply to AI systems which harvest \nlarge amounts of facial images from the internet to build AI models that generate new \nimages about fictitious persons because such systems would not result in the recognition \nof real persons. Such AI systems could fall under the transparency requirements of \nArticle 50 AI Act. (236) The prohibition in Article 5(1)(e) AI Act covers AI systems used to create or expand \nfacial recognition databases. When it comes to existing facial databases built up prior \nto the entry into application of the prohibition, which are not further expanded through \nAI-enabled untargeted scraping, those databases and their use must comply with the  \napplicable Union data protection rules. (237) The prohibition in Article 5(1)(e) AI Act  is targeted at the creation or expansion of \nfacial recognition databases. The concrete act of biometric identification is subject to \nspecific rules in the AI Act and other relevant Union legislation. 6.4. Interplay with other Union legal acts \n(238) In relation to Union data protection law, the untargeted scraping of the internet or CCTV \nmaterial to build -up or expand face recognition databases , i.e. the processing of \npersonal data (collection of data and use of databases) would be unlawful and no legal \nbasis under the GDPR, EUDPR and the LED could be relied upon. 7."}
{"meta": {"section_type": "guideline", "article_ref": "ARTICLE 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [ARTICLE 5]\n\nARTICLE 5(1)(F) AI ACT EMOTION RECOGNITION  \n(239) Article 5(1)(f) AI Act prohibits AI systems to infer emotions of a natural person in the \nareas of workplace and education institutions, except where the use of the system is \nintended for medical or safety reasons. Emotion recognition systems  that do not fall \nunder the prohibition are considered high-risk pursuant to point (1)(c) of Annex III AI \nAct. Article 50(3)  AI Act lays down certain transparency requirements for the use of \nemotion recognition systems. 7.1. Rationale and objectives \n(240) Emotion recognition technology is quickly evolving and comprehends different \ntechnologies and processing operations to detect, collect, analyse, categori se, re act, \ninteract and learn emotions from persons. Such technology is also referred to as ‘affect \ntechnology’. Emotion recognition can be used in multiple areas and domains for a wide \nrange of applications 146 such as for analysing customer behaviour 147 and targeted"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education]\n\n77 \nadvertising and neuromarketing ;148 in the entertainment industry , for example  to \nprovide personalised recommendations or to predict reactions to movies ; in medicine \nand healthcare, for example to detect depression, for suicide prevention or to detect \nautism, in education, for example to monitor attention or engagement of learners (pupils \nand students at different ages) ; in employment , for example  to accompany the \nrecruitment process, to monitor emotions or boredom of employees, but also well-being \napplications for ‘making workers happier’ ;149 for law enforcement and public safety, \nfor example with lie detectors or emotion screening at big events ; and for many other \npurposes. (241) Emotion recognition is frequently doubted as to its effectiveness or as to its accuracy.150 \nRecital 44 AI Act explains that there are ‘serious concerns about the scientific basis of \nAI systems aiming to identify or infer emotions, particularly as expression of emotions \nvary considerably across cultures and situations, and even within a single individual. Among the key shortc omings of such systems are the limited reliability, the lack of \nspecificity and the limited generalisability.’ It further explains that emotion recognition \ncan lead to ‘discriminatory outcomes and can be intrusive to the rights and freedoms of \nthe concerned persons’, in particular the rights to privacy, human dignity and freedom \nof thought. This plays an important role in asymmetric relationships  especially in the \ncontext of the workplace and education  and training institutions, where both workers \nand students are in particularly vulnerable position s. At the same time, emotion \nrecognition in specific use contexts, such as for safety  and medical care (e.g., health \ntreatment and diagnosis) has benefits.151 \n7.2."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment]\n\n77 \nadvertising and neuromarketing ;148 in the entertainment industry , for example  to \nprovide personalised recommendations or to predict reactions to movies ; in medicine \nand healthcare, for example to detect depression, for suicide prevention or to detect \nautism, in education, for example to monitor attention or engagement of learners (pupils \nand students at different ages) ; in employment , for example  to accompany the \nrecruitment process, to monitor emotions or boredom of employees, but also well-being \napplications for ‘making workers happier’ ;149 for law enforcement and public safety, \nfor example with lie detectors or emotion screening at big events ; and for many other \npurposes. (241) Emotion recognition is frequently doubted as to its effectiveness or as to its accuracy.150 \nRecital 44 AI Act explains that there are ‘serious concerns about the scientific basis of \nAI systems aiming to identify or infer emotions, particularly as expression of emotions \nvary considerably across cultures and situations, and even within a single individual. Among the key shortc omings of such systems are the limited reliability, the lack of \nspecificity and the limited generalisability.’ It further explains that emotion recognition \ncan lead to ‘discriminatory outcomes and can be intrusive to the rights and freedoms of \nthe concerned persons’, in particular the rights to privacy, human dignity and freedom \nof thought. This plays an important role in asymmetric relationships  especially in the \ncontext of the workplace and education  and training institutions, where both workers \nand students are in particularly vulnerable position s. At the same time, emotion \nrecognition in specific use contexts, such as for safety  and medical care (e.g., health \ntreatment and diagnosis) has benefits.151 \n7.2."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Healthcare]\n\n77 \nadvertising and neuromarketing ;148 in the entertainment industry , for example  to \nprovide personalised recommendations or to predict reactions to movies ; in medicine \nand healthcare, for example to detect depression, for suicide prevention or to detect \nautism, in education, for example to monitor attention or engagement of learners (pupils \nand students at different ages) ; in employment , for example  to accompany the \nrecruitment process, to monitor emotions or boredom of employees, but also well-being \napplications for ‘making workers happier’ ;149 for law enforcement and public safety, \nfor example with lie detectors or emotion screening at big events ; and for many other \npurposes. (241) Emotion recognition is frequently doubted as to its effectiveness or as to its accuracy.150 \nRecital 44 AI Act explains that there are ‘serious concerns about the scientific basis of \nAI systems aiming to identify or infer emotions, particularly as expression of emotions \nvary considerably across cultures and situations, and even within a single individual. Among the key shortc omings of such systems are the limited reliability, the lack of \nspecificity and the limited generalisability.’ It further explains that emotion recognition \ncan lead to ‘discriminatory outcomes and can be intrusive to the rights and freedoms of \nthe concerned persons’, in particular the rights to privacy, human dignity and freedom \nof thought. This plays an important role in asymmetric relationships  especially in the \ncontext of the workplace and education  and training institutions, where both workers \nand students are in particularly vulnerable position s. At the same time, emotion \nrecognition in specific use contexts, such as for safety  and medical care (e.g., health \ntreatment and diagnosis) has benefits.151 \n7.2."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\n77 \nadvertising and neuromarketing ;148 in the entertainment industry , for example  to \nprovide personalised recommendations or to predict reactions to movies ; in medicine \nand healthcare, for example to detect depression, for suicide prevention or to detect \nautism, in education, for example to monitor attention or engagement of learners (pupils \nand students at different ages) ; in employment , for example  to accompany the \nrecruitment process, to monitor emotions or boredom of employees, but also well-being \napplications for ‘making workers happier’ ;149 for law enforcement and public safety, \nfor example with lie detectors or emotion screening at big events ; and for many other \npurposes. (241) Emotion recognition is frequently doubted as to its effectiveness or as to its accuracy.150 \nRecital 44 AI Act explains that there are ‘serious concerns about the scientific basis of \nAI systems aiming to identify or infer emotions, particularly as expression of emotions \nvary considerably across cultures and situations, and even within a single individual. Among the key shortc omings of such systems are the limited reliability, the lack of \nspecificity and the limited generalisability.’ It further explains that emotion recognition \ncan lead to ‘discriminatory outcomes and can be intrusive to the rights and freedoms of \nthe concerned persons’, in particular the rights to privacy, human dignity and freedom \nof thought. This plays an important role in asymmetric relationships  especially in the \ncontext of the workplace and education  and training institutions, where both workers \nand students are in particularly vulnerable position s. At the same time, emotion \nrecognition in specific use contexts, such as for safety  and medical care (e.g., health \ntreatment and diagnosis) has benefits.151 \n7.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 5]\n\nMain concepts and components of the prohibition \nArticle 5(1) (f) AI Act provides: \nThe following AI practices shall be prohibited: \nf) the placing on the market, the putting into service for this specific purpose, or the use \nof AI systems to infer emotions of a natural person in the areas of workplace and \neducation institutions, except where the use of the AI system is intended to be put in \nplace or into the market for medical or safety reasons. (242) Several cumulative conditions must be fulfilled for the prohibition in Article 5(1)(f) AI \nAct to apply:"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment] [Article 5]\n\nMain concepts and components of the prohibition \nArticle 5(1) (f) AI Act provides: \nThe following AI practices shall be prohibited: \nf) the placing on the market, the putting into service for this specific purpose, or the use \nof AI systems to infer emotions of a natural person in the areas of workplace and \neducation institutions, except where the use of the AI system is intended to be put in \nplace or into the market for medical or safety reasons. (242) Several cumulative conditions must be fulfilled for the prohibition in Article 5(1)(f) AI \nAct to apply:"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Healthcare] [Article 5]\n\nMain concepts and components of the prohibition \nArticle 5(1) (f) AI Act provides: \nThe following AI practices shall be prohibited: \nf) the placing on the market, the putting into service for this specific purpose, or the use \nof AI systems to infer emotions of a natural person in the areas of workplace and \neducation institutions, except where the use of the AI system is intended to be put in \nplace or into the market for medical or safety reasons. (242) Several cumulative conditions must be fulfilled for the prohibition in Article 5(1)(f) AI \nAct to apply:"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nMain concepts and components of the prohibition \nArticle 5(1) (f) AI Act provides: \nThe following AI practices shall be prohibited: \nf) the placing on the market, the putting into service for this specific purpose, or the use \nof AI systems to infer emotions of a natural person in the areas of workplace and \neducation institutions, except where the use of the AI system is intended to be put in \nplace or into the market for medical or safety reasons. (242) Several cumulative conditions must be fulfilled for the prohibition in Article 5(1)(f) AI \nAct to apply:"}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 3]\n\n78 \n(i) The practice must constitute the ‘placing on the market’, ‘the putting into service \nfor this specific purpose’ or the ‘use’ of an AI system;  \n(ii) AI system to infer emotions;152 \n(iii) in the area of the workplace or education and training institutions; and \n(iv)  excluded from the prohibition are AI systems intended for medical or safety \nreasons. (243) For the prohibition to apply all four conditions must be simultaneously fulfilled. The \nfirst element, i.e. the placing on the market, putting into service or use of the AI system, \nhas already been analysed in sectio n 2.3.. The prohibition, therefore, applies to both \nproviders and deployers of AI systems, each within their respective responsibilities, not \nto place on the market, put into service or use such AI  systems. The other conditions \nrelated to the prohibition are further described and analysed below. 7.2.1. AI systems to infer emotions \na)  AI systems to infer emotions versus emotion recognition systems \n(244) Article 3(39) AI Act defines ‘emotion recognition systems’ as AI systems ‘for the \npurpose of identifying and inferring emotions or intentions of natural persons  on the \nbasis of their biometric data. The prohibition in Article 5(1)(f) AI Act does not refer to \n‘emotion recognition systems’, but only to ‘AI systems to infer emotions of a natural \nperson’. Recital 44 further clarifies that that prohibition covers AI systems ‘to identify \nor infer emotions.’ \n(245) Inferring generally encompasses identifying as a prerequisite , so that t he prohibition \nshould be understood as including both AI systems identifying or inferring emotions or \nintentions.153 For consistency reasons, it is also important to construe the prohibition in \nArticle 5(1)(f) AI Act as having a similar scope as the rules applicable to other emotion \nrecognition systems (Annex III, point 1 (c), and Article 50 AI Act) and to limit it to \ninferences based on a person’s biometric data."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 3]\n\nThe definition in Article 3(39) AI Act of \nemotion recognition systems should therefore be considered relevant in relatio n to \nArticle 5(1)(f) AI Act. b) Identification and inference of emotions or intentions  \n(246) ‘Identification’ occurs where the processing of the biometric data (for example, of the \nvoice or a facial expression) of a natural person allows to directly compare and identify \nan emotion with one that has been previously programmed in the emotion recognition \nsystem. ‘Inferring’ is done by deducing information generated by analytical and other \nprocesses by the system itself. In such a case, the information about the emotion is not \nsolely based on data collected on the natural person, but it is inferred from other data, \nincluding machine learning approaches that learn from data how to detect emotions.154"}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Biometrics", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Biometrics] [Article 3]\n\n80 \n(250) According to the definition in Article 3(39) AI Act, only AI systems  identifying or \ninferring emotions or intentions based on biometric data constitute emotion recognition \nsystems.155 \n(251) Personal characteristics from which biometric data can be extracted are physical or \nbehavioural attributes. Physiological biometrics employ physical, structural, and \nrelatively static attributes of a person, such as their fingerprints, the pattern of their iris, \ncontours of their face, or the geometry of veins in their hands. Some modalities are \nmicroscopic in nature , but still exhibit biological and chemical structures that can be \nacquired and identified e.g., DNA and odour.156 Behavioural biometrics monitor the \ndistinctive characteristics of movements, gestures, and motor -skills of individuals as \nthey perform a task or series of tasks. This means  that human movements , such as \nwalking (gait analysis) or finger contact with a keyboard (keystrokes), are captured and \nanalysed. Behavioural biometrics encompass a variety of modalities that exhibit both \nvoluntary and involuntary repeated motions and associated rhythmic timings/pressures \nof body features ranging from signat ures, gait, voice, and keystrokes through to eye \ntracking and heartbeats ,157 electroencephalography (EEG) ,158 or electrocardiograms \n(ECG).159 The biometric input can relate to one modality ( e.g., facial images) or \nmultiple modalities ( e.g., facial information combined with electroencephalogram \n(EEG)). Recital 18 gives as examples facial expressions, gestures such as movement of \nhands or characteristics of a person’s voice. For example, \n- An AI system inferring emotions from written text (content/sentiment analyses) to \ndefine the style or the tone of a certain article is not based on biometric data and \ntherefore does not fall within the scope of the prohibition."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 3]\n\n80 \n(250) According to the definition in Article 3(39) AI Act, only AI systems  identifying or \ninferring emotions or intentions based on biometric data constitute emotion recognition \nsystems.155 \n(251) Personal characteristics from which biometric data can be extracted are physical or \nbehavioural attributes. Physiological biometrics employ physical, structural, and \nrelatively static attributes of a person, such as their fingerprints, the pattern of their iris, \ncontours of their face, or the geometry of veins in their hands. Some modalities are \nmicroscopic in nature , but still exhibit biological and chemical structures that can be \nacquired and identified e.g., DNA and odour.156 Behavioural biometrics monitor the \ndistinctive characteristics of movements, gestures, and motor -skills of individuals as \nthey perform a task or series of tasks. This means  that human movements , such as \nwalking (gait analysis) or finger contact with a keyboard (keystrokes), are captured and \nanalysed. Behavioural biometrics encompass a variety of modalities that exhibit both \nvoluntary and involuntary repeated motions and associated rhythmic timings/pressures \nof body features ranging from signat ures, gait, voice, and keystrokes through to eye \ntracking and heartbeats ,157 electroencephalography (EEG) ,158 or electrocardiograms \n(ECG).159 The biometric input can relate to one modality ( e.g., facial images) or \nmultiple modalities ( e.g., facial information combined with electroencephalogram \n(EEG)). Recital 18 gives as examples facial expressions, gestures such as movement of \nhands or characteristics of a person’s voice. For example, \n- An AI system inferring emotions from written text (content/sentiment analyses) to \ndefine the style or the tone of a certain article is not based on biometric data and \ntherefore does not fall within the scope of the prohibition."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Biometrics", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Biometrics]\n\n- An AI  system inferring emotions from key stroke (way of typing), facial \nexpressions, body postures or movements is based on biometric data and falls within \nthe scope of the prohibition. (252) The AI Act definition of biometric data is therefore broad and includes any biometric \ndata used for emotion recognition, biometric categorisation or other purposes.160 \n7.2.2. Limitation of the prohibition to workplace and educational institutions"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education]\n\n- An AI  system inferring emotions from key stroke (way of typing), facial \nexpressions, body postures or movements is based on biometric data and falls within \nthe scope of the prohibition. (252) The AI Act definition of biometric data is therefore broad and includes any biometric \ndata used for emotion recognition, biometric categorisation or other purposes.160 \n7.2.2. Limitation of the prohibition to workplace and educational institutions"}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Biometrics", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Biometrics] [Article 3]\n\n155  Article 3(34) AI Act: defines ‘biometric data’ as ‘personal data resulting from specific technical processing relating to the physical, \nphysiological or behavioural characteristics of a natural person, such as facial images or dactyloscopic data’’ See also Recital 18 AI \nAct. About emotion inferences from voice and speech. 156  Physiological and Behavioural Biometrics - Biometrics Institute \n157  Physiological and Behavioural Biometrics - Biometrics Institute \n158  See EDPS, TechDispatch  1/2024 – Neurodata, 3.6.2024, in which the use of brain data and related technology is discussed, as well \nas the legal implication, including the proposition of new ‘neurorights’, including mental privacy and integrity. In S. O’Sullivan, H. Chneiweiss, A. Pierucci and K. Rommelfanger, Neurotechnologies and Human Rights Framework: Do we need new Human Rights?, \nReport, OECD and CoE, 9.11.2021, p.33 , a state of the art and legal aspects of neurotech is discussed. 159  See Hasnul et al., 2021, Electrocardiogram-Based Emotion Recognition Systems and Their Applications in Healthcare. 160  In the AI Act, the definition of biometric data does not include the wording ‘which allow or confirm the unique identificatio n’ (the \nfunctional use of biometric data), contrary to the definition of biometric data in the GDPR that includes this requirement. The GDPR \ndefinition of biometric data will apply under data protection rules with regard to the processing of personal data (and when for \nexample Article 9(1) and 9(2) GDPR would be applicable)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Healthcare] [Article 3]\n\n155  Article 3(34) AI Act: defines ‘biometric data’ as ‘personal data resulting from specific technical processing relating to the physical, \nphysiological or behavioural characteristics of a natural person, such as facial images or dactyloscopic data’’ See also Recital 18 AI \nAct. About emotion inferences from voice and speech. 156  Physiological and Behavioural Biometrics - Biometrics Institute \n157  Physiological and Behavioural Biometrics - Biometrics Institute \n158  See EDPS, TechDispatch  1/2024 – Neurodata, 3.6.2024, in which the use of brain data and related technology is discussed, as well \nas the legal implication, including the proposition of new ‘neurorights’, including mental privacy and integrity. In S. O’Sullivan, H. Chneiweiss, A. Pierucci and K. Rommelfanger, Neurotechnologies and Human Rights Framework: Do we need new Human Rights?, \nReport, OECD and CoE, 9.11.2021, p.33 , a state of the art and legal aspects of neurotech is discussed. 159  See Hasnul et al., 2021, Electrocardiogram-Based Emotion Recognition Systems and Their Applications in Healthcare. 160  In the AI Act, the definition of biometric data does not include the wording ‘which allow or confirm the unique identificatio n’ (the \nfunctional use of biometric data), contrary to the definition of biometric data in the GDPR that includes this requirement. The GDPR \ndefinition of biometric data will apply under data protection rules with regard to the processing of personal data (and when for \nexample Article 9(1) and 9(2) GDPR would be applicable)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 5]\n\n81 \n(253) The prohibition in Article 5(1)(f) AI Act is limited to emotion recognition systems in \nthe ‘areas of workplace and educational institutions’. As clarified in recital 44 AI Act, \nthis limitation is meant to address the imbalance of  power in the context of work or \neducation. a) ‘Workplace’  \n(254) The notion of ’w orkplace’ should be interpreted broadly . That notion  relates to any \nspecific physical or virtual space where natural persons engage in tasks and \nresponsibilities assigned by their employer or by the organisation they are affiliated to, \nfor example in case of self -employment. This includes any setting where the work is \nperformed and can vary widely based on the nature of the job, spanning from indoor \noffice spaces, factories and warehouses  to publicly accessible spaces like shops, \nstadiums or museums, to open -air sites or cars , as well as  temporary or mobile work \nsites. This is independent from the status as an employee, contractor, trainee, volunteer, \netc.161 The notion of ‘workplace’ in Article 5(1)(f) AI Act should also be understood to \napply to candidates during the selection and hiring process , consistently with other \nprovisions of the AI Act addressing the placing on the market, putting into service or \nuse of AI systems in the area of employment, workers management and access to self-\nemployment, since there is an imbalance of powers and the intrusive nature of emotion \nrecognition may already apply at the recruitment stage. For example, \n- Using webcams and voice recognition systems by a call centre to track their \nemployee’s emotions, such as anger, is prohibited. 162 If only deployed for personal \ntraining purposes, emotion recognition systems are allowed if the results are not shared \nwith HR responsible persons and cannot impact the assessment, promotion etc."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment] [Article 5]\n\n81 \n(253) The prohibition in Article 5(1)(f) AI Act is limited to emotion recognition systems in \nthe ‘areas of workplace and educational institutions’. As clarified in recital 44 AI Act, \nthis limitation is meant to address the imbalance of  power in the context of work or \neducation. a) ‘Workplace’  \n(254) The notion of ’w orkplace’ should be interpreted broadly . That notion  relates to any \nspecific physical or virtual space where natural persons engage in tasks and \nresponsibilities assigned by their employer or by the organisation they are affiliated to, \nfor example in case of self -employment. This includes any setting where the work is \nperformed and can vary widely based on the nature of the job, spanning from indoor \noffice spaces, factories and warehouses  to publicly accessible spaces like shops, \nstadiums or museums, to open -air sites or cars , as well as  temporary or mobile work \nsites. This is independent from the status as an employee, contractor, trainee, volunteer, \netc.161 The notion of ‘workplace’ in Article 5(1)(f) AI Act should also be understood to \napply to candidates during the selection and hiring process , consistently with other \nprovisions of the AI Act addressing the placing on the market, putting into service or \nuse of AI systems in the area of employment, workers management and access to self-\nemployment, since there is an imbalance of powers and the intrusive nature of emotion \nrecognition may already apply at the recruitment stage. For example, \n- Using webcams and voice recognition systems by a call centre to track their \nemployee’s emotions, such as anger, is prohibited. 162 If only deployed for personal \ntraining purposes, emotion recognition systems are allowed if the results are not shared \nwith HR responsible persons and cannot impact the assessment, promotion etc."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 5]\n\nof the \nperson trained, provided that  the prohibition is not circumvented and the use of the  \nemotion recognition system does not have any impact on the work relationship. - Using voice recognition systems by a call centre to track their customers emotions, \nsuch as anger or impatience, is not prohibited by Article 5(1)(f) AI Act (for example \nto help the employees cope with certain angry customers). - AI systems monitoring the emotional tone in hybrid work teams  by identifying and \ninferring emotions from voice and imagery of hybrid video calls , which would \ntypically serve the purpose of  fostering social awareness, emotional dynamics \nmanagement, and conflict prevention, are prohibited. - Using emotion recognition AI systems during the recruitment process is prohibited. - Using emotion recognition AI systems during the probationary period is prohibited."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment] [Article 5]\n\nof the \nperson trained, provided that  the prohibition is not circumvented and the use of the  \nemotion recognition system does not have any impact on the work relationship. - Using voice recognition systems by a call centre to track their customers emotions, \nsuch as anger or impatience, is not prohibited by Article 5(1)(f) AI Act (for example \nto help the employees cope with certain angry customers). - AI systems monitoring the emotional tone in hybrid work teams  by identifying and \ninferring emotions from voice and imagery of hybrid video calls , which would \ntypically serve the purpose of  fostering social awareness, emotional dynamics \nmanagement, and conflict prevention, are prohibited. - Using emotion recognition AI systems during the recruitment process is prohibited. - Using emotion recognition AI systems during the probationary period is prohibited."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment]\n\n161 See also the recitals in relation to the high-risk AI systems in the workplace, such as Recital 56, deploying a broad interpretation. See \nalso the list of high-risk AI systems in Annex III, referring to self-employment at 4. Self-employment is also broadly covered by EU \nanti-discrimination law. 162 Example from Boyd et al., 2023, Automated Emotion Recognition in the Workplace: How Proposed Technologies Reveal Potential \nFutures of Work."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 5]\n\n82 \n- Using cameras by a supermarket to track its employees’ emotions, such as happiness, \nis prohibited. - Using cameras by a supermarket or a bank to detect suspicious customers, for example \nto conclude that somebody is about to commit a robbery , is not prohibited under \nArticle 5(1)(f) AI Act, when it is ensured that no employees are being tracked  and \nthere are sufficient safeguards. b) ‘Education institutions’ \n(255) The reference to education institutions is broad and should be understood to include \nboth public and private institutions. There is no limitation as regards the types or ages \nof pupils or students or of a specific environment (online, in person, in a blended \nmode,163 etc.). For example, education and training institutions at all levels fall under \nthe scope of the prohibition in Article 5(1)(f) AI Act, including vocational schools, i.e. schools where students learn skills involving the use of their hands 164 and continuous \ntraining165. Education institutions are normally accredited or sanctioned by the relevant \nnational education authorities or equivalent authorities. A key feature is that education \ninstitutions may provide a certificate  (respectively participation is a precondition for \nobtaining a certificate). The prohibition should be understood to also apply to \ncandidates during the admissibility process. For example, \n- An AI-based application using emotion recognition for learning a language online \noutside an education institution is not prohibited under Article 5(1)(f) AI Act. By \ncontrast, if students are required to use the application by an education institution, \nthe use of such emotion recognition system is prohibited. - An education institution  using AI-based eye tracking software when examining \nstudents online to track the fixation point and movement of the eyes (gaze point, e.g., \nto detect if unauthorized material is used)  is not prohibited, because the system does \nnot identify or infer emotions ."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education]\n\nB y contrast, if the system is also used to detect \nemotions, such as emotional arousal and anxiousness, this would fall within the scope \nof the prohibition. - Using an emotion recognition AI system  by a n education institution to infer the \ninterest and attention of students is prohibited. By contrast , i f only deployed for \nlearning purposes in the context of a role -play (for example, for training actors or \nteachers), emotion recognition systems are allowed if the results cannot impact the \nevaluation or certification of the person being trained."}
{"meta": {"section_type": "guideline", "article_ref": "Article 14", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 14]\n\n163  Blended learning is to be understood as taking more than one approach in the education and training process, including blendi ng \ndigital (including online learning) and non-digital learning tools. 164  See e.g. the impact assessment accompanying the proposal  of the Commission, where specific AI uses by vocational training \ninstitutions were mentioned as posing intense interference with a broad range of fundamental rights, e.g. when assessing: EU \nCommission, Commission Staff Working Document. Impact Assessment. Annexes, SWD(2021)84 final, Part2/2,  p. 43. See also I. Tuomi, The , The use of Artificial Intelligence (AI) in education, European Parliament, 2020, pp. 9-10. 165  See Article 14 Charter."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 5]\n\n83 \n- Using an emotion recognition AI system  by an education institut ion during \nadmissibility tests for new students is prohibited. - Using an AI system that allows to capture students talking to each other via their \nphones or other channels during online lectures by an education institution is not \nprohibited, since it does not infer emotions. By contrast, if the system is also used to \ndetect emotions, such as emotional arousal, anxiousness and interest, this would fall \nwithin the scope of the prohibition. - An education institution employing an emotion recognition AI system on both \nteachers (workplace) and students (education) is prohibited. 7.2.3. Exceptions for medical and safety reasons \n(256) The prohibition in Article 5(1)(f) AI Act  contains an explicit exception for emotion \nrecognition systems used  in the area of the workplace and education institutions  for \nmedical or safety reasons, such as systems for therapeutical use .166 In light of the AI \nAct’s objective to ensure a high -level of fundamental rights protection, this  exception \nshould be narrowly interpreted. (257) In particular,  therapeutic uses should be understood to mean uses of  CE-marked \nmedical devices. Moreover, t his exception does not comprise  the use of emotion \nrecognition systems to detect general aspects of wellbeing. The general monitoring of \nstress levels at the workplace is not permitted under health or safety aspects. For \nexample, an AI system intended to detect burnout or depression at the workplace or in \neducation institutions would not be covered by the ex ception and would remain \nprohibited. (258) The notion of safety reasons within this exception should be understood to apply only \nin relation to the protection of  life and health and not to protect other interests, for \nexample property against theft or fraud."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education]\n\n(259) It follows from this narrow interpretation of the exception that any use for medical and \nsafety reasons sh ould always remain limited to what is strictly necessary and \nproportionate, including limits in time, personal application and scale, and should be \naccompanied by sufficient safeguards. Such safeguards could include, for example, \nprior written and motivated expert opinion relating to t he specific use case. The \nnecessity should be assessed on an objective basis in relation to the medical and safety \npurpose, and not refer to the employer’s or educational institution’s ‘needs’. This \nassessment should inquire whether less intrusive alternative means exist which would \nachieve the same purpose. (260) Employers and educators should only deploy emotion recognition systems for medical \nand safety reasons in case of an explicit need.167 Data collected and processed in this \ncontext may not be used for any other purpose. This is particularly important given that"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Employment]\n\n166  Recital 44 AI Act. 167  In conformity with EU employment law, if such new technologies are introduced, employers shall also consult with workers or their \nrepresentatives, conform national procedures. Without respecting these procedural requirements, such systems cannot be introduced \nby reference to the AI Act as such. They will require also consent from the point of view of data protection legislation, which remains \napplicable."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 5]\n\n85 \n(265) As mentioned before, out of scope are: \n- AI systems inferring emotions and sentiments not on the basis of biometric data,  \n- AI systems inferring physical states such as pain and fatigue. (266) Emotion recognition systems used in all other domains other than in the areas of the \nworkplace and education institutions do not fall under the prohibition in Article 5(1)(f) \nAI Act. Such systems  are, however, considered high-risk AI systems .171 At the same \ntime, such systems may be prohibited in certain cases by virtue of Article 5(1)(a) and \n(b) AI Act  (harmful manipulation and exploitation) , or by virtue of other Union \nlegislation. All other  applicable legislation , such as U nion data protection law, \nconsumer protection etc. continue to apply to such systems. For example, \nEmotion recognition systems used in a commercial context  for addressing customers  \ndo not fall under the prohibition of Article 5(1)(f) AI Act, whether based on biometric \ndata or not. Hence, examples such as AI systems that enable emotion recognition based \non keystroke or based on voice messages of customers ( e.g., chat messages, use of \nvirtual voice assistants) , used in online marketing for applications for displaying \npersonalized messages and for advertisement purposes including in smart environments \n(‘intelligent billboards’) are not covered by the prohibition. Nevertheless, such practices may be covered by the prohibitions of harmful \nmanipulation and exploitation in Article 5(1)(a) and (b) AI Act,172 if all conditions for \nthe application of those prohibitions are met. a) Other systems out of scope \n(267) ‘Crowd control’ generally refers to the control and monitoring of the behaviour of \ngroups to maintain (public) order and event safety. It is often associated with large \ncrowd events (e.g., soccer or football games, concerts, etc) or specific places, such as \nairports or trains."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Education] [Article 5]\n\nCrowd control  systems can operate without inferring emotions of \nindividual persons, when for example analysing the general noise and mood level at a \ngiven place. In that case, the system would not fall within the scope of Article 5(1)(f) \nAI Act, because it does not infer emotions of (a concrete) natural person. (268) However, there may be instances where such crowd control systems infer emotions of \nindividuals, for example whether there are many angry faces. Normally, such AI \nsystems would not fall under the prohibition of Article 5(1) (f) AI Act, since they are \ntypically not used in the workplace or in education institutions. (269) Also out of scope are systems that are used in the medical field for example care robots, \nor medical practitioners using emotion recognition systems during an examination at \ntheir workplace, and voice monitors that analyse emergency calls."}
{"meta": {"section_type": "guideline", "article_ref": "ARTICLE 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [ARTICLE 5]\n\n86 \n(270) Such systems will often screen persons that are there in a work context, for example the \nsecurity staff at a football stadium or at a central station (where such systems are used \nto recognize aggressive behaviour) , or employees in the medical field. In such cases, \ndeployers must employ safeguards to avoid the screening of employees. However, it \ncannot be completely avoided that such systems also infer the emotions of those \nemployees. Since the primary objective of the system is not targeted at assessing \nemployees’ emotions, these systems should be considered to be outside the scope of the \nprohibition. Deployers of such systems remain responsible to ensure that employees are \nnot adversely affected by their use. 8. ARTICLE 5(1)(G) AI ACT: BIOMETRIC CATEGORISATION FOR CERTAIN \n‘SENSITIVE’ CHARACTERISTICS \n(271) Article 5(1) (g) AI Act prohibits biometric categorisation systems that categorise \nindividually natural persons based on their biometric data to deduce or infer their race, \npolitical opinions, trade union membership, religious or philosophical beliefs, sex -life \nor sexual orientation . This prohibition does not cover the labelling, filtering , or \ncategorisation of biometric data sets acquired in line with Union or national law, which \nmay be used, for example, for law enforcement purposes.173 \n8.1. Rationale and objectives  \n(272) A wide variety of information, including ‘sensitive’ information , may be extracted, \ndeduced or inferred from biometric information, even without the knowledge of the \npersons concerned, to categorise those persons. This may lead to unfair and \ndiscriminatory treatment, for example when a service is denied because somebody is \nconsidered to be of a certain race."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nAI-based biometric categorisation systems for the \npurpose of assigning natural persons to specific groups or categories, relating to aspects \nsuch as sexual or political orientation or race, violate human dignity and pose significant \nrisks to other fundamental rights , such as privacy and non-discrimination. They are \ntherefore prohibited by Article 5(1)(g) AI Act. 8.2. Main concepts and components of the prohibition \nArticle 5(1)(g) AI Act provides \nThe following AI practices shall be prohibited: \ng) the placing on the market, the putting into service for this specific purpose, or the \nuse of biometric categorisation systems that categorise individually natural persons \nbased on their biometric data  to deduce or infer th eir race, political opinions, trade \nunion membership, religious or philosophical beliefs, sex-life or sexual orientation; this \nprohibition does not cover any labelling or filtering of lawfully acquired biometric \ndatasets, such as images, based on biometric  data or categorising of biometric data in \nthe area of law enforcement;"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n87 \n(273) Several cumulative conditions must be fulfilled for the prohibition in Article 5(1)(g) AI \nAct to apply:  \n(i) The practice must constitute the ‘placing on the market’, ‘the putting into service \nfor this specific purpose’ or ‘the use’ of an AI system; \n(ii) The system must be a biometric categorisation system;  \n(iii) individual persons must be categorised; \n(iv)  based on their biometric data; \n(v) to deduce or infer their race, political opinions, trade union membership, religious \nor philosophical beliefs, sex-life, or sexual orientation. (274) For the prohibition to apply, all five conditions must be simultaneously fulfilled. The \nfirst condition, i.e. the placing on the market, the putting into service or the use of the \nAI system , is analysed in sectio n 2.3. The prohibition, therefore, applies to both \nproviders and deployers of AI systems, each within their respective responsibilities, not \nto place on the market, put into service or use such AI systems. The other conditions \nfor the application of the prohibition174 are further described and analysed below. (275) The prohibition does not cover the labelling or filtering of lawfully acquired biometric \ndatasets, including for law enforcement purposes. 8.2.1. Biometric categorisation system \n(276) ‘The categorisation of an individual by a biometric system is typically the process of \nestablishing whether the biometric data of an individual belongs to a group with some \npre-defined characteristic. It is not about  identifying an individual or verifying their \nidentity, but about assigning an individual  to a certain category. For instance, an \nadvertising display may show different adverts depending on the individual that is \nlooking at it based on the ir age or gender.’175 Persons may also simply be categorised \nfor statistical reasons , without being identified and without the objective to identify \nthem."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 3]\n\n(277) Article 3(40) AI Act defines a biometric categorisation system as an AI system for the \npurpose of assigning natural persons to specific cat egories on the basis of their \nbiometric data, unless it is ancillary to another commercial service and strictly necessary \nfor objective technical reasons. As explained in section 7.2.1.d), ‘biometric data’ is \ndefined in Article 3(34) AI Act . In particular, biometric data comprises behavioural \ncharacteristics that are based on biometric features. The scope of biometric \ncategorisation excludes  categorisation according to  clothes or accessories, such as \nscarfs or crosses, as well as social media activity. (278) Biometric categorisation may rely on categories of physical characteristics (e.g., facial \nfeatures and form , skin colour)  based on  which persons are assigned to specific \ncategories. Some of  these categories  may be of a special ‘sensitive’ nature ’ or"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n89 \n(282) Furthermore, for the prohibition to apply, natural persons must be ‘individually’ \ncategorised. If this is not the purpose or outcome  of the biometric categorisation, the \nprohibition does not apply, for example if a whole group is categorised without looking \nat the individual. Examples of individual categorisation include: \n- AI systems that conduct ‘Attribute Estimation’ (count demographics), including for \nexample ‘age, gender, ethnicity’, on the basis of for example bodily features, such as \nface, height, or skin, eye and hair colour (or a combination thereof). - AI systems capable of categorising individuals and singling them out based on a \nspecific feature (e.g., a scar under the right eye), or because they have a tattoo on their \nright hand. These use -cases are examples for individual biometric categorisation. For these \nexamples to fall within the prohibition of  Article 5(1)(g) AI Act all conditions of that \nprovision must be fulfilled. 8.2.3. To deduce or infer their race, political opinions, trade union \nmembership, religious or philosophical beliefs, sex -life or sexual \norientation \n(283) Article 5(1)(g) AI Act prohibits o nly biometric categorisation systems which have as \ntheir objective to deduce or infer a limited number of  sensitive characteristics: race, \npolitical opinions, trade union membership, religious or philosophical beliefs, sex -life \nor sexual orientation. For example, systems prohibited under Article 5(1)(g) AI Act include:  \n- a biometric categorisation system that claims to be capable of deducing an individual’s \nrace from their voice (This is different from a system that categorises persons \naccording to skin or eye colour, or a system that analyses the DNA of victims of crimes \nin view of their origin. Those systems would not be prohibited). - a biometric categorisation system that claims to be capable of deducing an individual’s \nreligious orientation from their tattoos or faces. 8.3."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nOut of scope \n(284) The prohibition in Article 5(1)(g) AI Act  does not cover AI systems engaged  in the \nlabelling or filtering of lawfully acquired biometric datasets, such as images, based on \nbiometric data, including in the area of law enforcement. This is further explained in \nrecital 30 AI Act.177 \n(285) The labelling or filtering of biometric datasets may be done by biometric categorisation \nsystems precisely to guarantee that the data equally represent all demographic groups, \nand not, for example, over-represent one specific group. If the data used for training an"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\n177  Recital 30 AI Act: ‘That prohibition should not cover the lawful labelling, filtering or categorisation of biometric data sets acquired \nin line with Union or national law according to biometric data, such as the sorting of images according to hair colour o r eye colour, \nwhich can for example be used in the area of law enforcement’."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n90 \nalgorithm are biased against a specific group (i.e. systematic differences in the data exist \nbetween groups due to the way the data are collected, or data is historically biased), the \nalgorithm may replicate this bias, possibly resulting in unlawful discrimination against \npersons or groups of persons.178 For this reason, labelling on the basis of some protected \nsensitive information may be necessary for high -quality data, precisely to prevent \ndiscrimination. The AI Act may even require labelling operations to conform to the AI \nAct’s requirements for high-risk AI systems.179 Such labelling or filtering of biometric \ndata is therefore explicitly exempted from the prohibition in Article 5(1)(g) AI Act. The \nprohibition only applies  where biometric data is categorised to infer race, political \nopinions, trade union membership, religious or philosophical beliefs, sex-life or sexual \norientation. Examples of permissible labelling or filtering include: \n- the labelling of biometric data to avoid cases where a member of an ethnic group has \na lower chance of being invited to a job interview because the algorithm was ‘trained’ \nbased on data where that particular group performs worse, i.e. has worse outcomes \nthan other groups.180 \n- the categorisation of patients using images according to their skin or eye colour may \nbe important for medical diagnosis, for example cancer diagnoses. (286) Article 5(1)(g) AI Act also provides that the prohibition in that provision does not apply \nto the l abelling or filtering of lawfully acquired datasets  in the area of law \nenforcement.181 \nFor example, this covers the use by a law enforcement authority of an AI system that \nallows labelling and filtering of a dataset suspected of containing child sexual abuse \nmaterial. In a first step, law enforcement would use the support of AI systems to detect \nand redact sensitive data from images."}
{"meta": {"section_type": "guideline", "article_ref": "Article 9", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 9]\n\nFurthermore, filtering and labelling according to \ngender, age, biometric data such as eye and hair colour, scars and marking could help \nwith identifying the victim s or creating links with other cases. Similarly filtering and \nlabelling abusers’ hands based on specific characteristics such as length of fingers  or \nany distinguishing markings or tattoos to help with identifying possible suspects  is \npermitted. 8.4. Interplay with other Union law  \n(287) AI sys tems intended to be used for biometric categorisation according to sensitive \nattributes or characteristics protected under Article 9(1) GDPR on the basis of biometric \ndata, in so far as these are not prohibited under this Regulation, are classified as high -\nrisk182 under the AI Act.183"}
{"meta": {"section_type": "guideline", "article_ref": "Article 10", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 10]\n\n178  Ibid. 179  See e.g. Article 10 and 17 AI Act. 180 FRA,  # BigData. Discrimination in data supported decision making, Luxemburg, 2018, 14, p. 5. 181 The AI Act concerning the use of biometric categorisation systems for law enforcement is based on Article 16 TFEU. See also Recital \n3 AI Act. 182 Recital 54 and Annex III, point 1 letter b) AI Act. 183 Recital 54 and Annex III, point 1 letter b). AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n91 \n(288) Article 5(1) (g) AI Act  further restricts the possibilities for a lawful personal data \nprocessing under Union data protection law, such as the GDPR, LED, EUDPR . In \nparticular, Article 5(1)(g) AI Act excludes the possibilities for biometric categorisation \nof natural persons, based on their biometric data, as defined in the AI Act, to infer race, \npolitical opinions, trade union membership, religious or philosophical beliefs, sex -life \nor sexual orientation, subject to the exception for labelling or filtering of lawfully \nacquired biometric data  sets, including in the area of law enforcement , as described \nabove. Moreover, the prohibition in Article 5(1)(g) AI Act is consistent with \nArticle 11(3) LED, which explicitly prohibits any ‘profiling’ that results in \ndiscrimination on the basis of special categories of personal data, such as race, ethnic \norigin, sexual orientation, political opinion, or religious beliefs. 9. ARTICLE 5(1) (H) AI ACT  - REAL-TIME REMOTE BIOMETRIC \nIDENTIFICATION (RBI) SYSTEMS FOR LAW ENFORCEMENT PURPOSES \n(289) Article 5(1)(h) AI Act prohibits the use of real-time RBI systems in publicly accessible \nspaces for law enforcement purposes, subject to limited exceptions exhaustively set out \nin the AI Act. Specifically, Article 5(1)(h)(i)-(iii) AI Act envisages three situations in \nwhich the use of such systems  may be permitted  where authorised by national \nlegislation and where the conditions and safeguards of Article 5(2) to ( 7) AI Act  are \nmet. (290) In accordance with Article 5(5) AI Act, Member States are free to decide whether and \nin which of the three situations the use of real-time RBI systems in publicly accessible \nspaces for law enforcement purposes is permitted  in their territory . In the absence of \nnational legislation allowing and regulating such use, law enforcement authorities and \nentities acting on their behalf may not deploy such systems  for law enforcement \npurposes."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nThe existence of national legislation that complies with the relevant \nrequirements of the AI Act is therefore a pre-requisite of such use. (291) Article 5(1)(h) AI Act  only prohibits the use of real -time RBI systems in publicly \naccessible spaces for law enforcement purposes, so that only deployers of such systems \nare concerned by that provision. The placing on the market and the putting into service \nof such systems, as well as the use of other RBI systems,  is not prohibited, but subject \nto the rules for high-risk AI systems  in accordance with Article 6(2) and point a) of \nAnnex I II AI Act .184 Where a  Member States authorises the use of  real-time RBI \nsystems in publicly accessible spaces for law enforcement purposes for any of the three \nobjectives listed in Article 5(1)(h) AI Act, the rules for high-risk AI systems also apply \nto that use. (292) Finally, specific rules apply to the retrospective use of RBI systems for law enforcement \npurposes. Such non-real-time use is not prohibited, but subject to additional safeguards \nfor the deployment of high-risk AI systems (Article 26(10) AI Act). 9.1. Rationale and objectives"}
{"meta": {"section_type": "guideline", "article_ref": "Article 26", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 26]\n\n184  In addition, specific rules applicable to the retrospective use of RBI systems for a law enforcement purposes (Article 26(10) AI Act)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n92 \n(293) Recital 32 AI Act acknowledges the intrusive nature of real-time RBI systems  in \npublicly accessible spaces for law enforcement purposes  to the rights and freedoms of \npersons concerned, to the extent that it may affect the private life of a  large part of the \npopulation, evoke a  feeling of constant surveillance , and indirectly dissuade the \nexercise of the freedom of assembly and other fundamental rights. Technical \ninaccuracies of AI systems intended for the remote biometric identification of natural \npersons can lead to biased results and entail discriminatory effects. Such possibly biased \nresults and discriminatory effects are particularly relevant with regard to age, ethnicity, \nrace, sex or disabilities. In addition, the immediacy of the impact and the limited \nopportunities for further checks or corrections in relation to the use of such systems \noperating in real-time carry heightened risks for the rights and freedoms of the persons \nconcerned in the context of, or impacted by, law enforcement activities. (294) However, where the use of such systems is strictly necessary to achieve a substantial \npublic interest and where the situations in which such use may occur  are exhaustively \nlisted and narrowly defined, that use outweighs the risks to fundamental rights (Recital \n33 AI Act). To ensure that such systems are used in a  ‘responsible and proportionate \nmanner’, their use is subject to  the safeguards and the specific obligations and \nrequirements in Article 5(2)-(7) AI Act. 9.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nMain concepts and components of the prohibition \nArticle 5(1)(h) AI Act \nThe following AI practices shall be prohibited: \nh) the use of ‘real-time’ remote biometric identification systems in publicly accessible \nspaces for the purpose of law enforcement, unless and in so far as such use is strictly \nnecessary for one of the following objectives: \ni) the targeted search for specific victims of abduction, trafficking in human beings \nor sexual exploitation of human beings, as well as the search for missing persons;  \nii) the prevention of a specific, substantial and imminent threat to the life or physical \nsafety of natural persons or a genuine and present or genuine and foreseeable threat of \na terrorist attack; \niii) the localisation or identification of a person suspected of having committed a \ncriminal offence, for the purpose of conducting a criminal investigation or prosecution \nor executing a criminal penalty for offences referred to in Annex II and punishable in \nthe Member State concerned by a custodial sentence or a detention order for a maximum \nperiod of at least four years. Point (h) of the first subparagraph is without prejudice to Article 9 of Regulation (EU) \n2016/679 for the processing of biometric data for purposes other than law enforcement. (295) Several cumulative conditions must be fulfilled for the prohibition in Article 5(1)(h) AI \nAct to apply:  \n(i) The AI system must be a RBI system,"}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 3]\n\n93 \n(ii) The activity consists of the ‘use’ of that system, \n(iii)in ‘real-time’,  \n(iv) in publicly accessible spaces, and \n(v) for law enforcement purposes. (296) The second condition, i.e. the ‘use’ of the AI system , has been already analysed in \nsection 2.3. of these Guidelines. The other conditions listed above are further described \nand analysed below. 9.2.1. The Notion of Remote Biometric Identification \n(297) Biometric recognition technologies detect, capture, and transform measurable physical \ncharacteristics (such as eye distance and size, nose length, etc.) or behavioural \ncharacteristics (such as gait or voice) into machine-readable biometric data (see section \n7.2.1.d) above). These data are available in different forms: images or templates, which \nare a mathematical representation of the salient features  of an individual,  used for \nrecognition purposes. Biometric recognition technologies are used for verification and \nidentification purposes.185 \n(298) According to Article 3(41) AI Act, a RBI system is \n[a]n AI system for the purpose of identifying natural persons, without their \nactive involvement, typically at a distance through the comparison of a person’s \nbiometric data with the biometric data contained in a reference database. (299) This definition covers only  the identification functionality of biometric  recognition \nsystems, which implies the absence of active involvement of the persons concerned (i.e. no active participation) and results in the capture of the characteristics of those persons \ntypically at a distance. For identification performance, the captured biometric data are \ncompared with biometric data already stored in a reference database (such as a \nrepository, e.g., a criminal database containing facial images or templates of suspects)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 3]\n\na) Identification purposes only \n(300) The notion of ‘biometric identification’ is defined in Article 3(35) of the AI Act as  \nthe automated recognition of physical,  physiological and behavioural or \npsychological human features for the purpose of establishing the identity of \na natural person by comparing biometric data of that individual to biometric \ndata of individuals stored in a database. (301) Recital 15 AI Act further clarifies that such human features may comprise \nthe face, eye movement, body shape, voice, prosody, gait, posture, heart rate, \nblood pressure, odour, keystroke characteristics, \n(302) AI systems used for following natural persons can also be included in the definition of \nbiometric identification, for example to see in which direction a suspect escapes. This"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Biometrics", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Biometrics]\n\n185 As defined by the biometrics community in ISO/IEC Standard 2382 -37:2022 Information Technology - Vocabulary, Biometric \nrecognition, Term 37.01.03."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Biometrics", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Biometrics]\n\n186  Recital 17 AI Act. 187  E.g., Ross A, Jain AK (2015) ‘Biometrics, Overview’ in Li S.Z. and Jain A.K. (eds) Encyclopedia of Biometrics, (1 st ed. Springer \nScience, New York), pp. 289-294."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n95 \n- Systems that are used to give access  to the metro station , such as biometric metro \ntickets, where persons are actively involved and consciously approach the biometric \nsensor to obtain access, do not fulfil that condition. (307) Biometric recognition systems that process (contactless) fingerprints, gait, voice, DNA, \nkeystrokes and other (biometric) behavioural signals may also constitute RBI \nsystems.188 \nFor example, \n- A voice biometric technology system may be  deployed to identify a person \nspeaking. The microphone then collects the biometric sample. - A gait recognition system may be used via CCTV and the videos are automatically \nchecked for matches with previously captured templates. - Keystroke biometric technology may be used to identify the person typing a \nfraudulent message. The fact that these systems are given as examples of RBI systems does not imply that \nthey are prohibited under Article 5 AI Act. (308) In the case of body -cams capable of RBI used by individual law enforcement agents, \nthe untargeted filming during, for example , a demonstration with hundreds of \nparticipants will be considered to fulfil the condition of remoteness. c) Reference database \n(309) Identification is not possible without a reference database containing biometric data for \ncomparison purposes. Thus, the existence of a reference database is indispensable to \nperform the comparison for identification purposes.189 \nFor example, in the case of  missing persons, the Schengen Information System190 \ndatabase could be used as the reference database for facial recognition purposes (once \noperational). 9.2.2. Real-time \n(310) Real-time means that the system capture s and further process es biometric data \n‘instantaneously, near-instantaneously or in any event without any significant delay.’191 \nAll the processing steps, i.e."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\nthe capture, comparison, and identification of biometric \ndata, occur simultaneously or almost simultaneously, which may include a ‘limited \nshort delay’ to avoid the prohibition being circumvented through the retrospective use \nof RBI systems.192 The notion of ‘without a significant delay’ is not defined  in the AI"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n96 \nAct; it will have to be assessed on a case -by-case basis. As the devices used for real -\ntime or post -remote identification are increasingly one and the same with differ ent \nfunctionalities, the distinction is temporal. Generally speaking, a delay is significant at \nleast when the person is likely to have left the place where the biometric data was taken. (311) Real-time systems in general are used at a given place to facilitate a quick reaction and \nnot to retrospectively identify persons. They provide the user of the system with a means \nto track the movements of persons under surveillance and to monitor them. a) An AI system screens all incoming visitors to a concert venue: Real-time RBI \nb) A system films all incoming visitors to a concert. An incident happens  at the \nconcert. After the concert, the identification system is operated on the video material \nin order to identify the offender: Post-RBI. (312) When a law enforcement authority covertly takes a picture of a person via a mobile \ndevice and submits it to a database for immediate search, depending on the \ncircumstances, this may fall under the prohibition of Article 5(1)(h) AI Act. 9.2.3. In publicly accessible spaces \n(313) Article 3(44) AI Act defines publicly accessible spaces as \nany publicly or privately owned physical space accessible to an undetermined \nnumber of natural persons, regardless of whether certain conditions for access \nmay apply, and regardless of the potential capacity restrictions. (314) Recital 19 AI Act lists several elements that characterise such spaces: \n- Accessibility to an undetermined number of persons, independently of the potential \ncapacity or security restrictions, such as purchasing a ticket or title of transport, \nprior registration or having a certain age."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\nThe possibility of getting access to a space \nthrough an unlocked door does not mean that the space is publicly accessible if \nindications or circumstances suggest the contrary (such as a sign restricting access). Moreover, the access to a space can be limited to certain persons, as defined by law, \nlinked to public safety or security, or to the decision of the person having the \nrelevant authority over the space. For example, publicly accessible spaces are in principle:  \n- a concert venue for which participants pay an entrance fee. - an event location where a trade fair is organised targeting participants over \nthe age of 50. A space closed by a gate, even if the gate is unlocked, such as the gated entrance of \na fenced residential area of several houses, will normally not be considered a publicly \naccessible space. By contrast, a park in a gated residence with public opening hours \nwithout any access restrictions during those hours will generally constitute a publicly \naccessible space during those hours and a closed space outside those hours."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n98 \ncontrol (where the customs officials stand and passports or ID checks occur) is excluded \nfrom the scope of the prohibition. (318) As clarified in Recital 19 AI Act, assessing whether a space is accessible to the public \nshould be done based on a case-by-case analysis. 9.2.4. For law enforcement purposes \n(319) The prohibition in Article 5(1)(h) AI Act applies to the use of RBI systems for  law \nenforcement purposes, irrespective of the entity, authority, or body carrying out the law \nenforcement activities. (320) Law enforcement is defined in Article 3(46) AI Act as the ‘activities carried out by law \nenforcement authorities or on their behalf for the prevention, investigation, detection or \nprosecution of criminal offences or the execution of criminal penalties, including \nsafeguarding against and preventing threats to public security.’ These purposes are the \nsame as those listed in Article 1 LED.196 Thus, any interpretation of those purposes in \nrelation to LED may also be relevant for the purpose of interpreting the notion of  ‘law \nenforcement’ used in the AI Act. (321) Law enforcement purposes comprise the investigation, detection, and prosecution of \ncriminal offences. They also comprise activities in relation to the prevention of criminal \noffences, including safeguarding against and the prevention of threats to public security, \nbefore any crime has actually been committed. For instance,  the police may take \n‘coercive measures at demonstrations, major sporting events or riots’ in the context of \ncrime prevention.197 Finally, those activities comprise the execution of penalties, such \nas the execution of sentences. (322) According to Article 3(46) AI Act, law enforcement activities may be performed by \nlaw enforcement authorities or on their behalf. Law enforcement authorities are further \ndefined in Article 3(45) AI Act in the same manner as national competent authorities \nare defined in the LED."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\n198 That definition covers law enforcement authorities and \nentrusted bodies or entities (which may be private parties): \n(a) any public authority competent for the prevention, investigation, detection \nor prosecution of criminal offences or the execution of criminal penalties, \nincluding the safeguarding against and the prevention of threats to public \nsecurity; or \nFor example, such public authorities include police authorities and criminal justice \nauthorities (such as prosecutors) when they carry out a law enforcement task. (b) any other body or entity entrusted by Member State law to exercise public \nauthority and public powers for the purposes of the preventio n, \ninvestigation, detection or prosecution of criminal offences or the execution"}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 3]\n\n196 Some activities of law enforcement authorities are excluded from the scope of the LED, such as when they perform administrati ve \ntasks (such as human resources), these activities are carried outside the law enforcement framework. They fall under the GDPR. See \nRecital 19 GDPR. 197 Recital 12 LED. 198 Article 3(7) LED."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n99 \nof criminal penalties, including the safeguarding against and the prevention \nof threats to public security; \n(323) Under the AI Act other entities, bodies, or persons, may exercise l aw enforcement \nactivities after being entrusted by Member States law entrusting them public authority \nand public powers for the purposes listed above. (324) ‘On behalf of’ means that a law enforcement authority has delegated the performance \nof a law enforcement activity (or part of it) to another entity or person, including private \nparties, or has requested in specific cases another entity or person to act to support law \nenforcement activities. In both cases, the law enforcement authorities must instruct on \nall major aspects and supervise the other entity, as this requirement is inherent to the \nnotion of acting ‘on behalf’ of a person. Delegation of tasks to other bodies may include, for example,  \n- Public transport companies requested by law enforcement authorities to \nensure security on the public transport networks  under their instructions \nand supervision. - Sports federations requested by law enforcement authorities to act under \ntheir instructions and supervision to provide security at sporting events. - Banks that are requested by law enforcement authorities to conduct certain \nactions to ‘counter certain crimes in specific cases’ under the instructions \nand supervision of law enforcement authorities. These activities fall within the definition of “for the purpose of law enforcement” \nsince those entities act ‘on behalf’ of law enforcement authorities. If those entities act \non their ‘own behalf’  when detecting and countering crimes (such as fraud , money \nlaundering), they will not be considered to fall under the prohibition of Article 5(1)(h) \nAI Act. (325) Only when those other bodies or entities have been entrusted with a specific law \nenforcement task will their activities fall under the definition of ‘law enforcement’. 9.3."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nExceptions to the prohibition \n(326) The AI Act provides three exceptions to the general prohibition on the use of real-time \nRBI in publicly accessible spaces for law enforcement purposes. Article 5(1)(h)(i) to \n(iii) AI Act exhaustively list s three objectives for which real -time RBI may be \nauthorised, while Article 5(2) to 5(7) AI Act lays down the conditions and safeguards \nfor such authorisation. Article 5(1)(h)(i)-(iii) AI Act does not in itself constitute a legal \nbasis for the real-time use of RBI systems in publicly accessible spaces. Rather, only a \ndomestic Member State law that fulfils, in particular, the requirements in Article 5(2)-\n(7) AI Act can allow the use of real-time RBI, as provided by Article 5(2) AI Act. Consequently, in the absence of Member State legislation authorising the use of real -\ntime RBI for one or more of those objectives, such use is prohibited as from 2 February \n2025."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n100 \n9.3.1. Rationale and objectives \n(327) The objectives set out in Article 5(1)(h)(i)-(iii) AI Act aim to allow the use of certain \nAI and investigative tools for law enforcement purposes. These objectives are: \n(i) the targeted search of victims of three specific serious crimes and missing \npersons [protection];  \n(ii) the prevention of imminent threats to life or physical safety or a genuine \nthreat of terrorist attacks [prevention]; and  \n(iii) the localisation or identification of suspects and offenders of certain serious \ncrimes as listed in Annex II [prosecution/investigation]. (328) In those scenarios, the Union legislature has balanced the security needs of society \nagainst the risk that real-time RBI systems pose to the fundamental rights of individuals \nsubject to those systems. According to Recital 33 AI Act, the objectives for which the \nuse of real -time RBI systems for law enforcement purposes in publicly accessible \nspaces is allowed must be strictly, exhaustively, and narrowly defined, and appear when \nthere is a ‘strict necessity’ to achieve ‘a substantial public interest’ which ‘outweighs \nthe risks’ posed to fundamental rights. Any other  use of  real-time RBI systems in \npublicly accessible spaces for law enforcement purpose s which is not listed in Article \n5(1)(h)(i)-(iii) AI Act is prohibited. For instance, the use of real-time RBI systems by the police to identify a shoplifter and \ncompare their facial images against criminal databases is prohibited, as it does not fall \nunder any of the objectives listed in Article 5(1)(h)(i)-(iii) AI Act. 9.3.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nTargeted search for the victims of three serious crimes and missing \npersons \n(329) According to Article 5(1)(h)(i) AI Act, the use of real-time RBI in publicly accessible \nspaces for law enforcement purposes is allowed , subject to strict necessity and the \nconditions in Article 5(2) -(7) AI Act, for the targeted search of victims of abduction, \ntrafficking in human beings , or sexual exploitation of human beings, as well as the \nsearch for missing persons. a) Targeted search for victims of three types of crimes \n(330) The scenario described in Article 5(1)(h)(i) AI Act  seeks to assist law enforcement \nauthorities to search for victims of three serious crimes. (331) A targeted search would involve the localisation and identification of victims. Three types of crimes"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n101 \n(332) The targeted search  for specific  victims of three serious crimes are covered  by the \nscenario listed in Article 5(1)(h)(i) AI Act: the abduction of, trafficking in, and sexual \nexploitation of human beings.199 \nIf, for example , a child is kidnapped and there are concrete indications that the \nkidnapper intends to bring the child from one place to another  by car, the police may \nuse a real-time RBI system for the targeted search of that child , but it must define a \nperimeter of deployment and duration of use to identify the child. b) Searching for missing persons  \n(333) The first scenario also covers the search for a missing person.200  \n(334) A distinction may be made between missing children and missing adults, since the \nvoluntary disappearance of a missing adult will not always trigger a search . The \napplicable rules regarding missing children vary considerably from one Member State \nto another.201 In any event, Article 5(1)(h)(i) AI Act only allows the use of a real-time \nRBI system to search for missing persons for law enforcement purposes. (335) The disappearance of an adult does not always lead to a search of that person by police, \nas adults have the right to disappear. A search could be linked to the legal status of the \nperson (‘under curatorship’), their health condition (a mental illness), the existence of \na suicidal note, but also the departure without personal belongings. If the circumstances \nof the disappearance are a cause for concern, the disappearance may be filed with the \npolice so that a search can start. (336) In some Member States , the search for a missing person  may occur  under an \nadministrative procedure and not for law enforcement purposes."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nFor example, where a \nvulnerable person  is missing, but there is no suspicion of a crime  or any other law \nenforcement purpose, the use of real-time RBI systems to search for that person would \nnot be deemed to be for law enforcement purposes and  would therefore fall under the \nrules for such use under the GDPR. 9.3.3. Prevention of imminent threats to life or terrorist attacks \n(337) Article 5(1)(h)(ii) AI Act lists the second scenario in which the use of real-time RBI in \npublicly accessible spaces for law enforcement purposes is allowed , subject to strict \nnecessity and the conditions established in Article 5(2)-(7) AI Act"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Migration]\n\n199 Kidnapping, trafficking in human beings, and sexual exploitation are three crimes that can trigger a European Arrest Warrant (EAW) \nto arrest and transfer a criminal suspect or a sentenced person to the country that issued the EAW. The three crimes relate mostly but \nnot exclusively to women and children. According to the European Commission’s DG Migration and Home Affairs, almost 40 percent \nof the victims are EU citizens, and most of them are wom en and children trafficked for sexual exploitation. The number of men \nvictims has nearly doubled in ten years. They are trafficked for forced labour and forced begging, while most of the women an d \nchildren are trafficked for sexual exploitation. https://home-affairs.ec.europa.eu/policies/internal-security/organised-crime-and-\nhuman-trafficking/together-against-trafficking-human-beings_en \n200  A ‘missing person’ is not defined at EU level. But in Council Conclusions of December 2021 on ‘Stepping Up Cross-Border Police \nCooperation in the area of Missing Persons’, the Council takes as reference both the definition of a missing person in the Council of \nEurope’s Recommendation CM/Rec (2009) 12 and in national regulations. Council Conclusions (2021) 14808/21, para 11, page 4. 201  European Commission, European Migration Network, ‘How do EU Member States treat cases of missing unaccompanied minors?’ \nEMN Inform, 2020."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Critical Infrastructure", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Critical Infrastructure] [Article 2]\n\n102 \nthe prevention of a specific, substan tial and imminent threat to the life or \nphysical safety of natural persons or a genuine and present or genuine and \nforeseeable threat of a terrorist attack. a) Specific, substantial and imminent threat to life or physical safety of natural \npersons \n(338) In application of Article 2 of the Charter, which guarantees the right to life, the U nion \nand its Member States must safeguard and, thus, protect the lives of individuals. The \ncriteria in Article 5(1)(h)(ii) AI Act concerning the threat to life to allow for the use of \nreal-time RBI systems in publicly accessible spaces  require the existence of (1) a \nspecific, (2) substantial and (3) imminent threat to the life or physical safety of (4) \nnatural persons. The threat does not need to be limited to identified indiv iduals or a \ngroup, as it relates to natural persons in general. (339) Recital 33 AI Act clarifies that an imminent threat to life or the physical safety of natural \npersons may also include an imminent threat to critical infrastructure 202 ‘where the \ndisruption or destruction of such critical infrastructure would result in an imminent \nthreat to the life, or the physical safety of a person, including through serious harm to \nthe provision of basic supplies to the population or to the exercise  of the core function \nof the State.’ \nFor example,203  \nA serious disruption and destruction of critical infrastructure ( e.g., a power plant, \nwater supply, or a hospital) may result in an imminent threat to the life or the physical \nsafety of a person when the re is serious harm  of cessation of basic supplies to the \npopulation (deprivation of electricity or drinkable water for a long period, in a \nparticularly warm or cold weather, etc)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 2]\n\n102 \nthe prevention of a specific, substan tial and imminent threat to the life or \nphysical safety of natural persons or a genuine and present or genuine and \nforeseeable threat of a terrorist attack. a) Specific, substantial and imminent threat to life or physical safety of natural \npersons \n(338) In application of Article 2 of the Charter, which guarantees the right to life, the U nion \nand its Member States must safeguard and, thus, protect the lives of individuals. The \ncriteria in Article 5(1)(h)(ii) AI Act concerning the threat to life to allow for the use of \nreal-time RBI systems in publicly accessible spaces  require the existence of (1) a \nspecific, (2) substantial and (3) imminent threat to the life or physical safety of (4) \nnatural persons. The threat does not need to be limited to identified indiv iduals or a \ngroup, as it relates to natural persons in general. (339) Recital 33 AI Act clarifies that an imminent threat to life or the physical safety of natural \npersons may also include an imminent threat to critical infrastructure 202 ‘where the \ndisruption or destruction of such critical infrastructure would result in an imminent \nthreat to the life, or the physical safety of a person, including through serious harm to \nthe provision of basic supplies to the population or to the exercise  of the core function \nof the State.’ \nFor example,203  \nA serious disruption and destruction of critical infrastructure ( e.g., a power plant, \nwater supply, or a hospital) may result in an imminent threat to the life or the physical \nsafety of a person when the re is serious harm  of cessation of basic supplies to the \npopulation (deprivation of electricity or drinkable water for a long period, in a \nparticularly warm or cold weather, etc)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Critical Infrastructure", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Critical Infrastructure] [Article 5]\n\n(340) What constitutes an imminent threat to life or the physical safety of natural persons is \nultimately defined and assessed at the level of the Member State based on its national \nlaws, in accordance with EU law, in particular taking into account the key elements and \nrationale of Article 5 AI Act. This will have to be laid down/referred to  in the laws \nMember States must adopt to make use of the exceptions to the prohibition on the use \nof real-time RBI for law enforcement purposes in publicly accessible spaces. (341) An imminent threat to life or physical safety is a threat that can occur at any mo ment \nand requires ‘immediate action to be taken.’204 A substantial threat to physical safety \nrelates to serious bodily injuries. (342) A specific threat means that the threat is clearly defined , individualised and concrete, \nin that it should not be hypothetical or relate to certain dangers in general."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n(340) What constitutes an imminent threat to life or the physical safety of natural persons is \nultimately defined and assessed at the level of the Member State based on its national \nlaws, in accordance with EU law, in particular taking into account the key elements and \nrationale of Article 5 AI Act. This will have to be laid down/referred to  in the laws \nMember States must adopt to make use of the exceptions to the prohibition on the use \nof real-time RBI for law enforcement purposes in publicly accessible spaces. (341) An imminent threat to life or physical safety is a threat that can occur at any mo ment \nand requires ‘immediate action to be taken.’204 A substantial threat to physical safety \nrelates to serious bodily injuries. (342) A specific threat means that the threat is clearly defined , individualised and concrete, \nin that it should not be hypothetical or relate to certain dangers in general."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n103 \nFor example, the police are informed that a former student plans a deadly attack at \nhis former university as he seeks revenge on several former classmates. The police \nreceives information about the imminence of the attack, the targeted school, and the \nweapons he plans to use to execute his plans. (343) A specific threat needs not be intentional. Non-intentional actions could also result in a \nthreat to life or physical safety. b) A genuine and present or genuine and foreseeable threat of a terrorist attack \n(344) This part of the second scenario described in Article 5(1)(h)(ii) AI Act is comprised of \nseveral elements: the existence of a threat of a terrorist attack and the characteristics of \nthe threat, which must be genuine and present or genuine and foreseeable. Threat of a terrorist attack \n(345) The assessment concerning the existence and seriousness of the threat is ma de at \nnational level when assessing the actual circumstances of a measure to be taken to \nsafeguard national security, and more specifically, in case of a terrorist attack. The \nterrorist threat level is defined at national level  and varies from one Member State \nto another. For example, the Netherlands has established five levels of threats, 205 \nBelgium four,206 France three, 207 and Sweden five. 208 However, the concept of ‘a \ngenuine and present or genuine and foreseeable threat’, as used in Article 5(1)(h)(ii), is \nan autonomous notion of U nion law and should therefore be assessed, in principle, \nindependently of national definitions. The threat relates not to terrorism in general, but \nspecifically to a threat of a terrorist attack."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nCharacteristics of the threat: genuine and present or genuine and foreseeable \n(346) The threshold of seriousness that a threat needs to reach to allow for the use of real-\ntime RBI systems in publicly accessible spaces for law enforcement purposes  was \ninspired by the CJEU’s case law on data retention and passenger name record measures \naimed at safeguarding national security, in particular, against terrorist attacks. According to the CJEU, in those contexts, ‘a threat to national security must be genuine \nand present, or at the very least, foreseeable, which  presupposes that sufficiently \nconcrete circumstances have arisen.’209 \nPrevention  \n(347) Contrary to Article 5(1)(h)(i) and Article 5(1)(h)(iii)  AI Act, the scenario described in \nArticle 5(1)(h)(ii) does not specify that the use of real-time RBI is permitted to locate \nor identify a concrete person. Its purpose is  the prevention of a particular threat. Accordingly, the scenario may also cover the use of real-time RBI to detect and follow"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n105 \n- sabotage, \n- participation in a criminal organisation involved in one or more of the offences listed \nabove. a) Localisation and identification  \n(349) A Member State may authorise the use of real-time RBI in publicly accessible spaces \nfor law enforcement purposes to locate and identify a suspect of a criminal offence to \nconduct a criminal investigation, prosecute that person for the crime  committed, or \nexecute an existing sentence. b) Suspects and Perpetrators  \n(350) Article 5(1)(h)(iii)  AI Act  covers two categories of individuals: suspects and \nperpetrators. A suspect is a person with regard to whom there are serious grounds for \nbelieving that they have committed a criminal offence , and sufficient evidence of that \nperson’s involvement in the offence has already been gathered. A perpetrator is a person \nwho is accused or convicted  of having committed a criminal offence . The same \nconditions (crime listed in Annex II and maximum punishment of at least four years) \napply to locate or identify the accomplice of the crimes listed in Annex II AI Act. c) List of serious crimes  \n(351) Only serious crimes justify the use of real -time RBI systems  in publicly accessible \nspaces for law enforcement purposes. (352) The first five offences listed in Annex II AI Act are the same as the ‘euro crimes’; listed \nin Article 83 TFEU, while the other offences constitute priorities for law enforcement \ncooperation.210 Some of them ( e.g., kidnapping, illicit trafficking in nuclear or \nradioactive materials) may be linked to terrorism.211 \n(353) Although all the criminal offences listed in Annex II may trigger the issuance of a \nEuropean Arrest Warrant (‘EAW’) against a suspect or perpetrator, the use of real-time \nRBI to locate and identify a suspect for one of these serious criminal offences does not \nrequire that an EAW has been issued."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\n(354) Moreover, to use real-time RBI for this purpose, the respective criminal offence must \nbe punishable in the Member State concerned by a custodial sentence or a detention \norder for a maximum period of at least four years. During a busy festival in a city, police authorities deploy live facial recognition \ntechnologies to monitor the area around the festival and identify wanted individuals \nwith outstanding arrest warrants for illegal drug trafficking and sexual offences. At \ndifferent entrances to the festival, the police use live video footage of people passing \nin front of a mobile camera to compare their faces with a watchlist of faces of wanted \nindividuals."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n106 \nFirst, concerning the offence types, RBI can be used in case of illegal drug \ntrafficking. However sexual offences are not on the list of offences, unless they relate \nto the sexual exploitation of children, child sexual abuse material, or rape. The police \nare not allowed to deploy real-time facial recognition technologies  in a broad, \nuntargeted manner, i.e. in the hope of finding wanted criminal s and taking them off \nthe streets. The case is different if the police have received a physical description with a \nphotograph of a wanted individual  that is subject to a European Arrest Warrant for \ndrug trafficking and they have reasons to believe that he will be present at the festival. In those circumstances , deploying real -time facial recognition technologies to \nidentify a targeted individual may be covered by Article 5(1)(h)(iii) AI Act. After a serious terror attack at a Christmas market with 12 deaths, the police uses \nreal-time facial recognition technologies to identify the offender and to see to where \nhe is escaping. In that context they als o use the real -time facial recognition \ntechnologies of the nearby train station and at destination stations of the trains leaving \nfrom there shortly after the attack. In the case of a terror attack, such use can be \npermitted under Article 5(1)(h)(iii) AI Act. (355) A link between Article 5(1)(h)(i) and Article 5(1)(h)(iii) AI Act may be made for the \ncrimes covered by the scenario described in Article 5(1)(h)(i) AI Act. While real-time \nRBI systems may be deployed to find a victim or a missing person, those systems may \nalso be used to locate and identify the perpetrator or suspect of trafficking in human \nbeings, sexual exploitation as far as it concerns children (as listed in Annex II), and \nkidnapping (as far as the abduction mentioned in Article 5(1)(h)(i) AI Act qualifies as \nkidnapping as listed in Annex II AI Act)."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article \n5]\n\nA link may also be made between Article \n5(1)(h)(ii) and (iii) AI Act: real-time RBI systems may be used to prevent a threat falling \nwithin the scope of Article 5(1)(h)(ii) and, if that threat materialises, those systems may \nbe used to identify/locate the perpetrator ‘on the move’. 10. SAFEGUARDS AND CONDITIONS FOR THE EXCEPTIONS  (ARTICLE 5(2)-\n(7) AI ACT) \n10.1. Targeted individual and safeguards (Article 5(2) AI Act) \nArticle 5(2) AI Act provides: \n‘The use of ‘real-time’ remote biometric identification systems in publicly accessible \nspaces for the purposes of law enforcement for any of the objectives referred to in \nparagraph 1, first subparagraph, point (h), shall be deployed for the purposes set out \nin that point only to confirm the identity of the specifically targeted individual, and \nit shall take into account the following elements:"}
{"meta": {"section_type": "guideline", "article_ref": "Article  27", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article  27]\n\n107 \n(a) the nature of the situation giving rise to the possible use, in particular the \nseriousness, probability and scale of the harm that would be caused if the \nsystem were not used; \n(b) the consequences of the use of the system for the rights and freedoms of \nall persons concerned, in particular the seriousness, probability and scale \nof those consequences. In addition, the use of ‘real-time’ remote biometric identification systems in publicly \naccessible spaces for the purposes of law enforcement for any of the objectives \nreferred to in paragraph 1, first subparagraph, point (h), of this Article shall comply \nwith necessary and proportionate safeguards and conditions in relation to the use in \naccordance with the national law authorising the use thereof, in particular as regards \nthe temporal, geographic and personal limitations. The use of the ‘real -time’ remote \nbiometric identification system in publicly accessible spaces shall be authorised only \nif the law enforcement authority has completed a  fundamental rights impact \nassessment as provided for in Article  27 and has registered the system in the EU \ndatabase according to Article 49. However, in duly justified cases of urgency, the use \nof such systems may be commenced without the registration in the EU database, \nprovided that such registration is completed without undue delay.’ \n(356) The use of real-time RBI systems for one of the objectives listed in Article 5(1)(h)(i) to \n(iii) AI Act is subject to certain safeguards and conditions, which are detailed in Article \n5(2) to Article 5(7) AI Act. (357) First, the use of real-time RBI systems  in publicly accessible spaces for law \nenforcement purposes  is only allowed  to ‘confirm the identity of the specifically \ntargeted individual.’ This first condition aims to balance the seriousness of the situation \nand the harm resulting from not using the system with the impact of the technology on \nindividuals’ rights and freedoms."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nIt aims to avoid mass surveillance by targeting an \nindividual for the deployment of real-time RBI. As a consequence, the deployment of \na real-time RBI system in publicly accessible spaces  for law enforcement purposes  \nshould only be authorized for targeted individuals. (358) The use of the expression ‘confirming the identity’ , as opposed to ‘identification’, is \nmeant as an additional safeguard for the fundamental rights limiting the risk of \nindiscriminate surveillance and implies that the identification of an individual within \nthe meaning of Article 5(1)(h)  AI Act must be targ eted. That expression  should be \nunderstood as meaning that the use of real-time RBI may only be initiated to search for \nspecific individuals for which the law enforcement authorities have reasons to believe \nor are informed that they are victims of the crimes listed in Article 5(1)(h)(i) AI Act or \nare involved in one of the scenarios described in Article 5(1)(h)(ii) or Article 5(1)(h)(iii) \nAI Act. This means, in practice, a comparison of the data collected real -time with the \ndata contained in the reference database. As regards the use of real-time RBI system in \nthe scenarios described  in Article 5(1)(h)(ii ) AI  Act and for conducting a criminal \ninvestigation within the meaning of 5(1)(h)(iii) AI Act, law enforcement authorities do"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n108 \nnot necessarily need to know the identity of the individuals they are searching for before \nusing the system . I f they have factual indications and information about a planned \nterrorist attack by a terrorist group (without knowing who will execute the plan) at a \nspecific time and place, the RBI system  may be used to identify the offender from the \nterrorist group, provided the law enforcement authorities have  constituted a reference \ndatabase containing the biometric data of the individuals forming part of the terrorist \ngroup. In all three scenarios described in Article 5(1)(h)(i) to (iii) AI Act, ‘confirming \nthe identity’ may also include the localisation of the person in question. (359) Second, before using the system, the nature of the situation giving rise to the possible \nuse, in particular, the seriousness, probability and scale of the harm for natural persons, \nsociety and law enforcement purposes that would be caused if the system were not used, \nshould be assessed against the consequences of the use of the system on the rights and \nfreedoms of the persons concerned, in particular, the seriousness, probability and scale \nof those consequences. This should include evaluating whether less intrusive alternative \nsolutions are available to the law enforcement authorities or entit ies acting on their \nbehalf. For example, law enforcement authorities are prohibited from using  real-time facial \nrecognition systems in the street based on general security, crime prevention and over-\ncrowding concerns, since that would involve the constant monitoring and surveillance \nof all persons, it is not limited in time , and it would therefore not meet the criteria for  \nthe exception from the prohibition laid down in Article 5(1)(h) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\n(360) The ‘ seriousness’ criterion , applied here in connection to the possible harm and \nconsequences, implies a variation in degrees of interference with the fundamental rights \nat stake, which is linked to the principle of proportionality. 212 Concerning the \ninterferences with fundamental rights, some interfe rences are viewed as more serious \nthan others. (361) The ‘ scale’ criterion refers , in particular,  to the number and categories of persons \naffected by the interference (including children and vulnerable or marginalised \npersons). (362) Finally, the ‘probability’ is the likelihood that an event will occur. (363) The assessment of the seriousness, scale and probability of the harm and consequences \nshould all  be part of the Fundamental Rights Impact Assessment that the law \nenforcement authority is obliged to complete (see below) . That assessment will be \nconcluded on a case-by-case basis. (364) Third, the real-time use of RBI should be clearly limited in terms of geographic scope, \nduration, and the targeted person. This is to ensure that the RBI system is only used \nwhen strictly necessary."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n109 \n(365) Concerning the geographic restriction, it may cover one or several geographical areas \nbased on ‘objective and non -discriminatory factors’ . In the case of biometric \nidentification this implies that the geographic restriction applies to a clearly delineated \nboundary for which there are indications that the event will take place. Such a \ndelineation should -under normal circumstances- not comprise an entire city or country, \nbut should be more targeted. (366) Another safeguard relates to the personal scope  of the measure, i.e. defining the \ncategories of persons concerned . This would exclude the untargeted, indiscriminat e \nidentification of persons, without further indications of an incident. (367) Finally, the time limit is a period limited to what is strictly necessary, but which may \nbe extended in case of need  in accordance with the applicable rules. The use of real -\ntime RBI systems therefore cannot be for an indefinite or vague period  of time. The \nperiod needs to be determined in light of the concrete indications that lead to the use of \nRBI systems. (368) Fourth, before deployment, the law enforcement authority deploying the real-time RBI \nsystem must have conducted a Fundamental Right Impact Assessment (FRIA) and \nregistered the system in the EU database (except in a duly justified case). 10.1.1. Fundamental Rights Impact Assessment \n(369) FRIAs carried out in application of Article 5(2) AI Act must comply with the conditions \nlaid down in Article 27  AI Act. Th at provision sets out  the requirements concerning \nFRIAs applicable to high-risk AI systems."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n(370) In the period between when the prohibitions  in Article 5 AI Act  become applicable \n(after 2 February 2025) but the provisions on high-risk AI systems are not yet applicable \n(before 2 August 2026), the requirements for FRIA set out in Article 27 AI Act should \nbe implemented by the deployers  of real-time RBI systems meeting the conditions to \nbenefit from one or more of the exceptions in Article 5(1) (h) AI Act . The following \nprovisional guidance relates only to the use of real -time RBI in publicly accessible \nspaces for law enforcement purposes for the period before the obligations for high-risk \nAI systems become applicable and the Commission adopts the template for FRIA and  \nprovides further guidance on the obligation under Article 27 AI Act. (371) A FRIA is a new type of impact assessment that aims to identify the impact that certain \nhigh-risk AI systems, including RBI systems,  may produce on fundamental rights. A \nFRIA is an accountability tool. The FRIA does not replace the existing Data Protection \nImpact Assessment (DPIA) that data controllers (i.e. those responsible for the  \nprocessing of personal data) must perform under Article 27 LED, Article 35 GDPR or \nArticle 39 EUDPR. For example, a DPIA must be conducted when biometric data are processed through \nnew technologies likely to result in a high risk to the rights and freedoms of natural \npersons (such as CCTV, AI facial recognition, and body -worn cameras) in publicly \naccessible spaces."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n110 \n(372) Whereas a DPIA focuses on the risks to the rights and freedoms of individuals resulting \nfrom the processing of their personal data , a FRIA covers the possible impact of AI \nsystems on individuals’ fundamental rights  more generally . The scope of a FRIA is \ntherefore broader in terms of activities covered and fundamental rights assessed. Where \npersonal data are processed by the AI system (which is the case of RBI systems), the \nFRIA should complement the DPIA performed by the deployer as data controller,213 \nwithout covering aspects already addressed in the DPIA and avoiding overlap s. The \nanalysis of the FRIA in these Guidelines is limited to the authorized use of RBI in real-\ntime and is aimed to serve as a preliminary guidance for deployers in this interim period \nbefore the AI Office provides a template.214 \n(373) The obligation to carry out the FRIA under Article 5(2) AI Act  is imposed on the \ndeployers of the RBI system , and not entities or bodies or anyone else acting on their \nbehalf. If other actors are acting on behalf of the deployer/the law enforcement \nauthority, they will have to contribute to the preparation of the FRIA with all relevant \ninformation to ensure it is properly carried out. (374) A FRIA must be carried out before the deployment of the authorized  real-time RBI \nsystem."}
{"meta": {"section_type": "guideline", "article_ref": "Article 27", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 27]\n\n(375) According to Article 27 AI Act, a FRIA should include the following information:  \n• A description of the RBI use and the deployer’s processes for the use, together with \nthe intended purpose of use \nThe description should include:  \n- the name of the deployer;  \n- the law enforcement purpose(s) for which the real-time RBI system will be used; \n- the description of the reference database against which the biometric identification \nwill be compared, including the sources of the biometric data (facial images, voice \nsamples, etc.) that will be used;  \n- the description of the technology underlying the system to explain its functioning (by \nreferencing the available documentation provided by the provider and its name);215 \n- the legal basis on which the real-time RBI will be deployed. • The period of use and frequency of use \nEach individual use of a real-time RBI system for one of the permitted exceptions must \nbe authorised prior to its deployment by a judicial authority or other independent \nauthority under Article 5(3) AI Act. By contrast, for the FRIA, deployers must provide \na general indication of the intended period of use and the expected frequency."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n111 \n• The categories of persons and groups affected by the system  \nFor the purpose of the exception in Article 5(1)(h) AI Act, the FRIA should distinguish \nbetween: \n- The targeted individual, who may be a victim of a crime, a perpetrator, or a suspect, \n- The individuals whose biometric data are included in the reference database, and \n- Categories of persons who are present in the surrounding areas where the RBI system \nwill be deployed. The use of  real-time RBI systems will not only affect the fundamental rights of the \ntargeted individual. The rights of other individuals whose biometric data are used for \ncomparison purposes, passersby, and people incidentally presented in the search area \nwill also be affected . The description of the geographic scope of the  search area(s) \ncovered by the real-time RBI system will impact the number of persons affected by the \nsystem."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\n• The specific risks of harm to the affected persons \n(376) The fundamental rights that may be affected by the use of  real-time RBI in publicly \naccessible spaces for law enforcement purposes include, in particular:  \n- the right to private and family life , including people’s reasonable expectation of \nanonymity in public spaces;  \n- the right to data protection, as RBI systems rely on the processing of biometric data \nand other personal data (e.g., names, ID numbers, as well as sensitive data such as \nethnicity) to identify specific individuals;  \n- freedom of thought, conscience and religion, freedom of expression and freedom of \nassembly and association in the public spaces  being searched, on which the use of \nRBI systems could have a chilling effect, preventing individuals to fully exercise \ntheir rights and freedoms, since if individuals know  that they are monitored, th ey \nmight change their behaviour , or even prevent themselves to behave in a certain \nmanner; \n- the right to an effective remedy and a fair trial;  \n- the right to non-discrimination if the system embeds biases (such as gender, ethnic \nor racial biases) and leads to the misidentification of a suspect or perpetrator;  \n- the right to human dignity by the feeling of being reduced to an object of the system; \n- the presumption of innocence and right t o defence; since no decision adversely \naffecting an individual may be solely taken on the output of the real -time RBI \nsystem; \n- the rights of the child in case the victim, missing person or suspect is a minor; \n- the rights of the elderly in the case of a missing person."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n113 \nOther considerations for human oversight and monitoring under Articles 14 and 26 AI \nAct are also relevant and should be described. • Risk mitigation measures \nBeyond implementing human oversight measures (including to avoid discriminatory \nmeasures), the deployer should explain redress measures in case a risk materialises , \nincluding the governance procedures and the complaint mechanisms (such as in  the \ncase of a misidentification). 10.1.2. Registration of the authorized RBI systems \n(377) Article 5(2) AI Act also obliges the deployer of a real-time RBI system used in publicly \naccessible spaces for law enforcement purposes  to register  the system in the EU \ndatabase provided  for in Article 49 AI Act . However , in case s of a duly justified \nemergency (such as an imminent threat), deployment may start even prior to \nregistration, provided the law enforcement authority registers the system without undue \ndelay. Undue delay should be understood as meaning ‘as soon as possible’ considering \nthe circumstances of the emergency that prevented the registration of the system prior \nto its use. Whether registration meets that criterion requires an appreciation on a case-\nby-case basis. It cannot be defined a priori with a precise time limit. The delay should \nnot be caused by a deliberate action. According to Article 49(4 ) AI Act, RBI systems \nused for the purpose of law enforcement will be registered in a se cure non -public \nsection of the database, with limited information and limited access to that information. For example,  requesting law enforcement authorities to register the RBI system \nwithin 24 hours of the use might be considered a reasonable delay where the system \nwas deployed in a situation of an imminent threat to life, such as in the scenario of a \nlive shooter. 10.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nNeed for prior authorisation \n(378) Article 5(3) AI Act requires prior authorisation of each individual use of a real -time \nRBI system and prohibits automated decision-making based solely on the output  of \nsuch a system which produces an adverse legal effect. Article 5(3) AI Act provides: \nFor the purposes of paragraph 1, first subparagraph, point (h) and paragraph 2, each \nuse for the purposes of law enforcement of a ‘real -time’ remote biometric \nidentification system in publicly accessi ble spaces shall be subject to a prior \nauthorisation granted by a judicial authority or an independent administrative \nauthority whose decision is binding of the Member State in which the use is to take \nplace, issued upon a reasoned request and in accordanc e with the detailed rules of \nnational law referred to in paragraph 5. However, in a duly justified situation of \nurgency, the use of such system may be commenced without an authorisation \nprovided that such authorisation is requested without undue delay, at the latest within"}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article \n5]\n\n114 \n24 hours. If such authorisation is rejected, the use shall be stopped with immediate \neffect and all the data, as well as the results and outputs of that use shall be \nimmediately discarded and deleted. The competent judicial authority or a n independent administrative authority whose \ndecision is binding shall grant the authorisation only where it is satisfied, on the basis \nof objective evidence or clear indications presented to it, that the use of the ‘real -\ntime’ remote biometric identificat ion system concerned is necessary for, and \nproportionate to, achieving one of the objectives specified in paragraph 1, first \nsubparagraph, point (h), as identified in the request and, in particular, remains limited \nto what is strictly necessary concerning the period of time as well as the geographic \nand personal scope. In deciding on the request, that authority shall take into account \nthe elements referred to in paragraph 2. No decision that produces an adverse legal \neffect on a person may be taken based so lely on the output of the ‘real-time’ remote \nbiometric identification system. 10.2.1. Objective \n(379) The objective for requiring prior authorisation (‘authorisation ex ante’) for any use of a \n‘real-time’ RBI system in publicly accessible spaces for law enforcement purposes is \nthe need for an assessment and a decision as to whether any envisaged use of such a \nsystem for such purposes is: \n- necessary and proportionate to achieve any one of the objectives listed in Article \n5(1)(h)(i) to (iii) , i.e. for the targeted search of specific victims, the prevention of \nspecific threats, or the localisation or identification of offenders; and \n- limited to what is strictly necessary concerning the time period  and the geographic \nand personal scope."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n(380) The consequence of th ese requirements is that a double necessity and proportionality \nassessment should occur prior to the deployment of any real -time RBI system  in \npublicly accessible spaces for law enforcement purposes . First, an assessment should \nbe made by the user when performing a FRIA , as required by Article 5 (2) AI Act. Second, in accordance with Article 5(3) AI Act, a judicial or independent administrative \nauthority must also assess the necessity and proportionality of using such a system \nwithin the limits of the national law providing the legal basis for any such use, taking \nthe Charter and other Union law into consideration. As a consequence, any such system \nmay only be used 1) after a  FRIA and 2) when the  competent national authority has \nauthorised such use. (381) Article 5(3) AI Act must be read and understood in conjunction with Article 5(5) AI \nAct: for the use of a real -time RBI system to be authorized, a national law adopted in \nthe Member States concerned must exist authorising such use.217 Certain Member States \nalready have a system of  prior authorisation in place for the use of biometric systems \nunder other Union or national law, such as data protection law."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article \n5]\n\n115 \n10.2.2. The main principle: Prior authorisation by a judicial authority or an \nindependent administrative authority \n(382) The use of real-time RBI systems which pursue one of the objectives listed in Article \n5(1)(h)(i) to (iii ) AI Act and which has been provided for in  the national law of the \nMember States concerned must be authorized by a judicial authority or an independent \nadministrative authority prior to its use. This is the main principle. (383) However, there is an exception in  the case of urgency. This shall be duly justified.218 \nUrgency is described as ‘situations where th e need to use the systems concerned is \nsuch as to make it effectively and objectively impossible to obtain an authorisation \nbefore commencing the use of the AI system’ .219 In such case of urgency, ‘the use of \nthe AI system should be restricted to the absolute minimum necessary  and should be \nsubject to appropriate safeguards and conditions , as determined in national law and \nspecified in the context of each individual urgent use case by the law enforc ement \nauthority itself.’ \n10.2.2.1. Prior and reasoned request in accordance with national procedural rules  \na) Request by whom? (384) Whilst not specified, it may be assumed that the request will normally be initiated by \nthe deployer, i.e. by the competent (law enforcement) authority . According to the \ndefinition of law enforcement authority under Article 3(45) b) AI Act, any ‘other body \nor entity entrusted by Member State law to exercise public authority and public powers \nfor the purposes of the prevention, investigation, detection or prosecution of criminal \noffences or the execution of criminal penalties, including the safeguarding against and \nthe prevention of threats to public security’ is considered a law enforcement authority \nand could also be the responsible as the ‘competent authority’ for submitting the request \nfor prior authorisation."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n(385) The use of a real-time RBI system for activities falling outside the scope of the AI Act \ndoes not need to be authorised under Article 5(3) AI Act. If subsequently such a system \nis being used for law enforcement purposes, the use would fall within the scope of the \nAI Act and an authorisation would be required where the requirements of Article 5(1)(h) \nAI Act are met. b) Request for which use? (386) Prior authorisation is needed for the use of ‘real -time’ RBI systems in publicly \naccessible spaces for law enforcement purposes, even if the systems are operated by \nother parties  on behalf of law enforcement authorities , for example sport clubs or \nshopping malls. For example,"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\n218 This means that ‘the law enforcement authority should in such situations request such authorisation while providing the reasons for \nnot having been able to request it earlier, without undue delay and at the latest within 24 hours’. (Recital 35 AI Act). 219 Recital 35 AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article \n5]\n\n116 \n- An organisation entrusted with resources for searching for missing children decides \nto use a real-time RBI system . It has no mandate to exercise public authority and \npublic powers or for preventing criminal offences or tasks for the prevention of threats \nto public security. Such use does not fall under the prohibition laid down in Article \n5(1)(h) AI Act, since it is not for law enforce ment purposes. That system will, \nhowever, be categorised as ‘high-risk’ (point 1 (a) of Annex III) and a requirement of \nprior consultation of the su pervisory data protection authority may be necessary \npursuant to Article 36 GDPR. Depending on the applicable national law and whether \none of the exceptions to Article 9(1) GDPR applies, a prior authorisation may also be \nrequired for such processing. By contrast, if the same organisation were requested by \nlaw enforcement authorities to act on their behalf for the search of missing children in \na law enforcement context and under the supervision and instructions of the competent \nlaw enforcement authorities, prior authorisation would be needed pursuant to Article \n5(3) AI Act. - A private organisation entrusted with  providing resources to aid persons who risk \nbecoming victims of a natural disaster 220 decides to use a real -time RBI system for \nthat purpose. Such use does not fall under the prohibition laid down in Article 5(1)(h) \nAI Act, since it is not for law enforcement purposes. That system will, however, be \ncategorised as ‘high-risk’ (point 1 (a) of Annex III) and a requirement of prior \nconsultation of the supervisory data protection authority may be necessary pursuant to \nArticle 36 GDPR. Depending on the applicable national law and whether one of the \nexceptions to Article 9(1) GDPR applies , a prior authorisation may also be required \nfor such processing. c) When? ‘Each use’ \n(387) In accordance with Article 5(3) AI Act, prior authorisation is needed for ‘each use’."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nThis implies that the decisive moment for obtaining such authorisation is not the  \nmoment prior to installing real-time RBI systems, but each concrete use thereof. For example, \n- the police install biometric ready CCTV cameras at the main train station of a city (no \nauthorisation under the AI Act is needed, but the biometric system must comply with \nthe requirements on high-risk systems, a FRIA must be prepared prior to the first use \nand an individual authorisation by a judicial or independent administrative authority \nis needed before each individual use of the system). The police has concrete indications that a terrorist will arrive by train in the town \n(prior authorisation is needed for real-time identification). d) Motivated Request \n(388) Article 5(3) AI Act requires each individual request for the use of real-time RBI to be \n‘reasoned’ and hence substantiated and motivated."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n118 \nc) Authorisation only if ‘necessary and proportionate’ to achieving one of the \nobjectives set forth in the exceptions  \n(395) Any authorisation to use real -time RBI in publicly accessible spac es for law \nenforcement purposes must assess whether the requirements of Article 5(3) AI Act have \nbeen met. Highly intrusive  \n(396) In the data protection context, the use of biometric data, in particular facial recognition \ntechnology, has been considered by the European Data Protection Board (EDPB) in its \nGuidelines 5/2022 and the European Data Protection Supervisor (EDPS) as affecting \nseveral fundamental rights and freedoms. That view is shared by The EU Agency for \nFundamental Rights and by the Council of Europe .226 Both the CJEU 227 and the \nECtHR228 have confirmed the sensitive nature of processing biometric data. (397) Any interference in fundamental rights and freedoms must always respect the \nessence of the rights and freedoms. This follows from Article 52(1) Charter. (398) The notion of ‘essence’ of fundamental rights and freedoms has been developed in the \nCJEU’s case-law and is an independent value in the Union’s legal order. If the essence \nof a fundamental right or freedom is not respect ed, it means that a right or freedom is \nunduly touched by a measure so that no interference shall be allowed upfront. Only ‘if necessary and proportionate’ \n(399) Any interference with fundamental rights and freedoms requires ‘a law’  that should in \nprinciple respect the necessity and proportionality pursuant to Article 52 Charter. (See \nbelow under Article 5(5) AI Act.) Article 5(3) AI Act requires that  the national law \npermitting the use of real -time RBI in publicly accessible spaces for law enforcement \npurposes must provide that authorisation for such use is pe rmitted ‘only where it [the \nauthority] is satisfied [..] that the use is necessary for, and proportionate to, achieving \none of the objectives specified ’ in Article 5(1)(h) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\nNational authorities need to \nverify whether the biometric identification is strictly necessary .229 This assessment \nshould be based on the FRIA that should have already include an assessment of the \nnecessity and the proportionality in a general manner prior to  requesting the \nauthorisation for each use with the specific circumstances. 10.2.2.3. The exception to the requirement of prior authorisation: request \nwithin 24 hours and consequences if rejected \n(400) In cases of urgency, a user may submit a request for authorisation within 24 hours  as \nfrom the moment that the real-time RBI system is used. In practice, that will generally \nbe the moment that the biometric ready or capably cameras are ‘switched on’ and"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n119 \ndeployed and the first biometric comparison is made with the system. The logging of \nthe processing activities should be made available to the authority to substantiate the \ntimeliness of the request.230 \n(401) In such a case, the request should motivate why no prior request was submitted prior to \nstarting using the system. 10.2.2.4. Immediate cessation in case the request for authorisation is rejected \nand deletion of the data  \n(402) Article 5(3 ) AI  Act further provides that if an authorisation request  in the case of \nurgency is rejected, the use of the real -time RBI system should be ceased with \nimmediate effect. In such cases, all the data, including the results and outputs of that \nuse, must be immediately discarded and deleted.231 Article 5(3) AI Act is explicit in this \nregard, without exception. The deployer will have: \na) a reference database, containing the biometric information ( e.g., facial images, \nvoice snippets, ….) and related identifying information, if applicable, against which  \nb) captured biometric information from individuals present in the publicly accessible \nspace is compared to identify and single out those individuals. c) This comparison will lead to the comparison result. (403) The requirement to discard and delete the data collected and processed also means that \nthe reference database(s) used for the unauthorized biometric identification must be \nremoved and deleted if it was built specifically for the contested search. Only where the \nlaw enforcement authorities had built and intended to maintain the database used for \nidentification in a lawful manner for legitimate aims other than for the unauthorized \nuse of real-time RBI may the database be maintained."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n(404) Besides the deletion of any (unlawful) database with biometric information, all the \ncollected images and other personal data, including the meta data, technical processing \ndata, including the templates and other personal data, and other comparison - and output \ndata obtained during the unlawful use of the real-time RBI system must also be deleted. (405) Where the law enforcement authority challenges the rejection, the data may be kept by \na trustee until a final decision has been taken  on the request. During that period, those \ndata should normally not be placed at the disposal of the law enforcement authority.310 \n10.2.2.5. No decision-making solely on the output of the real-time RBI system \n(406) In accordance with Article 5(3) AI Act , even where the deployer of a real -time RBI \nsystem obtains an authorisation, no decision that produces an adverse legal effect on a  \nperson may be taken based solely on the output of the ‘real-time’ RBI system. For example,"}
{"meta": {"section_type": "guideline", "article_ref": "Article 14", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 14]\n\n120 \n- A person is arrested and imprisoned for a serious crime solely based on identification \nby a facial recognition system, without any further checks. This comes on top of the \nrequirement under Article 14 AI Act for human oversight. Checks could relate for \nexample to the question whether a given person has been at a different place or also \nwhether there are other reasons for that the person cannot be the person searched. Requirements of Article 14 AI Act for Human Oversight \n(407) The use of real-time RBI that is permitted because it pursues one of the objectives listed \nin Article 5(1)(h) and complies with Articles 5(2) -(6) AI Act still falls under the rules \nfor high-risk systems. In accordance with Article 14 AI Act, high-risk AI systems ‘shall \nbe designed and developed in such a way, including with appropriate human -machine \ninterface tools, that they can be effectively overseen by natural persons during the \nperiod in which they are in use.’ Pursuant to Article 14(5) AI Act, no action or decision \nmay be taken by the deployer on the basis of the identification resulting from the \nsystem, ‘unless that identification has been separately verified and confirmed by at least \ntwo natural persons with the necessary competence, training and a uthority’ or unless \n‘Union or national law  considers the application of this requirement to be \ndisproportionate’. Article 4 AI Act prescribes AI literacy measures  for providers and \nusers of AI systems to ensure ‘a sufficient level of AI literac y of their staff and other \npersons dealing with the operation and use of AI systems’ and considering the persons \non whom the systems are to be used. (408) As stated by the EDPB  in the data protection context , for human oversight to be \neffective it is crucial  ‘to enable the person to understand the (in that case facial \nrecognition) system and its limits as well as to interpret its results properly."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nIt is also \nnecessary to establish a workplace and organisation that counteracts the effects of \nautomation bias, and av oids fostering the uncritical acceptance of the results e.g., by \ntime pressure, burdensome procedures, potential detrimental career effects etc .’232 \nSimilar considerations may apply in the context of the AI Act. 10.3. Notification to the authorities of each use of ‘real -time’ remote biometric \nidentification systems in publicly accessible spaces for law enforcement \nArticle 5(4) AI Act provides: \nWithout prejudice to paragraph 3, each use of a ‘real -time’ remote biometric \nidentification system in publicly accessible spaces for law enforcement purposes shall \nbe notified to the relevant market surveillance authority and the national data protection \nauthority in accordance with the national rules referred to in paragraph 5. The \nnotification shall, as a minimum, contain the information specified under paragraph 6 \nand shall not include sensitive operational data. (409) Each use of an RBI system pursuing one of the objectives listed in Article 5(1)(h)(i)-\n(iii) AI Act must be notified  to the relevant market surveillance authority and the \nnational data protection authority. Notification must take place after each use in order"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\n232 EDPB, Guidelines 05/2022 on the use of facial recognition technology in the area of law enforcement v2.0, 26 April 2023, p. 22."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 3]\n\n121 \nto be able to report about the number of authorisations and their result. The notification \ndoes not need to include sensitive operational data. According to Article 3 (38) AI Act, \n‘sensitive operational data’ means operational data related to law enforcement activities \n(prevention, detection, investigation or prosecution of criminal offences), the disclosure \nof which could jeopardise the integrity of criminal proceedings. (410) For the details on the reporting requirement, see section 10.6 below. 10.4. Need for national laws within the limits of the AI Act exceptions \n10.4.1. Principle: national law required to provide the legal basis for the \nauthorisation for all or some of the exceptions \n(411) National laws are required for operationali sing the use of ‘real -time’ RBI systems in \npublicly accessible sp aces for the purposes of law enforcement. At the same time, \nArticle 5(5) AI Act provides that Member States remain free to decide whether to adopt \nsuch national laws. If a national law authorising the use of real-time RBI is adopted, the \nAI Act specifies  the substantive elements which the national laws must contain to \ncomply with the requirements laid down in the AI Act. Article 5(5) AI Act  \nA Member State may decide to provide for the possibility to fully or partially authorise \nthe use of ‘real -time’ remote biometric identification systems in publicly accessible \nspaces for the purposes of law enforcement within the limits and under the conditions \nlisted in paragraph 1, first subpara graph, point (h), and paragraphs 2 and 3. Member \nStates concerned shall lay down in their national law the necessary detailed rules for \nthe request, issuance and exercise of, as well as supervision and reporting relating to, \nthe authorisations referred to in paragraph 3."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nThose rules shall also specify in respect of \nwhich of the objectives listed in paragraph 1, first subparagraph, point (h), including \nwhich of the criminal offences referred to in point (h)(iii) thereof, the competent \nauthorities may be authorised to use those systems for the purposes of law enforcement. Member States shall notify those rules to the Commission at the latest 30 days following \nthe adoption thereof. Member States may introduce, in accordance with Union law, \nmore restrictive laws on the use of remote biometric identification systems. 10.4.2. National law shall respect the limits and conditions of Article 5(1)(h) AI \nAct \n(412) Since the use of ‘real -time’ RBI systems in publicly accessible spaces for law \nenforcement purposes is considered an interference with fundamental rights, Article \n5(5) AI Act provides that such use shall be established by national law in the Member \nStates. Those national laws provide the legal basis for the use of such systems. (413) National laws shall not exceed the limits set by Article 5(1)(h) AI Act and shall respect \nall further related conditions set forth in the AI Act. That implies that the Member States \nmay not expand the objectives for which real -time RBI may be used in publicly"}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article \n5]\n\n122 \naccessible spaces for law enforcement purposes beyond those listed in Article \n5(1)(h)(i)-(iii) AI Act.233 \n(414) Member States shall notify their national laws to the Commission at the latest 30 days \nfollowing the adoption thereof . Such notification does not provide a presumption of \nconformity of the Member States’ law with the AI Act . The AI Office, after receiving \nthe notification, will send a confirmation of receipt. Prior to its adoption, Member States \nare also encouraged to send a preliminary version of the national (or regional) law \nproposed to the AI Office. In any case, non-notification to the AI Office within the legal \ndeadline of 30 days following the adoption  set out in Article 5(5) may imply that the \nnational law is unenforceable in legal proceedings , as has been held in different \ncontexts.234 The Commission will publish the Member States’ laws on a public website. (415) Member States may introduce, in accordance with Union law, more restrictive laws, i.e. laws with stricter requirements than those laid down in Article 5(1)(h) and (2) to (7) AI \nAct. 10.4.3. Detailed national law on the authorisation request, the issuance and the \nexercise \n(416) As regards the detailed rules that apply for the request, the issuance and the exercise of \nthe authorisation, these are to be determined by national law. Each Member State that \nwishes to allow the use of the systems at issue must specify in its national laws such \nrules, which are aimed at providing relevant and complete information as to the use of \nreal-time RBI systems to the authorising authority to enable it to decide as to the strict \nnecessity and proportionality of such use."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nThe national law permitting the use of real-time RBI systems may regulate for example,  \n- who are the competent authorities subject to Art icle 5(1)(h) AI Act  and the \nindependent authorities in the M ember State competent for issuing (or refusing) an \nauthorisation;  \n- the detailed scope of the objectives for which real -time RBI in publicly accessible \nspaces may be used for law enforcement purposes  (without going beyond the \nobjectives listed in Article 5(1)(h) (i) to (iii), but possibly narrowing them further \ndown; \n- providing that requests shall be in writing and requiring a detailed explanation of the \nspecific use and the intended purpose of use for a specific criminal offence/situation \nwhich justifies its use;  \n- the requirement of motivation and the submission of supporting evidence (and need \nof translation if relevant) for justifying the use of the system pursuing the objectives \nlisted in Article 5(1)(h)(i) to (iii) AI Act , in particular relating to place, peri od, and"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\n235 See e.g., EDPB, Guidelines 05/2022 on the use of facial recognition technology in the area of law enforcement Version 2.0, 26 April \n2023, p. 24 et seq."}
{"meta": {"section_type": "guideline", "article_ref": "Article 70", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 70]\n\n124 \n(417) Article 70 AI Act obliges the Member States to ‘establish at least one notifying and at \nleast one market surveillance authority.’ Article 74(8) AI Act provides that ‘Member \nStates shall designate as market surveillance authorities for the purposes of this \nRegulation either the compete nt data protection supervisory authorities under \nRegulation (EU) 2016/679 or Directive (EU) 2016/680, or any other authority \ndesignated pursuant to the same conditions laid down in Articles 41 to 44 of Directive \n(EU) 2016/680.’ \n(418) This comes on top of the  designation of the authorising authority, which the Member \nState will have to establish before it can authorise the use of real-time RBI systems for \nany of the objectives listed in Article 5(1)(h)(i) to (iii) AI Act. 10.5. Annual reports by the national market surveillance authorities and the \nnational data protection authorities of Member States \nArticle 5(6) AI Act provides \nNational market surveillance authorities and the national data protection authorities of \nMember States that have been notified of the use of ‘real -time’ remote biometric \nidentification systems in publicly accessible spac es for law enforcement purposes \npursuant to paragraph 4 shall submit to the Commission annual reports on such use. For \nthat purpose, the Commission shall provide Member States and national market \nsurveillance and data protection authorities with a template , including information on \nthe number of the decisions taken by competent judicial authorities or an independent \nadministrative authority whose decision is binding upon requests for authorisations in \naccordance with paragraph 3 and their result."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n(419) The national market surveillance authorities and the national data protection authorities \nof Member States that have been informed by deployers of the use of real -time RBI \nsystems in publicly accessible spaces for law enforcement purposes (see Article 5 (4)) \nmust submit annual reports on such use to the Commission. These reports shall be made \non the basis of a template provided by the Commission. This template will be \nestablished in due time. (420) Where the deployer is an EU Institution, body, or agency, the EDPS is obliged to inform \nthe Commission accordingly on an annual basis of the real -time RBI systems used in \npublicly accessible spaces for law enforcement purposes. (421) Only the report of the national data protection authority will cover the period between \n2 February 2025 and 2 August 2025, since the AI Act does not require Member States \nto appoint a national market surveillance authority before the latter date. (422) National market surveillance authorities and national data protection authorities are free \nto decide whether they wish to submit individual reports or a joint report per Member \nState. 10.6. Annual reports by the Commission \nArticle 5(7) AI Act provides"}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n125 \nThe Commission shall publish annual reports on the use of real -time remote biometric \nidentification systems in publicly accessible spaces for law enforcement purposes, \nbased on aggregated data in Member States on the basis of the annual reports referred \nto in paragraph 6. Those annual reports shall not include sensitive operational data of \nthe related law enforcement activities. (423) The AI Act requires the Commission to publish annual reports of the uses of real-time \nRBI systems in publicly accessible spaces for law enforcement purposes in the Member \nstates and by Union institutions, agencies and bodies, based on aggregated data. These \nreports will be based on the information notified by the national authorities pursuant to \nArticle 5(6) AI Act. (424) The Commission’s annual report shall not contain sensitive operational data. Sensitive \noperational data means ‘operational data related to activities of prevention, detection, \ninvestigation or prosecution of criminal offences, the disclosure of which could \njeopardise the integrity of criminal proceedings ’.236 This could mean that specific \ndetails that reveal ongoing or past investigations, such as e.g., locations, camera’s used, \nshall not be published. 10.7. Out-of-Scope \n(425) All other uses of RBI systems that are not covered by the prohibition of Article 5(1)(h) \nAI Act fall within the category of high-risk AI systems as defined by Article 6 and listed \nin point 1(a) of Annex III AI Act provided they fall within the scope of the AI Act. (426) RBI systems that fall outside the  scope of the prohibition in Article 5(1)(h) AI Act \ninclude biometric verification/authentication systems and the retrospective use of (post-\n) RBI systems in publicly accessible spaces for law enforcement purposes."}
{"meta": {"section_type": "guideline", "article_ref": "Article 26", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 26]\n\nFor instance, \npolice authorities might be authoris ed by national law to perform retrospective facial \nrecognition to compare images of criminal suspects with recorded facial images in a \ncriminal database.237 Another use that falls outside the scope of the prohibition is the \nuse of real-time RBI systems for law enforcement purposes in either a private (such as \nat somebody’s place) or an online space (such as the use of a chat room or online game \nto identify a suspect of disseminating child sexual abuse material). Finally, the use of \nRBI systems by private actors, both in real -time and retrospective (such as the use of \nlive facial recognition technology by a supermarket to identify known shoplifters, the \nuse of live facial recognition technology by a sports arena to identify individuals banned \nfrom entering the arena, or the use of live facial recognition technology in schools for \nsecurity purposes and school attendance) fall outside the scope of the prohibition. (427) In addition to the rules that apply to high-risk AI systems generally, the retrospective \nuse of RBI systems  for law enforcement purpose s is subject to additional conditions \nand safeguards  in accordance with Article 26(10) AI Act (in application as from 2 \nAugust 2026).238"}
{"meta": {"section_type": "guideline", "article_ref": "Article 9", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 9]\n\n126 \n(428) Uses for purposes other than for law enforcement must in any event comply with \ndata protection rules . The cases below illustrate the interpretation of Article 9(2) \nGDPR in cases of such use and the exceptions to process biometric data. For example,  \n- A French administrative Court found that the trial of live facial recognition \ntechnology in two public schools for access control and security purposes was \nneither necessary nor proportionate  (under data protection rules) . Alternative \nsolutions that were less intrusive for the students were avail able, e.g., the use of \nbadges. In addition, the conditions for explicit consent were not met. Therefore, \nconsent could not be used as a valid legal basis to trial facial recognition technology \nin high schools.239 \n- A supermarket was not allowed to use live facial recognition technology to prevent \nshoplifting in the Netherlands. Without explicit consent from the customer or any \nlegal basis allowing the processing for a substantial public interest (such as security \npurposes), the supermarket could not process biometric data and thus deploy facial \nrecognition technology.240  \n- The use of live facial recognition technology  at the entrance of a football club  to \nidentify supporters was prohibited in France241 and to ensure the safety of spectators \nwas prohibited in Spain.242 \n10.8. Examples of uses \nThe police instals mobile CCTV cameras equipped with AI-based facial recognition \ntechnologies on a police van around the main entrance of a football stadium during a \nEuropean Championship match to secure the area and identify individuals whose \nfaces are re corded in an ad hoc watchlist database of wanted individuals. This \nwatchlist includes persons suspected of having committed a crime (ranging from \nserious crimes to frauds and burglaries), persons of possible interests for intelligence \npurposes, and vulnerable persons with mental issues."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement]\n\nThe police’s use of live facial \nrecognition technologies is not linked to information concerning the presence of a \nspecific person at the event. Although there are likely to be people on the watchlist \nfor the search of whom the use of real -time RBI would be allowed, this list is too \nunspecific and is not linked to the event of the football match. Such use would \ntherefore be prohibited."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\n128 \nrecognition by comparing the extracted images with photographs posted on social \nmedia. The retrospective use of facial recognition technology is not prohibited by the AI \nAct. That use is considered high-risk and should comply with the requirements in the \nAI Act for such systems.243 \nFurther examples of NOT prohibited practices: \n- Hotels using real -time RBI to recognise VIP guests. This is not law \nenforcement. - Shopping malls using real-time RBI to find shoplifters. This is not law \nenforcement. Prohibited: \nEntrusted by the police, a shopping mall is using real -time RBI to find \nshoplifters. The system is deployed for law enforcement purposes, in a \npublicly accessible space. The use is prohibited because the search for \nshoplifters does not fall under any of the exceptions of Article 5(1)(h) AI Act. 11. ENTRY INTO APPLICATION  \n(429) According to Article 113 AI Act,  Article 5 AI Act applies as from  2 February 2025. The prohibitions in that provision will apply in principle to all AI system regardless of \nwhether they were placed on the market or put into service before or after that date.244 \n(430) At the same time, the chapters on governance and penalties will become applicable on \n2 August 2025. Consequently, the provisions on penalties for non-compliance with the \nprohibitions in Article 5 AI Act will not apply before 2 August 2025 . In this interim \nperiod, there will also be no market surveillance authorities to monitor whether the \nprohibitions are being properly complied with. (431) Nevertheless, even in this interim period , the prohibitions are fully applicable and \nmandatory for providers and deployers of AI systems. Those operators should therefore \ntake necessary measures to ensure that they do not place on the market, put into service \nor use AI systems that could constitute prohibited practices under Article 5 AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 5]\n\nEven if the provisions on monitoring and fines do not apply until a later date, t he \nprohibitions themselves have direct effect and  thus enable affected parties to enforce \nthem in national courts and request interim injunctions against the prohibited practices. 12. REVIEW AND UPDATE OF THE COMMISSION GUIDELINES \n(432) These Guidelines constitute a  first interpretation with practical examples of the \nprohibitions in Article 5 AI Act. The Commission will provide additional support to"}
{"meta": {"section_type": "guideline", "article_ref": "Article 10", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[GUIDELINE] EU Commission [Domain: Law Enforcement] [Article 10]\n\n243 The processing of biometric data for a law enforcement purpose remains subject to Article 10 of the LED, which needs to be \nimplemented at national level. Their processing to perform the retrospective use of FRT should only be allowed if it is stric tly \nnecessary and should be subject to appropriate safeguards. Whether the retrospective use of FRT is strictly necessary to identify the \ndemonstrator is questionable. In the Glukhin v Russia judgment that serves as a basis for this scenario, the ECtHR ruled that while \ncrime detection can be a legitimate aim, the use of FRT, both retrospective and live, was disproportionate as there were no r isks to \npublic order or transport safety. The Court emphasized the ‘highly intrusive’ nature of FRTs. In that case, the Court concluded that \nusing FRTs did not answer a pressing social need, nor was it necessary in a democratic society. 244 See Article 111(1) and (2) AI Act which specifies that the grandfathe ring clause is without prejudice to the application of Article 5 \nAI Act as referred to in Article 113(3)(a) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 50", "guideline_source": "BSA"}, "text": "[GUIDELINE] BSA [Article-specific]\n\ns (Art. 50 of the EU AI Act) \n \nFebruary 2026 \n \n \nThe Business Software Alliance (BSA) wanted to raise concerns about the draft Code of Practice (CoP) on \nthe transparency requirements deriving from Article 50 of the EU AI Act. \n \nBSA is the global trade association representing the enterprise (B2B) software industry. Our members 1 \noperate at the forefront of AI, cybersecurity, cloud computing, and data -driven innovation. In particular, \nBSA members are on the leading edge of providing AI -enabled products and services. As such, they have \nunique insights into the technology’s potential to spur digital transformation and practices that can best \nsupport the responsible development and use of AI. \n \nBSA and its members are determined for the Code to be successful with many companies wanting to sign \non to it. This is directly linked to the EU’s ambition to be a leading AI continent with innovative AI \ncompanies.  However, the current draft does not meet this competitiveness requirement and, as it stands, \nwould further create complexity, burden and cost for businesses developing and deploying generative AI \nsystems in Europe."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "금융", "guideline_source": "FSC"}, "text": "[GUIDELINE] FSC [Domain: 금융]\n\nAI 가이드라인: 논의의배경\n(배경) 기술양상및규제환경변화→ 금융분야AI가이드라인개정필요성\n- 생성형AI, AI Agent 등급격한기술발전과금융회사내부의AI 활용범위확대\n- 2025년1월「인공지능발전과신뢰기반조성등에관한기본법」제정, 9월하위법령초안발표, 11월시행령입법예고\n→ 기존3개가이드라인(①운영, ②개발, ③보안)*을「금융분야AI 가이드라인」으로단일화\n* ①「금융분야AI 운영가이드라인(‘21.7월)」, ②「금융분야AI 개발·활용안내서(’22 8월)」, ③「금융분야AI 보안가이드라인(‘23.4월)」\n4\n(적용) 금융서비스〮상품제공시AI를활용하는금융회사및비금융회사\n- (업무) AI 활용신용평가, 대출심사, 챗봇, 사기탐지시스템(FDS) 등\n- (적용대상) 금융회사(은행, 금융투자업자등) + 비금융회사(AI 활용결과가금융거래영향O)\n- 본원칙은모범규준(Best Practice), 업권별자율규제형식으로규율하고의견을지속수렴하여상시적으로개선· 보완할계획\n기술양상 및 규제환경 변화\n를 반영한 가이드라인 개정\n필요성 증가(‘21.7월)\n「금융분야 AI\n운영 가이드라인」\n(‘23.4월)\n「금융분야 AI\n보안 가이드라인」\n(‘22.8월)\n「금융분야 AI \n개발·활용 안내서」\n과거 현재 미래\n생성형 AI 및\nAI Agent의 등장\n&\n인공지능기본법\n제정(’25.1월)\n논의의배경/ 7대원칙/ 결론\n(참고) 금융AI 7대 원칙(안)\n5\n분야 7대원칙\n거버넌스\n① 경영진의 역할과 책임 - 최고경영자를 포함한 경영진은 AI 개발〮활용에 대한 관심을 갖고 역할과 책임을 분담해야 함\n② 합법성 - AI 활용 전단계에서 금융〮AI 등 관련 법규를 준수해야 함\n③ 보조수단성 - 현 단계에서 AI는 업무의 보조 수단이므로 최종 의사결정과 그에 따른 책임은 임직원이 수행함\nA I  개발\n단계\n④ 신뢰성 - AI 개발 과정에서 신뢰할 수 있는 데이터와 모델을 사용해야 함\n⑤ 안정성 - AI 설계〮학습 등 전과정에서 금융 안정성 위험을 최소화해야 함\nA I  활용\n단계\n⑥ 신의성실성 - AI 활용 시 금융소비자의 이익을 최우선으로 해야 함\n⑦ 보안성 - AI 활용 시 보안성 기준 및 점검〮개선 체계를 마련해야 함\n< 금융AI 7대원칙(안) >\n2024년 12월, 금융위원회는금융권AI 개발‧활용의주요원칙으로금융AI 7대원칙을마련\n논의의배경/ 7대원칙/ 결론\n금융분야AI 가이드라인: 7대 원칙\n21 3\n6"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "신용", "guideline_source": "FSC"}, "text": "[GUIDELINE] FSC [Domain: 신용]\n\nAI 가이드라인: 논의의배경\n(배경) 기술양상및규제환경변화→ 금융분야AI가이드라인개정필요성\n- 생성형AI, AI Agent 등급격한기술발전과금융회사내부의AI 활용범위확대\n- 2025년1월「인공지능발전과신뢰기반조성등에관한기본법」제정, 9월하위법령초안발표, 11월시행령입법예고\n→ 기존3개가이드라인(①운영, ②개발, ③보안)*을「금융분야AI 가이드라인」으로단일화\n* ①「금융분야AI 운영가이드라인(‘21.7월)」, ②「금융분야AI 개발·활용안내서(’22 8월)」, ③「금융분야AI 보안가이드라인(‘23.4월)」\n4\n(적용) 금융서비스〮상품제공시AI를활용하는금융회사및비금융회사\n- (업무) AI 활용신용평가, 대출심사, 챗봇, 사기탐지시스템(FDS) 등\n- (적용대상) 금융회사(은행, 금융투자업자등) + 비금융회사(AI 활용결과가금융거래영향O)\n- 본원칙은모범규준(Best Practice), 업권별자율규제형식으로규율하고의견을지속수렴하여상시적으로개선· 보완할계획\n기술양상 및 규제환경 변화\n를 반영한 가이드라인 개정\n필요성 증가(‘21.7월)\n「금융분야 AI\n운영 가이드라인」\n(‘23.4월)\n「금융분야 AI\n보안 가이드라인」\n(‘22.8월)\n「금융분야 AI \n개발·활용 안내서」\n과거 현재 미래\n생성형 AI 및\nAI Agent의 등장\n&\n인공지능기본법\n제정(’25.1월)\n논의의배경/ 7대원칙/ 결론\n(참고) 금융AI 7대 원칙(안)\n5\n분야 7대원칙\n거버넌스\n① 경영진의 역할과 책임 - 최고경영자를 포함한 경영진은 AI 개발〮활용에 대한 관심을 갖고 역할과 책임을 분담해야 함\n② 합법성 - AI 활용 전단계에서 금융〮AI 등 관련 법규를 준수해야 함\n③ 보조수단성 - 현 단계에서 AI는 업무의 보조 수단이므로 최종 의사결정과 그에 따른 책임은 임직원이 수행함\nA I  개발\n단계\n④ 신뢰성 - AI 개발 과정에서 신뢰할 수 있는 데이터와 모델을 사용해야 함\n⑤ 안정성 - AI 설계〮학습 등 전과정에서 금융 안정성 위험을 최소화해야 함\nA I  활용\n단계\n⑥ 신의성실성 - AI 활용 시 금융소비자의 이익을 최우선으로 해야 함\n⑦ 보안성 - AI 활용 시 보안성 기준 및 점검〮개선 체계를 마련해야 함\n< 금융AI 7대원칙(안) >\n2024년 12월, 금융위원회는금융권AI 개발‧활용의주요원칙으로금융AI 7대원칙을마련\n논의의배경/ 7대원칙/ 결론\n금융분야AI 가이드라인: 7대 원칙\n21 3\n6"}
{"meta": {"section_type": "guideline", "article_ref": "제10조", "domain": "교육", "guideline_source": "FSC"}, "text": "[GUIDELINE] FSC [Domain: 교육] [제10조]\n\n1. 거버넌스원칙(3/3)\n위험수준별로차등화된통제〮관리수행및모니터링, 문서화, 교육등위험통제를위한제반절차마련〮이행\n9\n논의의배경/ 7대원칙/ 결론\n(예시) AI 서비스 위험 수준에 따른 차등화된 통제·관리 수행방안\n- 모든 AI 서비스에 기본 통제 방안을 적용하되, 저위험 AI 서비스는 완화된 통제 방안을 적용하여 운영 효율성 도모\n- 고위험 AI 서비스는 AI 윤리위원회 사전 승인·사후 검증, 제3자에 의한 평가 검증, 운영단계 모니터링 강화 등 추가 통제\n- AI 기본법에 따른 ‘고영향 AI’에 대해서는 위험점수에 따른 등급 분류와 관계없이 ‘고위험’으로 분류하여 통제 강화\n(참고) 금융감독원의「금융분야AI위험관리프레임워크(AI RMF)」\n10\n논의의배경/ 7대원칙/ 결론\n금융회사가AI 시스템의도입활용전주기에걸쳐유연하고체계적으로AI 위험을관리할수있도록, \n①거버넌스, ②위험평가, ③위험통제와관련한핵심프로세스를제시\n※ 거버넌스, 위험평가 및 위험통제 등의 구체적인 내용은 향후 금융감독원이 공개 예정인\n「금융분야 AI 위험관리 프레임워크」(AI Risk Management Framework in Financial Sector)를 참고 가능\n2. 합법성원칙\nAI 개발·이용시적용법규를사전파악, 내부정책및업무절차에반영및주기적인점검·개선\n- 공통적으로적용되는법령(인공지능기본법) 및하위법규(이하인공지능기본법령)와해당업종에따라적용되는관련법령을검토\n- 법규요구사항→ 내부정책및업무절차에반영, 주기적점검→ 실효성평가〮지속개선\n11\n논의의배경/ 7대원칙/ 결론\n구분 관련법규\n공통\n❶ 금융소비자 보호를 위한 영업행위 규정 준수 금융소비자보호법 제10조 및 제17~22조\n❷ 정보처리시스템의 안전성 확보 및 보호대택 수립・이행 전자금융감독규정 제14조 및 제21조\n❸ 위험관리 기준 및 절차 마련 금융사지배구조법 제27조 제1항\n❹ 자동화된 결정에 대한 정보주체 권리 보장\n(개인정보 처리시)\n개인정보보호법 제37조의2\n신용평가, 보험, \n여신 등\n❶ 자동화평가 등에 대한 설명 이행\n(개인신용정보처리시)\n신용정보법 제36조의2 제1항\n❷ 개인신용평가에 관한 원칙 준수 신용정보법 제22조의 3\n금융투자 등\n❶ 전자적 투자조언장치활용업무의요건 자본시장법시행령제2조및금융투자업규정 제1-2조의2\n❷ 전자적 투자조언장치의투자일임보고서작성・교부・보관등 자본시장법제99조, 동법 시행령 제100조, 금융투자업규정제4-78조, 4-13조\n금융분야관련법령\n3. 보조수단성원칙\nAI 산출물에대한최종책임은임직원이지며, 운영전단계에걸쳐임직원개입상황차등화\n- 인공지능을업무의보조수단으로활용하고최종의사결정과책임은임직원이수행하도록해야함. - 업무중요도, 위험수준에상응한의사결정단계별로역할과책임에대한사항을정함(예: RACI 차트). - 단, 고영향AI는사람이수행하는의사결정에보조적인용도로만사용(Human-in-the-loop)\n12\n논의의배경/ 7대원칙/ 결론\n구분 여신심사담당자 여신심사 팀장 리스크\n관리부서\n준법\n감시인 IT운영\n① 신청 접수·기본요건 확인 R I I I I\n② 모델 점수 산출·권고 표시 C I C I R\n③ 개입기준 판단·자료 보완 R A C C C\n④ 심사의견1차 검토·사유기재 R I C I I\n⑤ 상급자 검토/복수인 확인 R A C C I\n⑥ 최종결정·통지·기록 I A/R C C C\n주: RACI 차트란 업무 과정에서 누가 무엇을 책임지고, 승인하고, 참조하고, 통보받을 것인지를 명확히 하는 책임분담 구조도를 의미 (Responsible: 책임수행자, \nAccountable: 최종책임자, Consulted: 의견제시자, Informed: 통보대상자)\n(예시) RACI 차트를활용한책임수행체계구성\n(참고) 보조수단성하 인적개입원칙및 활용\n13\n논의의배경/ 7대원칙/ 결론\n법 제34조 (고영향 인공지능과 관련한 사업자의 책무) ① 인공지능사업자는 고영향 인공지능 또는\n이를 이용한 제품・서비스를 제공하는 경우 고영향 인공지능의 안전성・신뢰성을 확보하기 위하여\n다음 각 호의 내용을 포함하는 조치를 대통령령으로 정하는 바에 따라 이행하여야 한다. 4."}
{"meta": {"section_type": "guideline", "article_ref": "제10조", "domain": "금융", "guideline_source": "FSC"}, "text": "[GUIDELINE] FSC [Domain: 금융] [제10조]\n\n1. 거버넌스원칙(3/3)\n위험수준별로차등화된통제〮관리수행및모니터링, 문서화, 교육등위험통제를위한제반절차마련〮이행\n9\n논의의배경/ 7대원칙/ 결론\n(예시) AI 서비스 위험 수준에 따른 차등화된 통제·관리 수행방안\n- 모든 AI 서비스에 기본 통제 방안을 적용하되, 저위험 AI 서비스는 완화된 통제 방안을 적용하여 운영 효율성 도모\n- 고위험 AI 서비스는 AI 윤리위원회 사전 승인·사후 검증, 제3자에 의한 평가 검증, 운영단계 모니터링 강화 등 추가 통제\n- AI 기본법에 따른 ‘고영향 AI’에 대해서는 위험점수에 따른 등급 분류와 관계없이 ‘고위험’으로 분류하여 통제 강화\n(참고) 금융감독원의「금융분야AI위험관리프레임워크(AI RMF)」\n10\n논의의배경/ 7대원칙/ 결론\n금융회사가AI 시스템의도입활용전주기에걸쳐유연하고체계적으로AI 위험을관리할수있도록, \n①거버넌스, ②위험평가, ③위험통제와관련한핵심프로세스를제시\n※ 거버넌스, 위험평가 및 위험통제 등의 구체적인 내용은 향후 금융감독원이 공개 예정인\n「금융분야 AI 위험관리 프레임워크」(AI Risk Management Framework in Financial Sector)를 참고 가능\n2. 합법성원칙\nAI 개발·이용시적용법규를사전파악, 내부정책및업무절차에반영및주기적인점검·개선\n- 공통적으로적용되는법령(인공지능기본법) 및하위법규(이하인공지능기본법령)와해당업종에따라적용되는관련법령을검토\n- 법규요구사항→ 내부정책및업무절차에반영, 주기적점검→ 실효성평가〮지속개선\n11\n논의의배경/ 7대원칙/ 결론\n구분 관련법규\n공통\n❶ 금융소비자 보호를 위한 영업행위 규정 준수 금융소비자보호법 제10조 및 제17~22조\n❷ 정보처리시스템의 안전성 확보 및 보호대택 수립・이행 전자금융감독규정 제14조 및 제21조\n❸ 위험관리 기준 및 절차 마련 금융사지배구조법 제27조 제1항\n❹ 자동화된 결정에 대한 정보주체 권리 보장\n(개인정보 처리시)\n개인정보보호법 제37조의2\n신용평가, 보험, \n여신 등\n❶ 자동화평가 등에 대한 설명 이행\n(개인신용정보처리시)\n신용정보법 제36조의2 제1항\n❷ 개인신용평가에 관한 원칙 준수 신용정보법 제22조의 3\n금융투자 등\n❶ 전자적 투자조언장치활용업무의요건 자본시장법시행령제2조및금융투자업규정 제1-2조의2\n❷ 전자적 투자조언장치의투자일임보고서작성・교부・보관등 자본시장법제99조, 동법 시행령 제100조, 금융투자업규정제4-78조, 4-13조\n금융분야관련법령\n3. 보조수단성원칙\nAI 산출물에대한최종책임은임직원이지며, 운영전단계에걸쳐임직원개입상황차등화\n- 인공지능을업무의보조수단으로활용하고최종의사결정과책임은임직원이수행하도록해야함. - 업무중요도, 위험수준에상응한의사결정단계별로역할과책임에대한사항을정함(예: RACI 차트). - 단, 고영향AI는사람이수행하는의사결정에보조적인용도로만사용(Human-in-the-loop)\n12\n논의의배경/ 7대원칙/ 결론\n구분 여신심사담당자 여신심사 팀장 리스크\n관리부서\n준법\n감시인 IT운영\n① 신청 접수·기본요건 확인 R I I I I\n② 모델 점수 산출·권고 표시 C I C I R\n③ 개입기준 판단·자료 보완 R A C C C\n④ 심사의견1차 검토·사유기재 R I C I I\n⑤ 상급자 검토/복수인 확인 R A C C I\n⑥ 최종결정·통지·기록 I A/R C C C\n주: RACI 차트란 업무 과정에서 누가 무엇을 책임지고, 승인하고, 참조하고, 통보받을 것인지를 명확히 하는 책임분담 구조도를 의미 (Responsible: 책임수행자, \nAccountable: 최종책임자, Consulted: 의견제시자, Informed: 통보대상자)\n(예시) RACI 차트를활용한책임수행체계구성\n(참고) 보조수단성하 인적개입원칙및 활용\n13\n논의의배경/ 7대원칙/ 결론\n법 제34조 (고영향 인공지능과 관련한 사업자의 책무) ① 인공지능사업자는 고영향 인공지능 또는\n이를 이용한 제품・서비스를 제공하는 경우 고영향 인공지능의 안전성・신뢰성을 확보하기 위하여\n다음 각 호의 내용을 포함하는 조치를 대통령령으로 정하는 바에 따라 이행하여야 한다. 4."}
{"meta": {"section_type": "guideline", "article_ref": "제10조", "domain": "신용", "guideline_source": "FSC"}, "text": "[GUIDELINE] FSC [Domain: 신용] [제10조]\n\n1. 거버넌스원칙(3/3)\n위험수준별로차등화된통제〮관리수행및모니터링, 문서화, 교육등위험통제를위한제반절차마련〮이행\n9\n논의의배경/ 7대원칙/ 결론\n(예시) AI 서비스 위험 수준에 따른 차등화된 통제·관리 수행방안\n- 모든 AI 서비스에 기본 통제 방안을 적용하되, 저위험 AI 서비스는 완화된 통제 방안을 적용하여 운영 효율성 도모\n- 고위험 AI 서비스는 AI 윤리위원회 사전 승인·사후 검증, 제3자에 의한 평가 검증, 운영단계 모니터링 강화 등 추가 통제\n- AI 기본법에 따른 ‘고영향 AI’에 대해서는 위험점수에 따른 등급 분류와 관계없이 ‘고위험’으로 분류하여 통제 강화\n(참고) 금융감독원의「금융분야AI위험관리프레임워크(AI RMF)」\n10\n논의의배경/ 7대원칙/ 결론\n금융회사가AI 시스템의도입활용전주기에걸쳐유연하고체계적으로AI 위험을관리할수있도록, \n①거버넌스, ②위험평가, ③위험통제와관련한핵심프로세스를제시\n※ 거버넌스, 위험평가 및 위험통제 등의 구체적인 내용은 향후 금융감독원이 공개 예정인\n「금융분야 AI 위험관리 프레임워크」(AI Risk Management Framework in Financial Sector)를 참고 가능\n2. 합법성원칙\nAI 개발·이용시적용법규를사전파악, 내부정책및업무절차에반영및주기적인점검·개선\n- 공통적으로적용되는법령(인공지능기본법) 및하위법규(이하인공지능기본법령)와해당업종에따라적용되는관련법령을검토\n- 법규요구사항→ 내부정책및업무절차에반영, 주기적점검→ 실효성평가〮지속개선\n11\n논의의배경/ 7대원칙/ 결론\n구분 관련법규\n공통\n❶ 금융소비자 보호를 위한 영업행위 규정 준수 금융소비자보호법 제10조 및 제17~22조\n❷ 정보처리시스템의 안전성 확보 및 보호대택 수립・이행 전자금융감독규정 제14조 및 제21조\n❸ 위험관리 기준 및 절차 마련 금융사지배구조법 제27조 제1항\n❹ 자동화된 결정에 대한 정보주체 권리 보장\n(개인정보 처리시)\n개인정보보호법 제37조의2\n신용평가, 보험, \n여신 등\n❶ 자동화평가 등에 대한 설명 이행\n(개인신용정보처리시)\n신용정보법 제36조의2 제1항\n❷ 개인신용평가에 관한 원칙 준수 신용정보법 제22조의 3\n금융투자 등\n❶ 전자적 투자조언장치활용업무의요건 자본시장법시행령제2조및금융투자업규정 제1-2조의2\n❷ 전자적 투자조언장치의투자일임보고서작성・교부・보관등 자본시장법제99조, 동법 시행령 제100조, 금융투자업규정제4-78조, 4-13조\n금융분야관련법령\n3. 보조수단성원칙\nAI 산출물에대한최종책임은임직원이지며, 운영전단계에걸쳐임직원개입상황차등화\n- 인공지능을업무의보조수단으로활용하고최종의사결정과책임은임직원이수행하도록해야함. - 업무중요도, 위험수준에상응한의사결정단계별로역할과책임에대한사항을정함(예: RACI 차트). - 단, 고영향AI는사람이수행하는의사결정에보조적인용도로만사용(Human-in-the-loop)\n12\n논의의배경/ 7대원칙/ 결론\n구분 여신심사담당자 여신심사 팀장 리스크\n관리부서\n준법\n감시인 IT운영\n① 신청 접수·기본요건 확인 R I I I I\n② 모델 점수 산출·권고 표시 C I C I R\n③ 개입기준 판단·자료 보완 R A C C C\n④ 심사의견1차 검토·사유기재 R I C I I\n⑤ 상급자 검토/복수인 확인 R A C C I\n⑥ 최종결정·통지·기록 I A/R C C C\n주: RACI 차트란 업무 과정에서 누가 무엇을 책임지고, 승인하고, 참조하고, 통보받을 것인지를 명확히 하는 책임분담 구조도를 의미 (Responsible: 책임수행자, \nAccountable: 최종책임자, Consulted: 의견제시자, Informed: 통보대상자)\n(예시) RACI 차트를활용한책임수행체계구성\n(참고) 보조수단성하 인적개입원칙및 활용\n13\n논의의배경/ 7대원칙/ 결론\n법 제34조 (고영향 인공지능과 관련한 사업자의 책무) ① 인공지능사업자는 고영향 인공지능 또는\n이를 이용한 제품・서비스를 제공하는 경우 고영향 인공지능의 안전성・신뢰성을 확보하기 위하여\n다음 각 호의 내용을 포함하는 조치를 대통령령으로 정하는 바에 따라 이행하여야 한다. 4."}
{"meta": {"section_type": "guideline", "article_ref": "제27조", "domain": "교육", "guideline_source": "FSC"}, "text": "[GUIDELINE] FSC [Domain: 교육] [제27조]\n\n고영향 인공지능에 대한 사람의 관리・감독\n시행령(초안) 제27조 (고영향 인공지능과 관련한 사업자의 책무) ① 인공지능사업자는\n법제34조제1항 각 호의 조치 중에서 다음 각 호에 해당하는 내용을 자신의 홈페이지 등에\n게시하여야 한다. (이하 생략)\n4. 해당 고영향 인공지능을 관리・감독하는 사람의 성명 및 연락처\n사업자책무고시(초안) 제7조 (사람의 관리・감독) ① 사업자는 인공지능시스템 개발 과정에서\n사람의 관리감독을 위해 다음 각 호의 조치를 이행하여야 한다. 1. 사람이 인공지능 동작에 개입할 수 있는 기준 확립\n2. 사람이 즉각적으로 인공지능시스템을 정지하거나 작동을 변경할 수 있는 ‘긴급 정지’ 기능 등의\n개입 방법 마련\n② 사업자는 고영향 인공지능 운영 중 사람의 관리・감독을 위해 다음 각 호의 조치를 이행하여야\n한다. 1. 성능저하 및 오류 발생에 대한 정기적인 점검계획 및 방안 마련\n2. 인공지능의 범위 및 수행능력에 대한 이해도를 향상시키기 위한 교육 및 훈련 제공\n【관련법규】인 공 지 능 기 본법제34조/ 동법시행령(초안) 제26조/ 사업자책무고시(초안) 제7조\n구분 주요 내용 및 설계 방법 고위험 · \n고영향 AI\n사전 승인 절차\n◦ (내용) 사람의 검토 또는 승인을 거쳐 실행\n◦ (방법) 전자 승인·이중 승인·위원회 심의 등\n◦ (기타) 사유·근거 기록 의무화\n적용\nAI 권고\n무시/수정/반대\n결정(Override)\n◦ (내용) 감독자가합당한사유로모델 권고를\n무시·수정 또는 반대결정 권한 부여\n◦ (방법) 내부체계확립후오버라이드UI 화면제공\n◦ (기타) 사유·증빙 등을 표준화하여 기록\n필수 적용\n긴급 정지 기능 및\n격리\n◦ (내용) 이상발생시 즉시 중단 및 문제 요소\n격리, 안전모드 전환 후 롤백·재가동\n◦ (방법) 긴급정지. 안전모드전환 등을 위한\n직관적 UI 제공\n* 감독자 접근권한확보 필요\n필수 적용\n실시간\n모니터링\n대시보드\n◦ (내용) 성능, 공정성, 신뢰도, 규제/제재 신호\n등을 실시간 시각화\n◦ (방법) 임계치 초과 시 자동 알림·보고\n권고\n[참고] NIST AI RMF Playbook\n인공지능의사결정에대한인적개입방법\n4. 신뢰성원칙\n모델성능관리, 데이터품질확보, 의사결정과정설명, 체계적검증및오류대응체계등을구축\n- (모델성능관리) AI모델성능측정을위한명확한지표설정, 정기적점검∙개선\n- (데이터품질) AI 학습및참조에사용하는데이터및AI 시스템에입력되는데이터의품질검증∙확인\n- (설명가능성) AI 의사결정과정과결과← 이해관계자의합리적이해가가능하도록설명가능한형태로제공(신뢰성↑)\n14\n논의의배경/ 7대원칙/ 결론\n- 결과: 거절\n- 이용된 기초정보(해당 고객의 주요 원본 정보)\n- 주요 판단 사유: 제1금융권 대출 건수(부정적), 리볼빙 잔액\n비율(부정적), 거래 연체 비율(긍정적), 상환 비율(긍정적)\nAI 설명제공예시\n고객님은 연체되지 않은 거래 비율 94% 및\n과거 정상 상환된 신용 거래 비율 96%가\n긍정적인 요인이었으나, \n제1금융권 대출 건수 18건 및 신용한도 대비\n리볼빙 잔액 비율 63%가 부정적으로 작용하여\n대출이 거절되었습니다. 자동 설명 생성\n고객 대응 업무\n담당자가\n해석하여 설명\n생성\n항목명 설명 값\n제1금융권 대출 건수 제1금융권 대출 건수 18\n리볼빙 잔액 비율 신용한도 대비 리볼빙 잔액 비율 63\n거래 연체 비율 연체되지 않은 거래 비율 94\n상환 비율 과거 정상 상환된 신용 거래 비율 96\n... (이하 생략) ..."}
{"meta": {"section_type": "guideline", "article_ref": "제27조", "domain": "금융", "guideline_source": "FSC"}, "text": "[GUIDELINE] FSC [Domain: 금융] [제27조]\n\n고영향 인공지능에 대한 사람의 관리・감독\n시행령(초안) 제27조 (고영향 인공지능과 관련한 사업자의 책무) ① 인공지능사업자는\n법제34조제1항 각 호의 조치 중에서 다음 각 호에 해당하는 내용을 자신의 홈페이지 등에\n게시하여야 한다. (이하 생략)\n4. 해당 고영향 인공지능을 관리・감독하는 사람의 성명 및 연락처\n사업자책무고시(초안) 제7조 (사람의 관리・감독) ① 사업자는 인공지능시스템 개발 과정에서\n사람의 관리감독을 위해 다음 각 호의 조치를 이행하여야 한다. 1. 사람이 인공지능 동작에 개입할 수 있는 기준 확립\n2. 사람이 즉각적으로 인공지능시스템을 정지하거나 작동을 변경할 수 있는 ‘긴급 정지’ 기능 등의\n개입 방법 마련\n② 사업자는 고영향 인공지능 운영 중 사람의 관리・감독을 위해 다음 각 호의 조치를 이행하여야\n한다. 1. 성능저하 및 오류 발생에 대한 정기적인 점검계획 및 방안 마련\n2. 인공지능의 범위 및 수행능력에 대한 이해도를 향상시키기 위한 교육 및 훈련 제공\n【관련법규】인 공 지 능 기 본법제34조/ 동법시행령(초안) 제26조/ 사업자책무고시(초안) 제7조\n구분 주요 내용 및 설계 방법 고위험 · \n고영향 AI\n사전 승인 절차\n◦ (내용) 사람의 검토 또는 승인을 거쳐 실행\n◦ (방법) 전자 승인·이중 승인·위원회 심의 등\n◦ (기타) 사유·근거 기록 의무화\n적용\nAI 권고\n무시/수정/반대\n결정(Override)\n◦ (내용) 감독자가합당한사유로모델 권고를\n무시·수정 또는 반대결정 권한 부여\n◦ (방법) 내부체계확립후오버라이드UI 화면제공\n◦ (기타) 사유·증빙 등을 표준화하여 기록\n필수 적용\n긴급 정지 기능 및\n격리\n◦ (내용) 이상발생시 즉시 중단 및 문제 요소\n격리, 안전모드 전환 후 롤백·재가동\n◦ (방법) 긴급정지. 안전모드전환 등을 위한\n직관적 UI 제공\n* 감독자 접근권한확보 필요\n필수 적용\n실시간\n모니터링\n대시보드\n◦ (내용) 성능, 공정성, 신뢰도, 규제/제재 신호\n등을 실시간 시각화\n◦ (방법) 임계치 초과 시 자동 알림·보고\n권고\n[참고] NIST AI RMF Playbook\n인공지능의사결정에대한인적개입방법\n4. 신뢰성원칙\n모델성능관리, 데이터품질확보, 의사결정과정설명, 체계적검증및오류대응체계등을구축\n- (모델성능관리) AI모델성능측정을위한명확한지표설정, 정기적점검∙개선\n- (데이터품질) AI 학습및참조에사용하는데이터및AI 시스템에입력되는데이터의품질검증∙확인\n- (설명가능성) AI 의사결정과정과결과← 이해관계자의합리적이해가가능하도록설명가능한형태로제공(신뢰성↑)\n14\n논의의배경/ 7대원칙/ 결론\n- 결과: 거절\n- 이용된 기초정보(해당 고객의 주요 원본 정보)\n- 주요 판단 사유: 제1금융권 대출 건수(부정적), 리볼빙 잔액\n비율(부정적), 거래 연체 비율(긍정적), 상환 비율(긍정적)\nAI 설명제공예시\n고객님은 연체되지 않은 거래 비율 94% 및\n과거 정상 상환된 신용 거래 비율 96%가\n긍정적인 요인이었으나, \n제1금융권 대출 건수 18건 및 신용한도 대비\n리볼빙 잔액 비율 63%가 부정적으로 작용하여\n대출이 거절되었습니다. 자동 설명 생성\n고객 대응 업무\n담당자가\n해석하여 설명\n생성\n항목명 설명 값\n제1금융권 대출 건수 제1금융권 대출 건수 18\n리볼빙 잔액 비율 신용한도 대비 리볼빙 잔액 비율 63\n거래 연체 비율 연체되지 않은 거래 비율 94\n상환 비율 과거 정상 상환된 신용 거래 비율 96\n... (이하 생략) ..."}
{"meta": {"section_type": "guideline", "article_ref": "제27조", "domain": "신용", "guideline_source": "FSC"}, "text": "[GUIDELINE] FSC [Domain: 신용] [제27조]\n\n고영향 인공지능에 대한 사람의 관리・감독\n시행령(초안) 제27조 (고영향 인공지능과 관련한 사업자의 책무) ① 인공지능사업자는\n법제34조제1항 각 호의 조치 중에서 다음 각 호에 해당하는 내용을 자신의 홈페이지 등에\n게시하여야 한다. (이하 생략)\n4. 해당 고영향 인공지능을 관리・감독하는 사람의 성명 및 연락처\n사업자책무고시(초안) 제7조 (사람의 관리・감독) ① 사업자는 인공지능시스템 개발 과정에서\n사람의 관리감독을 위해 다음 각 호의 조치를 이행하여야 한다. 1. 사람이 인공지능 동작에 개입할 수 있는 기준 확립\n2. 사람이 즉각적으로 인공지능시스템을 정지하거나 작동을 변경할 수 있는 ‘긴급 정지’ 기능 등의\n개입 방법 마련\n② 사업자는 고영향 인공지능 운영 중 사람의 관리・감독을 위해 다음 각 호의 조치를 이행하여야\n한다. 1. 성능저하 및 오류 발생에 대한 정기적인 점검계획 및 방안 마련\n2. 인공지능의 범위 및 수행능력에 대한 이해도를 향상시키기 위한 교육 및 훈련 제공\n【관련법규】인 공 지 능 기 본법제34조/ 동법시행령(초안) 제26조/ 사업자책무고시(초안) 제7조\n구분 주요 내용 및 설계 방법 고위험 · \n고영향 AI\n사전 승인 절차\n◦ (내용) 사람의 검토 또는 승인을 거쳐 실행\n◦ (방법) 전자 승인·이중 승인·위원회 심의 등\n◦ (기타) 사유·근거 기록 의무화\n적용\nAI 권고\n무시/수정/반대\n결정(Override)\n◦ (내용) 감독자가합당한사유로모델 권고를\n무시·수정 또는 반대결정 권한 부여\n◦ (방법) 내부체계확립후오버라이드UI 화면제공\n◦ (기타) 사유·증빙 등을 표준화하여 기록\n필수 적용\n긴급 정지 기능 및\n격리\n◦ (내용) 이상발생시 즉시 중단 및 문제 요소\n격리, 안전모드 전환 후 롤백·재가동\n◦ (방법) 긴급정지. 안전모드전환 등을 위한\n직관적 UI 제공\n* 감독자 접근권한확보 필요\n필수 적용\n실시간\n모니터링\n대시보드\n◦ (내용) 성능, 공정성, 신뢰도, 규제/제재 신호\n등을 실시간 시각화\n◦ (방법) 임계치 초과 시 자동 알림·보고\n권고\n[참고] NIST AI RMF Playbook\n인공지능의사결정에대한인적개입방법\n4. 신뢰성원칙\n모델성능관리, 데이터품질확보, 의사결정과정설명, 체계적검증및오류대응체계등을구축\n- (모델성능관리) AI모델성능측정을위한명확한지표설정, 정기적점검∙개선\n- (데이터품질) AI 학습및참조에사용하는데이터및AI 시스템에입력되는데이터의품질검증∙확인\n- (설명가능성) AI 의사결정과정과결과← 이해관계자의합리적이해가가능하도록설명가능한형태로제공(신뢰성↑)\n14\n논의의배경/ 7대원칙/ 결론\n- 결과: 거절\n- 이용된 기초정보(해당 고객의 주요 원본 정보)\n- 주요 판단 사유: 제1금융권 대출 건수(부정적), 리볼빙 잔액\n비율(부정적), 거래 연체 비율(긍정적), 상환 비율(긍정적)\nAI 설명제공예시\n고객님은 연체되지 않은 거래 비율 94% 및\n과거 정상 상환된 신용 거래 비율 96%가\n긍정적인 요인이었으나, \n제1금융권 대출 건수 18건 및 신용한도 대비\n리볼빙 잔액 비율 63%가 부정적으로 작용하여\n대출이 거절되었습니다. 자동 설명 생성\n고객 대응 업무\n담당자가\n해석하여 설명\n생성\n항목명 설명 값\n제1금융권 대출 건수 제1금융권 대출 건수 18\n리볼빙 잔액 비율 신용한도 대비 리볼빙 잔액 비율 63\n거래 연체 비율 연체되지 않은 거래 비율 94\n상환 비율 과거 정상 상환된 신용 거래 비율 96\n... (이하 생략) ..."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "교육", "guideline_source": "FSC"}, "text": "[GUIDELINE] FSC [Domain: 교육]\n\n(참고) 데이터품질관리∙편향성예시\n15\n논의의배경/ 7대원칙/ 결론\n□ 치우친 표본(Skewed Sample): 우연히 초기 편향이 발생하는 경우 시간이\n지남에 따라 편향 증폭\n※ 예) 초기 범죄율이 높은 곳으로 더 많은 경찰관을 파견하는 경향이 있고, 그\n러한 지역에서 범죄율에 대한 기록이 더 높아질 확률이 높음\n□ 오염된 사례(Tainted Example): 축적된 데이터에 존재하는 사람의 편견을\n알고리즘에서 특별히 교정하지 않고 유지하는 경우 동일한 편향이 복제됨\n※ 예) 구글 뉴스 기사에서 남성-프로그래머의 관계는 여성-주부와의 관계와\n매우 유사한 것으로 밝혀짐(Bolukbasi et al., 2016)\n□ 제한된 속성(Limited Feature): 데이터의 특정 속성에 대해 소수 그룹에 대\n해서는 제한되거나 낮은 신뢰도의 정보만 수집\n□ 표본 크기의 불일치(Sample Size Disparity): 소수 그룹에서 제공되는 학습\n데이터가 대다수 그룹에서 제공되는 학습데이터보다 훨씬 적은 경우 소수 그룹\n을 정확히 모델링 할 가능성이 낮음\n□ 대리 변수의 존재(Proxy): 학습 시 공정성 측면에서 민감한 데이터 속성(인\n종, 성별 등)을 사용하지 않더라도 이를 대리하는 다른 속성(이웃 등)이 항상 존\n재할 수 있어, 이러한 속성이 포함되어 있으면 편향이 계속 발생\n자료: 기계학습 공정성 관련 연구 동향(소프트웨어정책연구소, `20.2월)\n(예시) AI 편향성의원인\n항목 설명\n노이즈\n(Noise) ∙ 측정 과정에서 무작위로 발생하는 측정값의 오류\n이상치\n(Outlier)\n∙ 나머지 데이터와 현저히 다른 특성을 보이는 값\n∙ 데이터 입력･측정 오류/실험 오류로 발생할 수 있지만, 일부 예외\n특성을 갖는 값일 수 있음\n결측치\n(Missing Value) ∙ 전산오류 및 미입력 등의 이유로 누락된 측정값\n불일치 값\n(Mismatch Value) ∙ 동일 개체에 있어, 측정 데이터가 다르게 나타나는 경우\n중복\n(Duplicate) ∙ 모든 속성 및 값이 동일한 경우\n바이어스\n(Bias) ∙ 측정 장비에서 측정하는 값과 실제 값과의 차이점\n아티펙트\n(Artifact)\n∙ 외부 요인으로 인해 반복적으로 발생하는 왜곡이나 에러\n※ (예시) 카메라를 이용한 영상 데이터 획득에 있어, 렌즈의 얼룩에 의\n해 지속적인 왜곡 발생 등\n오염\n(Poisoning) ∙ 악의적인 목적으로 변조한 데이터\n(예시) 데이터전처리시에처리해야할요소\n5. 금융안정성원칙\nAI의 설계, 학습 등 전 과정에서 금융안정성 위험을 최소화\n- (금융안정위험 평가∙관리) AI 시스템이 금융시장 전반, 금융안정에 미치는 영향 등을 평가하고 관리하는 방안 마련\n- (안전장치 마련) AI 시스템 오작동 시 백업모형 활용, 사후 개입을 위한 비상정지 장치, 회생계획 설계 등 안전장치 마련\n- (제3자 IT리스크 관리) AI 모델 외주개발 또는 오픈소스 기반 AI 활용 → 별도 제3자 IT리스크 관리방안 마련\n- (감독당국 정보공유 및 보고) 시스템위험을 초래할 수 있는 AI 사고 발생 시 감독당국에 보고하고, \nAI 활용구조를 감독당국이 사전에 파악할 수 있도록 정보 공유 필요\n16\n논의의배경/ 7대원칙/ 결론\n(예시) AI 관련 제3자 계약 체결 시 포함 사항\n① 제3자가 제공하는 서비스 등에 대해 기능, 위험 등에 대한 명확한 설명\n② 사이버 보안, 데이터 및 프라이버시 보호에 관한 내용\n③ 서비스를 제공하는 지역, 국가 장소\n④ 제3자의 파산 및 사업 운영 중단 또는 계약 종료 시 해당 서비스, 데이터 등에 대한\n금융회사 등의 접근 및 복구, 반환의 보장에 관한 내용\n⑤ 사고 발생 시 정보 공유 및 사고처리, 배상책임과 관련한 내용\n⑥ AI 관련 피해 발생 시 책임 소재\n⑦ AI 관련 법규 명령 및 정책 등의 준수 의무\n⑧ 안전하고 신뢰할 수 있는 AI 개발·활용과 관련 금융회사 등의 행동강령, 정책 등 준수\n6-7."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "금융", "guideline_source": "FSC"}, "text": "[GUIDELINE] FSC [Domain: 금융]\n\n(참고) 데이터품질관리∙편향성예시\n15\n논의의배경/ 7대원칙/ 결론\n□ 치우친 표본(Skewed Sample): 우연히 초기 편향이 발생하는 경우 시간이\n지남에 따라 편향 증폭\n※ 예) 초기 범죄율이 높은 곳으로 더 많은 경찰관을 파견하는 경향이 있고, 그\n러한 지역에서 범죄율에 대한 기록이 더 높아질 확률이 높음\n□ 오염된 사례(Tainted Example): 축적된 데이터에 존재하는 사람의 편견을\n알고리즘에서 특별히 교정하지 않고 유지하는 경우 동일한 편향이 복제됨\n※ 예) 구글 뉴스 기사에서 남성-프로그래머의 관계는 여성-주부와의 관계와\n매우 유사한 것으로 밝혀짐(Bolukbasi et al., 2016)\n□ 제한된 속성(Limited Feature): 데이터의 특정 속성에 대해 소수 그룹에 대\n해서는 제한되거나 낮은 신뢰도의 정보만 수집\n□ 표본 크기의 불일치(Sample Size Disparity): 소수 그룹에서 제공되는 학습\n데이터가 대다수 그룹에서 제공되는 학습데이터보다 훨씬 적은 경우 소수 그룹\n을 정확히 모델링 할 가능성이 낮음\n□ 대리 변수의 존재(Proxy): 학습 시 공정성 측면에서 민감한 데이터 속성(인\n종, 성별 등)을 사용하지 않더라도 이를 대리하는 다른 속성(이웃 등)이 항상 존\n재할 수 있어, 이러한 속성이 포함되어 있으면 편향이 계속 발생\n자료: 기계학습 공정성 관련 연구 동향(소프트웨어정책연구소, `20.2월)\n(예시) AI 편향성의원인\n항목 설명\n노이즈\n(Noise) ∙ 측정 과정에서 무작위로 발생하는 측정값의 오류\n이상치\n(Outlier)\n∙ 나머지 데이터와 현저히 다른 특성을 보이는 값\n∙ 데이터 입력･측정 오류/실험 오류로 발생할 수 있지만, 일부 예외\n특성을 갖는 값일 수 있음\n결측치\n(Missing Value) ∙ 전산오류 및 미입력 등의 이유로 누락된 측정값\n불일치 값\n(Mismatch Value) ∙ 동일 개체에 있어, 측정 데이터가 다르게 나타나는 경우\n중복\n(Duplicate) ∙ 모든 속성 및 값이 동일한 경우\n바이어스\n(Bias) ∙ 측정 장비에서 측정하는 값과 실제 값과의 차이점\n아티펙트\n(Artifact)\n∙ 외부 요인으로 인해 반복적으로 발생하는 왜곡이나 에러\n※ (예시) 카메라를 이용한 영상 데이터 획득에 있어, 렌즈의 얼룩에 의\n해 지속적인 왜곡 발생 등\n오염\n(Poisoning) ∙ 악의적인 목적으로 변조한 데이터\n(예시) 데이터전처리시에처리해야할요소\n5. 금융안정성원칙\nAI의 설계, 학습 등 전 과정에서 금융안정성 위험을 최소화\n- (금융안정위험 평가∙관리) AI 시스템이 금융시장 전반, 금융안정에 미치는 영향 등을 평가하고 관리하는 방안 마련\n- (안전장치 마련) AI 시스템 오작동 시 백업모형 활용, 사후 개입을 위한 비상정지 장치, 회생계획 설계 등 안전장치 마련\n- (제3자 IT리스크 관리) AI 모델 외주개발 또는 오픈소스 기반 AI 활용 → 별도 제3자 IT리스크 관리방안 마련\n- (감독당국 정보공유 및 보고) 시스템위험을 초래할 수 있는 AI 사고 발생 시 감독당국에 보고하고, \nAI 활용구조를 감독당국이 사전에 파악할 수 있도록 정보 공유 필요\n16\n논의의배경/ 7대원칙/ 결론\n(예시) AI 관련 제3자 계약 체결 시 포함 사항\n① 제3자가 제공하는 서비스 등에 대해 기능, 위험 등에 대한 명확한 설명\n② 사이버 보안, 데이터 및 프라이버시 보호에 관한 내용\n③ 서비스를 제공하는 지역, 국가 장소\n④ 제3자의 파산 및 사업 운영 중단 또는 계약 종료 시 해당 서비스, 데이터 등에 대한\n금융회사 등의 접근 및 복구, 반환의 보장에 관한 내용\n⑤ 사고 발생 시 정보 공유 및 사고처리, 배상책임과 관련한 내용\n⑥ AI 관련 피해 발생 시 책임 소재\n⑦ AI 관련 법규 명령 및 정책 등의 준수 의무\n⑧ 안전하고 신뢰할 수 있는 AI 개발·활용과 관련 금융회사 등의 행동강령, 정책 등 준수\n6-7."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "신용", "guideline_source": "FSC"}, "text": "[GUIDELINE] FSC [Domain: 신용]\n\n(참고) 데이터품질관리∙편향성예시\n15\n논의의배경/ 7대원칙/ 결론\n□ 치우친 표본(Skewed Sample): 우연히 초기 편향이 발생하는 경우 시간이\n지남에 따라 편향 증폭\n※ 예) 초기 범죄율이 높은 곳으로 더 많은 경찰관을 파견하는 경향이 있고, 그\n러한 지역에서 범죄율에 대한 기록이 더 높아질 확률이 높음\n□ 오염된 사례(Tainted Example): 축적된 데이터에 존재하는 사람의 편견을\n알고리즘에서 특별히 교정하지 않고 유지하는 경우 동일한 편향이 복제됨\n※ 예) 구글 뉴스 기사에서 남성-프로그래머의 관계는 여성-주부와의 관계와\n매우 유사한 것으로 밝혀짐(Bolukbasi et al., 2016)\n□ 제한된 속성(Limited Feature): 데이터의 특정 속성에 대해 소수 그룹에 대\n해서는 제한되거나 낮은 신뢰도의 정보만 수집\n□ 표본 크기의 불일치(Sample Size Disparity): 소수 그룹에서 제공되는 학습\n데이터가 대다수 그룹에서 제공되는 학습데이터보다 훨씬 적은 경우 소수 그룹\n을 정확히 모델링 할 가능성이 낮음\n□ 대리 변수의 존재(Proxy): 학습 시 공정성 측면에서 민감한 데이터 속성(인\n종, 성별 등)을 사용하지 않더라도 이를 대리하는 다른 속성(이웃 등)이 항상 존\n재할 수 있어, 이러한 속성이 포함되어 있으면 편향이 계속 발생\n자료: 기계학습 공정성 관련 연구 동향(소프트웨어정책연구소, `20.2월)\n(예시) AI 편향성의원인\n항목 설명\n노이즈\n(Noise) ∙ 측정 과정에서 무작위로 발생하는 측정값의 오류\n이상치\n(Outlier)\n∙ 나머지 데이터와 현저히 다른 특성을 보이는 값\n∙ 데이터 입력･측정 오류/실험 오류로 발생할 수 있지만, 일부 예외\n특성을 갖는 값일 수 있음\n결측치\n(Missing Value) ∙ 전산오류 및 미입력 등의 이유로 누락된 측정값\n불일치 값\n(Mismatch Value) ∙ 동일 개체에 있어, 측정 데이터가 다르게 나타나는 경우\n중복\n(Duplicate) ∙ 모든 속성 및 값이 동일한 경우\n바이어스\n(Bias) ∙ 측정 장비에서 측정하는 값과 실제 값과의 차이점\n아티펙트\n(Artifact)\n∙ 외부 요인으로 인해 반복적으로 발생하는 왜곡이나 에러\n※ (예시) 카메라를 이용한 영상 데이터 획득에 있어, 렌즈의 얼룩에 의\n해 지속적인 왜곡 발생 등\n오염\n(Poisoning) ∙ 악의적인 목적으로 변조한 데이터\n(예시) 데이터전처리시에처리해야할요소\n5. 금융안정성원칙\nAI의 설계, 학습 등 전 과정에서 금융안정성 위험을 최소화\n- (금융안정위험 평가∙관리) AI 시스템이 금융시장 전반, 금융안정에 미치는 영향 등을 평가하고 관리하는 방안 마련\n- (안전장치 마련) AI 시스템 오작동 시 백업모형 활용, 사후 개입을 위한 비상정지 장치, 회생계획 설계 등 안전장치 마련\n- (제3자 IT리스크 관리) AI 모델 외주개발 또는 오픈소스 기반 AI 활용 → 별도 제3자 IT리스크 관리방안 마련\n- (감독당국 정보공유 및 보고) 시스템위험을 초래할 수 있는 AI 사고 발생 시 감독당국에 보고하고, \nAI 활용구조를 감독당국이 사전에 파악할 수 있도록 정보 공유 필요\n16\n논의의배경/ 7대원칙/ 결론\n(예시) AI 관련 제3자 계약 체결 시 포함 사항\n① 제3자가 제공하는 서비스 등에 대해 기능, 위험 등에 대한 명확한 설명\n② 사이버 보안, 데이터 및 프라이버시 보호에 관한 내용\n③ 서비스를 제공하는 지역, 국가 장소\n④ 제3자의 파산 및 사업 운영 중단 또는 계약 종료 시 해당 서비스, 데이터 등에 대한\n금융회사 등의 접근 및 복구, 반환의 보장에 관한 내용\n⑤ 사고 발생 시 정보 공유 및 사고처리, 배상책임과 관련한 내용\n⑥ AI 관련 피해 발생 시 책임 소재\n⑦ AI 관련 법규 명령 및 정책 등의 준수 의무\n⑧ 안전하고 신뢰할 수 있는 AI 개발·활용과 관련 금융회사 등의 행동강령, 정책 등 준수\n6-7."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "교육", "guideline_source": "FSC"}, "text": "[GUIDELINE] FSC [Domain: 교육]\n\n신의성실성및 보안성원칙\n(신의성실성) AI활용對고객서비스제공→ 소비자이익최우선고려\n- (이해상충방지) 대고객서비스AI 활용시, 이해상충문제발생방지를위한관리∙감독장치마련\n- (소비자보호대책마련) AI활용과정에서충실한소비자보호→ AI활용사실사전고지, 피해대응을위한절차마련\n17\n논의의배경/ 7대원칙/ 결론\n(보안성) AI시스템보안성확보→ AI 시스템고유보안위협식별& 대응방안마련\n- (AI특화보안위협식별∙관리) AI시스템에특화된보안위협체계적식별, 대응전략마련\n- (AI 특화공격탐지∙대응) 식별된AI특화공격탐지, 차단, 대응체계구축\n- (AI자산보호및관리) 핵심자산(데이터, 파라미터등)의무단접근∙유출∙변조되지않도록보호대책적용\n- (외부모델및데이터검증) 외부도입모델∙데이터→ 보안및신뢰성검증수행→ 공급망위험최소화\n- (기존보안관리AI확장) 기존IT보안체계기반의AI시스템보안확장적용\n- (AI시스템보안성검증, 운영관리) 개발단계부터보안성의체계적검증, 지속적관리\n결론\n31 2\n18"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "금융", "guideline_source": "FSC"}, "text": "[GUIDELINE] FSC [Domain: 금융]\n\n신의성실성및 보안성원칙\n(신의성실성) AI활용對고객서비스제공→ 소비자이익최우선고려\n- (이해상충방지) 대고객서비스AI 활용시, 이해상충문제발생방지를위한관리∙감독장치마련\n- (소비자보호대책마련) AI활용과정에서충실한소비자보호→ AI활용사실사전고지, 피해대응을위한절차마련\n17\n논의의배경/ 7대원칙/ 결론\n(보안성) AI시스템보안성확보→ AI 시스템고유보안위협식별& 대응방안마련\n- (AI특화보안위협식별∙관리) AI시스템에특화된보안위협체계적식별, 대응전략마련\n- (AI 특화공격탐지∙대응) 식별된AI특화공격탐지, 차단, 대응체계구축\n- (AI자산보호및관리) 핵심자산(데이터, 파라미터등)의무단접근∙유출∙변조되지않도록보호대책적용\n- (외부모델및데이터검증) 외부도입모델∙데이터→ 보안및신뢰성검증수행→ 공급망위험최소화\n- (기존보안관리AI확장) 기존IT보안체계기반의AI시스템보안확장적용\n- (AI시스템보안성검증, 운영관리) 개발단계부터보안성의체계적검증, 지속적관리\n결론\n31 2\n18"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "신용", "guideline_source": "FSC"}, "text": "[GUIDELINE] FSC [Domain: 신용]\n\n신의성실성및 보안성원칙\n(신의성실성) AI활용對고객서비스제공→ 소비자이익최우선고려\n- (이해상충방지) 대고객서비스AI 활용시, 이해상충문제발생방지를위한관리∙감독장치마련\n- (소비자보호대책마련) AI활용과정에서충실한소비자보호→ AI활용사실사전고지, 피해대응을위한절차마련\n17\n논의의배경/ 7대원칙/ 결론\n(보안성) AI시스템보안성확보→ AI 시스템고유보안위협식별& 대응방안마련\n- (AI특화보안위협식별∙관리) AI시스템에특화된보안위협체계적식별, 대응전략마련\n- (AI 특화공격탐지∙대응) 식별된AI특화공격탐지, 차단, 대응체계구축\n- (AI자산보호및관리) 핵심자산(데이터, 파라미터등)의무단접근∙유출∙변조되지않도록보호대책적용\n- (외부모델및데이터검증) 외부도입모델∙데이터→ 보안및신뢰성검증수행→ 공급망위험최소화\n- (기존보안관리AI확장) 기존IT보안체계기반의AI시스템보안확장적용\n- (AI시스템보안성검증, 운영관리) 개발단계부터보안성의체계적검증, 지속적관리\n결론\n31 2\n18"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "금융", "guideline_source": "FSC"}, "text": "[GUIDELINE] FSC [Domain: 금융]\n\n논의의배경/7대원칙/ 결론\n결론\n19\n본가이드라인은금융업권의모범규준(Best Practice)으로서업계의자율적인노력을권장\n- 개별금융회사의상황에따라가이드라인의내용을적절히반영할필요가있음\n- AI 관련위험을관리하기위해금융회사간경험을공유하고업권별특성을반영하여자율규제를하는것이원칙\nAI 관련기술및정책논의가발전함에따라본가이드라인도지속적으로개정할예정\n- AI 기술이전격적인도입이시작된지얼마되지않아금융분야관련정책방식에대해현재도활발히논의되고있음\n- 향후각국당국과지식공유를통해금융분야위험사례들을선제적으로파악하고사고예방을위해장치를마련할필요\n- 기술변화에따른보안기준변경등세부내용은지속적으로개정할예정\n인공지능기본법하위법령의세부내용및금융업권의의견을지속적으로반영하여개정예정\n- 25년11월인공지능기본법시행령의입법예고가있었으나아직기타고시, 가이드라인등의구체적인내용은확정되지않은상황\n- 향후정책방향이구체화되면이를반영하여본금융분야AI 가이드라인도개정할예정\n- 또한업권의의견을지속수렴하여본가이드라인을개정하는것이큰방향임. 감사합니다\n한국금융연구원연구위원백연주\n2025.12."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용]\n\n- 2 -\n본안내서는이러한정책적경험을종합하여,생성형AI수명주기(lifecycle)각단계에서고려해야할개인정보처리및보호이슈를체계화하고,그에따른법적기준및안전조치등을제시하였습니다.이를통해생성형AI개발․활용에개인정보보호관점이균형있게반영되고,현장의예측가능성과자율적준수역량을높여신뢰와책임에기반한AI활용환경조성에기여하고자합니다.아울러,본안내서는글로벌논의와의연계성및정합성을고려하여작성되었으며,미국․영국․유럽연합등주요국의관련정책자료를참고할수있도록함께안내하였습니다.참고 본 안내서에 포함된 주요 법령 및 정책 사례□ 개인정보 보호법<\u0000개인정보보호법중관련조항>‣ 목적(§1):\u0000개인의자유와권리보호/\u0000개인의존엄과가치구현‣ 원칙(§3):\u0000목적명확성/\u0000최소수집/\u0000투명성/\u0000안전한관리/\u0000정보주체권리보장등‣ 개인정보의처리\u0000․ 수집․이용(§15➀):\u0000동의/\u0000법률/\u0000공공기관의소관업무/\u0000계약/\u0000정당한이익등\u0000․ 추가적이용(§15➂):\u0000당초수집목적과관련성/\u0000예측가능성/\u0000정보주체이익침해/\u0000안전조치\u0000․ 처리위탁(§26):\u0000문서로위탁/\u0000개인정보처리방침공개/\u0000수탁자감독등\u0000․ 가명특례(§28의2):\u0000통계작성․과학적연구목적등위한동의없는가명정보처리\u0000․ 국외이전(§28의8):\u0000별도동의·고지/\u0000법률․조약/\u0000계약/\u0000적정성결정등\u0000․\u0000특별한보호가필요한개인정보(§23~§24의2):\u0000민감정보,\u0000고유식별정보,\u0000주민등록번호‣ 안전한관리\u0000․ 파기(§21):\u0000목적달성,\u0000기간경과등불필요시파기\u0000․ 안전조치(§29):\u0000내부관리계획수립,\u0000접속기록보관등기술적․관리적․물리적조치시행\u0000․ 개인정보영향평가(§33):\u0000위험요인분석및개선사항도출위한평가시행\u0000․ 노출된개인정보의삭제․차단(§34의2):\u0000고유식별정보,\u0000계좌정보,\u0000신용카드정보등삭제․차단‣ 정보주체권리\u0000․ 개인정보의열람(§35)\u0000/\u0000정정․삭제(§36)\u0000/\u0000처리정지등(§37)\u0000․ 자동화된결정에대한정보주체의권리(§37의2):\u0000AI\u0000등완전히자동화된시스템으로개인정보를처리하여권리․의무에중대한영향을미치는결정에대한거부권,\u0000설명요구권등"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료]\n\n- 3 -\n□ 국내 AI 개인정보 처리 및 보호 관련 안내서 안내서주요내용비정형데이터가명처리기준\u0000(’24.2)‣ 이미지·영상·음성등비정형데이터가명처리기준제시\u0000-\u0000의료(CT,\u0000MRI),\u0000교통,\u0000챗봇\u0000등분야별7종시나리오통해가명정보활용전과정\u0000상세안내AI\u0000개발․서비스위한공개된개인정보처리안내서\u0000(’24.7)‣ 인터넷등에공개된정보를AI\u0000학습에활용할수있는법적기준제시\u0000-\u0000개인정보보호법상‘정당한이익’\u0000조항(§15➀6호)의적용요건및기술적․관리적안전조치기준\u0000등제시개인영상정보보호․활용안내서\u0000(’24.10)‣ 자율주행차,\u0000로봇,\u0000드론\u0000등\u0000이동형영상정보처리기기촬영영상을AI\u0000개발\u0000등에활용할수있는기준제시\u0000-\u0000개인영상정보의촬영․이용․제공․보관․파기등처리단계별준수사항제시합성데이터생성․활용안내서\u0000(’24.12)‣ 합성데이터생성․활용단계별적법절차,\u0000원본데이터의전처리방식,\u0000안전성․유용성검증방법및지표등세부절차안내\u0000\u0000※합성데이터생성․검증등실증사례는「합성데이터생성참조모델」(’24.5.)에서확인/\u0000합성데이터셋은‘가명정보지원플랫폼(dataprivacy.go.kr)에서다운로드가능AI\u0000프라이버시리스크관리모델\u0000(’24.12)‣ AI\u0000유형․용례․맥락에따른\u0000AI\u0000프라이버시리스크경감방안안내\u0000-\u0000AI\u0000수명주기별\u0000프라이버시리스크를기업스스로평가․경감하도록지원□ 해외 AI 데이터 처리 관련 참고자료 참고자료주요내용\n미국NIST\u0000Privacy\u0000Framework\u0000ver1.1(‘25.4.)‣ 미국가기술표준연구소(NIST)\u0000발간개인정보보호및리스크관리체계로,\u0000조직의자율적인프라이버시리스크식별․평가․관리지원\u0000 \u0000※AI\u0000프라이버시리스크관리섹션을신설한개정안(ver.\u00001.1)\u0000공개(’25.4.)\u0000후의견수렴통해최종안공개예정(~’25.12.)\n영국UK\u0000AI\u0000Playbook(‘25.2.)‣ 공공기관이AI를안전하고책임감있게도입·활용할수있도록돕기위한실무지침으로,\u0000조달·설계·데이터관리·프라이버시보호등AI\u0000도입·운용의전과정에대한단계별가이드라인제공\nEUAI\u0000Privacy\u0000Risks\u0000&\u0000Mitigations(‘25.4.)‣\u0000개인정보보호이사회(EDPB)에서LLM의프라이버시리스크를식별·평가·완화하기위한종합적인리스크관리방법론\u0000제시\u0000-\u0000개인정보유․노출,\u0000불투명성,\u0000권리행사제약등프라이버시리스크에대응하기위한기술적․관리적조치,\u0000거버넌스체계마련\u0000등종합적․지속적리스크관리강조"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "교통", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 교통]\n\n- 3 -\n□ 국내 AI 개인정보 처리 및 보호 관련 안내서 안내서주요내용비정형데이터가명처리기준\u0000(’24.2)‣ 이미지·영상·음성등비정형데이터가명처리기준제시\u0000-\u0000의료(CT,\u0000MRI),\u0000교통,\u0000챗봇\u0000등분야별7종시나리오통해가명정보활용전과정\u0000상세안내AI\u0000개발․서비스위한공개된개인정보처리안내서\u0000(’24.7)‣ 인터넷등에공개된정보를AI\u0000학습에활용할수있는법적기준제시\u0000-\u0000개인정보보호법상‘정당한이익’\u0000조항(§15➀6호)의적용요건및기술적․관리적안전조치기준\u0000등제시개인영상정보보호․활용안내서\u0000(’24.10)‣ 자율주행차,\u0000로봇,\u0000드론\u0000등\u0000이동형영상정보처리기기촬영영상을AI\u0000개발\u0000등에활용할수있는기준제시\u0000-\u0000개인영상정보의촬영․이용․제공․보관․파기등처리단계별준수사항제시합성데이터생성․활용안내서\u0000(’24.12)‣ 합성데이터생성․활용단계별적법절차,\u0000원본데이터의전처리방식,\u0000안전성․유용성검증방법및지표등세부절차안내\u0000\u0000※합성데이터생성․검증등실증사례는「합성데이터생성참조모델」(’24.5.)에서확인/\u0000합성데이터셋은‘가명정보지원플랫폼(dataprivacy.go.kr)에서다운로드가능AI\u0000프라이버시리스크관리모델\u0000(’24.12)‣ AI\u0000유형․용례․맥락에따른\u0000AI\u0000프라이버시리스크경감방안안내\u0000-\u0000AI\u0000수명주기별\u0000프라이버시리스크를기업스스로평가․경감하도록지원□ 해외 AI 데이터 처리 관련 참고자료 참고자료주요내용\n미국NIST\u0000Privacy\u0000Framework\u0000ver1.1(‘25.4.)‣ 미국가기술표준연구소(NIST)\u0000발간개인정보보호및리스크관리체계로,\u0000조직의자율적인프라이버시리스크식별․평가․관리지원\u0000 \u0000※AI\u0000프라이버시리스크관리섹션을신설한개정안(ver.\u00001.1)\u0000공개(’25.4.)\u0000후의견수렴통해최종안공개예정(~’25.12.)\n영국UK\u0000AI\u0000Playbook(‘25.2.)‣ 공공기관이AI를안전하고책임감있게도입·활용할수있도록돕기위한실무지침으로,\u0000조달·설계·데이터관리·프라이버시보호등AI\u0000도입·운용의전과정에대한단계별가이드라인제공\nEUAI\u0000Privacy\u0000Risks\u0000&\u0000Mitigations(‘25.4.)‣\u0000개인정보보호이사회(EDPB)에서LLM의프라이버시리스크를식별·평가·완화하기위한종합적인리스크관리방법론\u0000제시\u0000-\u0000개인정보유․노출,\u0000불투명성,\u0000권리행사제약등프라이버시리스크에대응하기위한기술적․관리적조치,\u0000거버넌스체계마련\u0000등종합적․지속적리스크관리강조"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "국경", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 국경]\n\n- 4 -\n□ AI 개인정보 처리 관련 조사·처분 등 집행사례구분 주요내용조사․처분\nAI\u0000챗봇서비스조사결과(’21.4.)‣\u0000기업이특정목적으로수집한개인정보를이용자의명시적동의없이관련성없는신규서비스개발에이용한사안에서,\u0000-\u0000AI\u0000챗봇서비스의개인정보목적외이용\u0000등에대해시정명령및과징금․과태료\u0000부과출입국관리AI\u0000식별추적시스템조사결과(’22.4.)‣ 출입국심사과정에서수집한안면정보를AI\u0000기술개발에활용하는것의적법성판단\u0000관련,\u0000-\u0000안면정보를출입국관리시스템고도화를위해활용하는것은출입국관리법의목적인안전한국경관리를달성하기위한것으로개인정보수집\u0000목적범위내에포함된다고판단\n사전실태점검\n국내외주요AI\u0000서비스대상사전실태점검\u0000(’24.3.)‣\u0000주요LLM\u0000서비스(6개사업자)\u0000대상\u0000공개된개인정보처리,\u0000이용자입력데이터처리,\u0000투명성및정보주체권리보장\u0000관련개선권고AI\u0000응용서비스사전실태점검\u0000(’24.6.)‣ AI\u0000응용서비스(AI\u0000통화요약,\u0000이미지생성등4개서비스)의개인정보처리과정에대한실태점검\u0000후,\u0000일부사업자대상시스템접속기록보관․점검등안전조치준수시정권고\u0000및투명성강화관련개선권고딥시크서비스사전실태점검\u0000(’25.4.)‣ 개인정보국외이전근거충실구비및이미이전한개인정보즉각파기,\u0000한국어개인정보처리방침투명성제고\u0000등시정권고‣\u0000AI\u0000관련강화된보호조치준수,\u0000처리시스템전반점검및안전조치수준향상,\u0000국내대리인지정\u0000등개선권고AI\u0000디지털교과서사전실태점검\u0000(’25.5.)‣\u0000학습이력을데이터베이스화하여저장하고개인맞춤형콘텐츠를제공하는AI\u0000디지털교과서의개인정보보호실태점검결과,\u0000\u0000-\u0000개인정보처리의적법성·투명성확보,\u0000정보주체권리행사명확화등시정권고,\u0000안전조치관련개선권고\u0000등□ AI 개인정보 처리·보호 관련 분야 혁신지원제도 사례구분 주요내용규제샌드박스\n자율 주행AI\u0000학 습(’24.2)‣ 자율주행·로봇기업등이강화된안전조치준수下\u0000동의없이영상원본활용허용※뉴빌리티·우아한형제들·현대차등5개기업승인첨단바 이오국 제공동 연구(’24.6.)‣서울대병원·하버드·MIT\u0000등과첨단바이오분야국제공동연구에필요한가명데이터셋을환자동의없이활용허용보 이 스 피 싱예 방A I\u0000학 습(’24.10.)‣금감원·국과수보유‘그놈목소리’\u0000통화데이터2만5천건을강화된안전조치\u0000下\u0000보이스피싱예방AI\u0000학습에활용허용"}
{"meta": {"section_type": "guideline", "article_ref": "제15조", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료] [제15조]\n\n- 5 -\n사전적정성\u0000검토제\n의료기관내생성형AI\u0000기반보고서작성지원(’24.10.)\n‣\u0000의료기관내진료대화기반의료기록작성업무를생성형AI를활용하여자동화하는솔루션개발‣ 개인정보의적법한처리위탁요건충족위해➀진료대화데이터처리에관한사항을고지하고➁기업용(Enterprise API) 라이선스를 사용해 의료기관의 목적으로만 데이터를 처리하도록 조치\u0000 \u0000※개인정보보호위원회제2024-017-237호(비공개)\n대화형AI\u0000서비스개발(’25.3.)‣ 대화맥락과이용자감정을파악해최적화된답변을제시하기위해자체가드레일모델*(LLM)과외부생성형AI\u0000병행활용\u0000 \u0000*\u0000유해한표현,\u0000개인정보등탐지‣ ➀개인정보처리위탁요건준수➁이용자발화문에대한안전조치강화\u0000➂사후관리체계구축\u0000등의조건부과\u0000 \u0000※개인정보보호위원회제2025-004-016호(비공개)이용자대화데이터의AI\u0000모델학습(’25.5.)‣대규모언어모델기반대화형AI\u0000서비스를제공하는과정에서의이용자프롬프트입력의AI\u0000학습활용\u0000방안검토‣ 개인정보보호법상추가적이용조항(제15조제3항)\u0000요건검토\u0000 \u0000※개인정보보호위원회제2025­011­031호(비공개)\n보이스피싱의심번호DB\u0000구축·활용(’25.7.)\n‣전화수발신내역데이터를활용하여보이스피싱의심번호를예측하고이를금융사의이상거래탐지·차단에이용‣보이스피싱범죄자의통화패턴등을학습*한예측모델생성\u0000 \u0000*\u0000경찰청등에서공유받은보이스피싱신고번호의통화패턴(전화번호,\u0000발신일시,\u0000종료일시등통화내역데이터)을분석·학습‣ 개인정보보호법상추가적이용조항(제15조제3항)\u0000요건등검토\u0000 \u0000※개인정보보호위원회제2025­015­231~232호(비공개)"}
{"meta": {"section_type": "guideline", "article_ref": "제15조", "domain": "금융", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 금융] [제15조]\n\n- 5 -\n사전적정성\u0000검토제\n의료기관내생성형AI\u0000기반보고서작성지원(’24.10.)\n‣\u0000의료기관내진료대화기반의료기록작성업무를생성형AI를활용하여자동화하는솔루션개발‣ 개인정보의적법한처리위탁요건충족위해➀진료대화데이터처리에관한사항을고지하고➁기업용(Enterprise API) 라이선스를 사용해 의료기관의 목적으로만 데이터를 처리하도록 조치\u0000 \u0000※개인정보보호위원회제2024-017-237호(비공개)\n대화형AI\u0000서비스개발(’25.3.)‣ 대화맥락과이용자감정을파악해최적화된답변을제시하기위해자체가드레일모델*(LLM)과외부생성형AI\u0000병행활용\u0000 \u0000*\u0000유해한표현,\u0000개인정보등탐지‣ ➀개인정보처리위탁요건준수➁이용자발화문에대한안전조치강화\u0000➂사후관리체계구축\u0000등의조건부과\u0000 \u0000※개인정보보호위원회제2025-004-016호(비공개)이용자대화데이터의AI\u0000모델학습(’25.5.)‣대규모언어모델기반대화형AI\u0000서비스를제공하는과정에서의이용자프롬프트입력의AI\u0000학습활용\u0000방안검토‣ 개인정보보호법상추가적이용조항(제15조제3항)\u0000요건검토\u0000 \u0000※개인정보보호위원회제2025­011­031호(비공개)\n보이스피싱의심번호DB\u0000구축·활용(’25.7.)\n‣전화수발신내역데이터를활용하여보이스피싱의심번호를예측하고이를금융사의이상거래탐지·차단에이용‣보이스피싱범죄자의통화패턴등을학습*한예측모델생성\u0000 \u0000*\u0000경찰청등에서공유받은보이스피싱신고번호의통화패턴(전화번호,\u0000발신일시,\u0000종료일시등통화내역데이터)을분석·학습‣ 개인정보보호법상추가적이용조항(제15조제3항)\u0000요건등검토\u0000 \u0000※개인정보보호위원회제2025­015­231~232호(비공개)"}
{"meta": {"section_type": "guideline", "article_ref": "제15조", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료] [제15조]\n\n- 10 -\n공개된 개인정보의 수집·이용LLM등기초모델을개발하는경우에는대량의데이터를스크래핑방식으로직접수집하거나거대말뭉치9)등을사용하는것이현실적관행으로자리잡고있습니다.이러한기초모델이실제현실에관한광범위한지식과적응력을확보하기위해서공개된개인정보처리의필요성이인정될수있으나,무분별한스크래핑을통한권리침해우려도있으므로적법성․안전성확보에특히주의를기울여야합니다.특정서비스매개없이인터넷공간에서개인정보를수집하는경우,기업․기관과정보주체사이에직접적인관계가형성되지않아동의․계약과같은적법근거를적용하는것이어려울수있습니다.이경우실질적으로고려할수있는적법근거로는정당한이익조항(法제15조제1항제6호)이있으며,이를충족하기위해서는▲목적의정당성▲공개된개인정보처리의필요성▲이익형량의3가지기준을검토해야합니다.특히,이익형량단계에서정보주체권리침해가능성을최소화하기위한기술적․관리적안전조치및정보주체권리보장방안을마련하는것이핵심입니다. 9) 일반적으로 수십억 개 이상의 단어로 구성된 대규모 자연어 텍스트 데이터셋을 의미하며, 웹 문서, 뉴스, 논문 등 다양한 출처에서 수집됨. 대표적인 영어 기반 말뭉치로는 Common Crawl, Wikipedia, OpenWeb Text 등이 있고, 한국어 특화 말뭉치로는 AI Hub 말뭉치, 모두의 말뭉치 등이 있음\n참고｢공개된 개인정보 처리 안내서｣(’24.7) 주요내용▪ (목적의 정당성) 개인정보처리자의 정당한 이익의 존재  - AI 기업·기관의 영업상 이익뿐 아니라 그로부터 발생하는 사회적 이익 등 다양한 층위의 이익을 포괄  - 다양한 과제(downstream task) 수행이 가능한 생성형 AI의 개인정보 처리 목적을 특정하기 어려운 한계 → AI의 유형·기능·성능을 고려하여 정당한 이익 명확화▪ (처리의 필요성) 공개된 개인정보 처리의 필요성과 상당성·합리성이 인정될 것    ※ (예) 의료진단보조 AI 개발시 개인의 소득·재산 등 관련없는 정보는 학습 배제▪ (이익형량) 개인정보처리자의 정당한 이익이 정보주체 권리에 명백히 우선  - 명백성 요건 충족을 위해 ▲정보주체 권익침해 방지를 위한 안전성 확보조치 ▲정보주체 권리보장 방안 마련으로 개인정보처리자 이익이 우선하도록 조치"}
{"meta": {"section_type": "guideline", "article_ref": "제28조", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료] [제28조]\n\n- 13 -\n  □3 당초 수집 목적과 별개의 신규 서비스 개발이용자개인정보를당초수집한목적과별개의신규AI서비스개발에활용하는경우에는▲가명․익명처리(法제28조의2및제58조의2)하여이용10)하거나▲새로운적법근거(法제18조제2항)마련이필요합니다. 10) “가명정보 처리 가이드라인” (‘24. 2., 개인정보보호위원회) 참고\n법률 제28조의2(가명정보의 처리 등) ① 개인정보처리자는 통계작성, 과학적 연구, 공익적 기록보존 등을 위하여 정보주체의 동의 없이 가명정보를 처리할 수 있다.제58조의2(적용제외) 이 법은 시간ㆍ비용ㆍ기술 등을 합리적으로 고려할 때 다른 정보를 사용하여도 더 이상 개인을 알아볼 수 없는 정보에는 적용하지 아니한다.제18조(개인정보의 목적 외 이용ㆍ제공 제한) ② 제1항에도 불구하고 개인정보처리자는 다음 각 호의 어느 하나에 해당하는 경우에는 정보주체 또는 제3자의 이익을 부당하게 침해할 우려가 있을 때를 제외하고는 개인정보를 목적 외의 용도로 이용하거나 이를 제3자에게 제공할 수 있다. 다만, 제5호부터 제9호까지에 따른 경우는 공공기관의 경우로 한정한다.1. 정보주체로부터 별도의 동의를 받은 경우2. 다른 법률에 특별한 규정이 있는 경우3. 명백히 정보주체 또는 제3자의 급박한 생명, 신체, 재산의 이익을 위하여 필요하다고 인정되는 경우4. 삭제 <2020. 2. 4.>5. 개인정보를 목적 외의 용도로 이용하거나 이를 제3자에게 제공하지 아니하면 다른 법률에서 정하는 소관 업무를 수행할 수 없는 경우로서 보호위원회의 심의ㆍ의결을 거친 경우 (이하 생략)\n사례 2 이용자 개인정보의 AI 활용 사례  ※ 개인정보보호위원회 제2025–015–231~2호(비공개) 결정▪이용자 통화내역 데이터를 사용한 보이스피싱 의심번호 DB를 구축하는데 있어, ① 보이스피싱은 통신망을 악용한 금융사기로, 이용자 보호 의무 준수를 위해 이를 사전에 예방하고 차단하는 것은 정상적인 통신·금융 서비스 제공과 밀접하게 관련된 점,  ② 대다수 통신사·금융사는 기존에도 스팸·이상거래 방지 등 부정이용 및 이용자 보호 목적의 서비스를 운영 중이며, 이용약관 등을 통해 이를 안내하고 있어 고객보호 의무의 연장선으로 보이스피싱 예방·탐지 서비스에 대한 예측이 가능한 점, ③ 통신사가 예측한 의심번호의 정·오탐 여부를 금융사가 별도 검토하고 피드백하는 절차가 구축되어 있어 정보주체의 이익을 부당하게 침해하지 않는 점, ④ 보이스피싱 의심 DB 구축 및 통신사, 금융사 간 통신 시 전화번호가 암호화되어 저장 및 송·수신되는 점 등을 고려하면 개인정보의 추가적 이용을 근거로 통화내역 데이터를 이용해 보이스피싱 의심번호 DB 구축·이용 가능\n사례 1 질병 진단을 보조하는 의료 AI 연구개발을 위해 정보주체의 동의 없이 병원이 보유한 MRI, CT, X-Ray 사진 등을 가명처리하여 학습데이터로 이용"}
{"meta": {"section_type": "guideline", "article_ref": "제28조", "domain": "금융", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 금융] [제28조]\n\n- 13 -\n  □3 당초 수집 목적과 별개의 신규 서비스 개발이용자개인정보를당초수집한목적과별개의신규AI서비스개발에활용하는경우에는▲가명․익명처리(法제28조의2및제58조의2)하여이용10)하거나▲새로운적법근거(法제18조제2항)마련이필요합니다. 10) “가명정보 처리 가이드라인” (‘24. 2., 개인정보보호위원회) 참고\n법률 제28조의2(가명정보의 처리 등) ① 개인정보처리자는 통계작성, 과학적 연구, 공익적 기록보존 등을 위하여 정보주체의 동의 없이 가명정보를 처리할 수 있다.제58조의2(적용제외) 이 법은 시간ㆍ비용ㆍ기술 등을 합리적으로 고려할 때 다른 정보를 사용하여도 더 이상 개인을 알아볼 수 없는 정보에는 적용하지 아니한다.제18조(개인정보의 목적 외 이용ㆍ제공 제한) ② 제1항에도 불구하고 개인정보처리자는 다음 각 호의 어느 하나에 해당하는 경우에는 정보주체 또는 제3자의 이익을 부당하게 침해할 우려가 있을 때를 제외하고는 개인정보를 목적 외의 용도로 이용하거나 이를 제3자에게 제공할 수 있다. 다만, 제5호부터 제9호까지에 따른 경우는 공공기관의 경우로 한정한다.1. 정보주체로부터 별도의 동의를 받은 경우2. 다른 법률에 특별한 규정이 있는 경우3. 명백히 정보주체 또는 제3자의 급박한 생명, 신체, 재산의 이익을 위하여 필요하다고 인정되는 경우4. 삭제 <2020. 2. 4.>5. 개인정보를 목적 외의 용도로 이용하거나 이를 제3자에게 제공하지 아니하면 다른 법률에서 정하는 소관 업무를 수행할 수 없는 경우로서 보호위원회의 심의ㆍ의결을 거친 경우 (이하 생략)\n사례 2 이용자 개인정보의 AI 활용 사례  ※ 개인정보보호위원회 제2025–015–231~2호(비공개) 결정▪이용자 통화내역 데이터를 사용한 보이스피싱 의심번호 DB를 구축하는데 있어, ① 보이스피싱은 통신망을 악용한 금융사기로, 이용자 보호 의무 준수를 위해 이를 사전에 예방하고 차단하는 것은 정상적인 통신·금융 서비스 제공과 밀접하게 관련된 점,  ② 대다수 통신사·금융사는 기존에도 스팸·이상거래 방지 등 부정이용 및 이용자 보호 목적의 서비스를 운영 중이며, 이용약관 등을 통해 이를 안내하고 있어 고객보호 의무의 연장선으로 보이스피싱 예방·탐지 서비스에 대한 예측이 가능한 점, ③ 통신사가 예측한 의심번호의 정·오탐 여부를 금융사가 별도 검토하고 피드백하는 절차가 구축되어 있어 정보주체의 이익을 부당하게 침해하지 않는 점, ④ 보이스피싱 의심 DB 구축 및 통신사, 금융사 간 통신 시 전화번호가 암호화되어 저장 및 송·수신되는 점 등을 고려하면 개인정보의 추가적 이용을 근거로 통화내역 데이터를 이용해 보이스피싱 의심번호 DB 구축·이용 가능\n사례 1 질병 진단을 보조하는 의료 AI 연구개발을 위해 정보주체의 동의 없이 병원이 보유한 MRI, CT, X-Ray 사진 등을 가명처리하여 학습데이터로 이용"}
{"meta": {"section_type": "guideline", "article_ref": "제23조", "domain": "교통", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 교통] [제23조]\n\n- 14 -\n한편,가명․익명처리나새로운적법근거마련없이개인정보를당초수집범위를벗어나이용하는경우에는적법하지않은목적외이용으로판단될수있습니다.단,해당이용이혁신성,공익성등을갖춘경우에는규제샌드박스제도를활용해강화된안전조치등일정요건을전제로개인정보처리근거를확보할수있습니다.사례 3 서비스 품질 개선 목적으로 수집한 이용자 대화데이터를 합리적 관련성 없는 신규 서비스(인공지능 챗봇) 개발에 암호화 등의 안전조치 없이 이용한 사안에 대해 적법 처리 근거가 없어 이용자 개인정보의 목적 외 이용으로 판단   ※ 개인정보보호위원회 제2021-007-072호 결정 참고사례 4 자율주행 기업이 수집한 영상정보를 가명처리하여 AI 학습에 이용시 자율주행 AI의 성능 향상에 어려움이 있는 상황 → 규제실증특례를 활용해 강화된 안전조치 준수 下 동의 및 가명처리 없이 영상 원본 활용 허용  □4 특수한 개인정보의 처리AI서비스개발에민감정보,고유식별정보등의처리가수반되는경우에는별도의동의또는법적근거가있는경우에한하여처리할수있습니다.또한,개인영상정보를AI학습에이용하는경우에는설치․운영․촬영목적의범위내에서처리해야합니다.사례 1 정보주체의 동의를 받아 자유 발화를 통해 수집한 음성정보를 저장한 후, AI 음성인식기술 기반 목소리 인증 서비스(민감정보 처리, 法 제23조제1항제1호)에 활용할 수 있음사례 2 지방자치단체가 교통정보의 수집·분석(法 제25조제1항제5호) 등을 위하여 CCTV를 설치하고 해당 영상정보를 AI 기반 스마트교차로, 감응신호시스템에 이용 가능\n사례 2 금융당국, 수사기관 등이 보유한 보이스피싱 통화데이터를 가명처리해 통신사 등이 정보주체의 동의 없이 보이스피싱 예방 AI 기술·서비스 개발에 활용"}
{"meta": {"section_type": "guideline", "article_ref": "제23조", "domain": "금융", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 금융] [제23조]\n\n- 14 -\n한편,가명․익명처리나새로운적법근거마련없이개인정보를당초수집범위를벗어나이용하는경우에는적법하지않은목적외이용으로판단될수있습니다.단,해당이용이혁신성,공익성등을갖춘경우에는규제샌드박스제도를활용해강화된안전조치등일정요건을전제로개인정보처리근거를확보할수있습니다.사례 3 서비스 품질 개선 목적으로 수집한 이용자 대화데이터를 합리적 관련성 없는 신규 서비스(인공지능 챗봇) 개발에 암호화 등의 안전조치 없이 이용한 사안에 대해 적법 처리 근거가 없어 이용자 개인정보의 목적 외 이용으로 판단   ※ 개인정보보호위원회 제2021-007-072호 결정 참고사례 4 자율주행 기업이 수집한 영상정보를 가명처리하여 AI 학습에 이용시 자율주행 AI의 성능 향상에 어려움이 있는 상황 → 규제실증특례를 활용해 강화된 안전조치 준수 下 동의 및 가명처리 없이 영상 원본 활용 허용  □4 특수한 개인정보의 처리AI서비스개발에민감정보,고유식별정보등의처리가수반되는경우에는별도의동의또는법적근거가있는경우에한하여처리할수있습니다.또한,개인영상정보를AI학습에이용하는경우에는설치․운영․촬영목적의범위내에서처리해야합니다.사례 1 정보주체의 동의를 받아 자유 발화를 통해 수집한 음성정보를 저장한 후, AI 음성인식기술 기반 목소리 인증 서비스(민감정보 처리, 法 제23조제1항제1호)에 활용할 수 있음사례 2 지방자치단체가 교통정보의 수집·분석(法 제25조제1항제5호) 등을 위하여 CCTV를 설치하고 해당 영상정보를 AI 기반 스마트교차로, 감응신호시스템에 이용 가능\n사례 2 금융당국, 수사기관 등이 보유한 보이스피싱 통화데이터를 가명처리해 통신사 등이 정보주체의 동의 없이 보이스피싱 예방 AI 기술·서비스 개발에 활용"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료]\n\n- 19 -\n❏LLM개발․활용분류에따른프라이버시유의사항은?서비스형 LLM서비스형LLM기반의AI서비스를제공하는기업․기관의경우,▲이용자데이터의보관및재이용(AI학습포함)여부▲국외이전여부등을사전에검토할필요가있습니다.서비스형LLM은주로기관간계약에따라API16)를통해데이터를송수신하는방식으로운용되며,이과정에서이용자의개인정보가전송되거나저장되는경우에는,라이선스계약,이용약관등을통해해당데이터처리의안전성을확보하는것이바람직합니다.우선,서비스형LLM은개인정보저장및재이용(AI학습포함)배제규정을둔‘기업용API라이선스(EnterpriseAPI)’를제공하는경우가많습니다.기업․기관은이와같은보다높은수준의프라이버시보호를제공하는라이선스이용을고려할수있습니다.< 개인용 vs. 기업용 API 라이선스 비교17) >구 분개인용 라이선스기업용 API 라이선스이용대상일반 개인 사용자기업, 조직, 개발자 등데이터 저장대화기록 저장 가능(옵션 제공) 기본적으로 입출력값 미저장AI 학습 활용기본적으로 학습 활용(opt-out 가능)학습 미활용(기본값)계약 관계이용약관에 따른 일반 사용자 계약개별 계약(서비스 이용계약+데이터 처리 부속서(DPA)) 체결 필요\n사례 서비스형 LLM을 활용한 진료 대화 처리 시 기업용 API 라이선스 사용▪의료기관이 진료 대화를 기반으로 의료기록 작성업무를 자동화하는 과정에서 개인용 무료 라이선스로 서비스형 LLM을 사용하면 입력 데이터가 LLM 서비스 제공자의 자체 목적(예: LLM 학습)으로 활용될 우려가 있음▪기업용 라이선스(Enterprise API)로 서비스형 LLM을 사용해 의료기관의 목적으로만 데이터가 처리되도록 조치    ※ 개인정보보호위원회 제2024-017-237호(비공개) 결정16) API(Application Programming Interface): 일종의 소프트웨어 인터페이스로서 다른 종류의 소프트웨어에 서비스를 제공17) OpenAI ChatGPT 서비스 중심으로 정리(’25.8.3. 기준)"}
{"meta": {"section_type": "guideline", "article_ref": "제28조", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료] [제28조]\n\n- 20 -\n또한,기업․기관은서비스형LLM에적용되는기업용이용약관(EnterpriseTermsofUse)및데이터처리부속서(DPA,DataProcessingAddendum)등계약을통해▲입력데이터의소유권▲재이용제한▲안전조치▲데이터파기등의사항을명확히규정하고,필요시서비스특성을고려한특약을추가하여데이터보호요구사항을계약상확보하는것이권장됩니다.< 기업용 이용약관 및 DPA 주요내용 비교18) >구 분A사 B사 C사데이터 소유권기업·기관(고객) 소유AI 학습 활용재이용·학습 금지안전조치TLS 암호화, 접근통제, 로그관리 등위수탁 관계 명시고객=처리자 / A·B·C사=수탁자 명시재위탁 제한재위탁시 고객에 사전 통보 + 이의제기 가능DPA 내 재위탁 관련 통제 조항 포함데이터 파기고객의 삭제 요청시 합리적 기간 내 파기관리·감독고객이 제3자 또는 내부 감사 수행 가능(10일전 요청)관리자용 감사 로그, 접근 기록 확인 가능연1회 감사 가능 및 직원 접근 기록 및 보안사고 통지추가로,LLM서버가해외에소재한다면,개인정보국외이전에해당하는지검토하고관련규율을확인할필요가있습니다.법률 제28조의8(개인정보의 국외 이전) ① 개인정보처리자는 개인정보를 국외로 제공(조회되는 경우를 포함한다)ㆍ처리위탁ㆍ보관(이하 이 절에서 “이전”이라 한다)하여서는 아니 된다. 다만, 다음 각 호의 어느 하나에 해당하는 경우에는 개인정보를 국외로 이전할 수 있다. 1. 정보주체로부터 국외 이전에 관한 별도의 동의를 받은 경우... 3. 정보주체와의 계약의 체결 및 이행을 위하여 개인정보의 처리위탁ㆍ보관이 필요한 경우로서 다음 각 목의 어느 하나에 해당하는 경우     가. 제2항 각 호의 사항을 제30조에 따른 개인정보 처리방침에 공개한 경우     나. 전자우편 등 대통령령으로 정하는 방법에 따라 제2항 각 호의 사항을 정보주체에게 알린 경우\n사례 서비스형 LLM 활용에서의 통화 내용 개인정보 국외이전 사례▪ 이용자에게 통화 녹음·요약 서비스를 제공하기 위해 서비스형 LLM을 사용하는 과정에서 통화 내용 등 개인정보가 국외 서버로 이전되었음▪ 그러나, 국외이전에 관한 고지사항이 개인정보 처리방침에서 일부 누락되는 등 法 제28조의8제1항제3호에 따른 적법근거를 갖추지 못함    ※ 개인정보보호위원회 제2024-010-184호(비공개) 결정(주요 AI 서비스 사전실태 점검 결과)18) 주요 서비스형 LLM의 엔터프라이즈 라이선스, 이용약관, DPA 등 바탕으로 재정리(‘25. 8월 기준)\n- 21 -\n기성 LLM 활용공개모델(open-weightmodel)에해당하는사전학습모델(‘기성LLM’)을이용하여AI시스템을개발하는기업․기관은공개모델을허깅페이스(HuggingFace)등모델저장소(repository)에서다운로드받아자체인프라또는제3자클라우드환경에업로드하여호스팅하게됩니다.이때,기업․기관은기성LLM의초기학습에사용된데이터셋의출처를검증하기위한노력을기울여야합니다.출처가불명확한학습데이터에는위법하거나정보주체의의사와무관하게공개된개인정보가포함될수있어각별한주의가요구됩니다19).따라서학습데이터의출처․이력을확인할수있는모델을우선적으로활용하는것이바람직하며,출처검증에한계가있는경우에는후속단계에서다양한기술적․관리적안전조치를적용함으로써잔여리스크를경감하는것이권장됩니다.아울러,모델카드,기술문서,라이선스정책등을통해기성LLM자체에어떤안전조치가내장되어있는지확인하고,레드팀테스트20)등을거쳐추가보완조치를적용할것이권장됩니다.또한,기성LLM의원개발자가모델배포이후발견된리스크를공지할경우,이를신속히반영하여리스크관리체계를보완하고모델의최신버전및패치를주기적으로적용함으로써시스템의안전성과신뢰성을확보하는것이중요합니다."}
{"meta": {"section_type": "guideline", "article_ref": "제28조", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용] [제28조]\n\n- 20 -\n또한,기업․기관은서비스형LLM에적용되는기업용이용약관(EnterpriseTermsofUse)및데이터처리부속서(DPA,DataProcessingAddendum)등계약을통해▲입력데이터의소유권▲재이용제한▲안전조치▲데이터파기등의사항을명확히규정하고,필요시서비스특성을고려한특약을추가하여데이터보호요구사항을계약상확보하는것이권장됩니다.< 기업용 이용약관 및 DPA 주요내용 비교18) >구 분A사 B사 C사데이터 소유권기업·기관(고객) 소유AI 학습 활용재이용·학습 금지안전조치TLS 암호화, 접근통제, 로그관리 등위수탁 관계 명시고객=처리자 / A·B·C사=수탁자 명시재위탁 제한재위탁시 고객에 사전 통보 + 이의제기 가능DPA 내 재위탁 관련 통제 조항 포함데이터 파기고객의 삭제 요청시 합리적 기간 내 파기관리·감독고객이 제3자 또는 내부 감사 수행 가능(10일전 요청)관리자용 감사 로그, 접근 기록 확인 가능연1회 감사 가능 및 직원 접근 기록 및 보안사고 통지추가로,LLM서버가해외에소재한다면,개인정보국외이전에해당하는지검토하고관련규율을확인할필요가있습니다.법률 제28조의8(개인정보의 국외 이전) ① 개인정보처리자는 개인정보를 국외로 제공(조회되는 경우를 포함한다)ㆍ처리위탁ㆍ보관(이하 이 절에서 “이전”이라 한다)하여서는 아니 된다. 다만, 다음 각 호의 어느 하나에 해당하는 경우에는 개인정보를 국외로 이전할 수 있다. 1. 정보주체로부터 국외 이전에 관한 별도의 동의를 받은 경우... 3. 정보주체와의 계약의 체결 및 이행을 위하여 개인정보의 처리위탁ㆍ보관이 필요한 경우로서 다음 각 목의 어느 하나에 해당하는 경우     가. 제2항 각 호의 사항을 제30조에 따른 개인정보 처리방침에 공개한 경우     나. 전자우편 등 대통령령으로 정하는 방법에 따라 제2항 각 호의 사항을 정보주체에게 알린 경우\n사례 서비스형 LLM 활용에서의 통화 내용 개인정보 국외이전 사례▪ 이용자에게 통화 녹음·요약 서비스를 제공하기 위해 서비스형 LLM을 사용하는 과정에서 통화 내용 등 개인정보가 국외 서버로 이전되었음▪ 그러나, 국외이전에 관한 고지사항이 개인정보 처리방침에서 일부 누락되는 등 法 제28조의8제1항제3호에 따른 적법근거를 갖추지 못함    ※ 개인정보보호위원회 제2024-010-184호(비공개) 결정(주요 AI 서비스 사전실태 점검 결과)18) 주요 서비스형 LLM의 엔터프라이즈 라이선스, 이용약관, DPA 등 바탕으로 재정리(‘25. 8월 기준)\n- 21 -\n기성 LLM 활용공개모델(open-weightmodel)에해당하는사전학습모델(‘기성LLM’)을이용하여AI시스템을개발하는기업․기관은공개모델을허깅페이스(HuggingFace)등모델저장소(repository)에서다운로드받아자체인프라또는제3자클라우드환경에업로드하여호스팅하게됩니다.이때,기업․기관은기성LLM의초기학습에사용된데이터셋의출처를검증하기위한노력을기울여야합니다.출처가불명확한학습데이터에는위법하거나정보주체의의사와무관하게공개된개인정보가포함될수있어각별한주의가요구됩니다19).따라서학습데이터의출처․이력을확인할수있는모델을우선적으로활용하는것이바람직하며,출처검증에한계가있는경우에는후속단계에서다양한기술적․관리적안전조치를적용함으로써잔여리스크를경감하는것이권장됩니다.아울러,모델카드,기술문서,라이선스정책등을통해기성LLM자체에어떤안전조치가내장되어있는지확인하고,레드팀테스트20)등을거쳐추가보완조치를적용할것이권장됩니다.또한,기성LLM의원개발자가모델배포이후발견된리스크를공지할경우,이를신속히반영하여리스크관리체계를보완하고모델의최신버전및패치를주기적으로적용함으로써시스템의안전성과신뢰성을확보하는것이중요합니다."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료]\n\n19) AI 학습에 자주 활용되는 LAION 이미지 데이터셋에서 최소 1,000장의 아동 성착취 이미지 발견(’23.12.)20) 본 안내서 제3장 제5절 “AI 프라이버시 거버넌스 체계” 참고\n참고기성 LLM 관련 역할분담 예시 ※ 「AI 프라이버시 리스크 관리 모델」(’24.12.) 50쪽 참고‣ (모델개발자) 학습단계에서 인지하지 못했던 리스크를 모델 출시 이후 인식할 경우, 모델 배포 플랫폼(허깅페이스 등)을 통해 해당 리스크를 공지하고, 기술적·경제적으로 합리적인 기간 내에 모델을 업데이트하여 재배포, 이전 모델 비활성화 등 보완 조치  - 프라이버시를 고려한 이용방법, 조건 등을 명시한 라이선스 약관을 수립·배포‣ (모델이용자) 모델카드 등을 통해 모델 개발자가 적용한 리스크 경감조치 등을 검토하는 등 안전성이 확보된 범용모델을 활용하기 위해 노력\n- 22 -\n자체 개발LLM시스템을자체개발하는기업․기관은AI수명주기모든과정을전적으로책임지게됩니다.이방식은대규모학습데이터기반의사전학습(pre-train),미세조정,배포및운영,사후관리에이르는모든단계에서개인정보리스크요인을파악하고,이를경감하기위한조치를취할것을요구합니다.참고LLM 자체 개발시 참고 가능한 안내서▪ 공개된 개인정보 처리 안내서(‘24. 7)  - 대규모 학습데이터 수집·이용의 적법기준으로서 ‘정당한 이익’ 조항(§15①6)의 적용 기준 제시(▲목적의 정당성 ▲처리의 필요성 ▲이익형량)  - 이익형량 기준 충족 위한 기술적·관리적 안전조치 및 정보주체 권리보장 방안 안내▪AI 프라이버시 리스크 관리 모델(‘24."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용]\n\n19) AI 학습에 자주 활용되는 LAION 이미지 데이터셋에서 최소 1,000장의 아동 성착취 이미지 발견(’23.12.)20) 본 안내서 제3장 제5절 “AI 프라이버시 거버넌스 체계” 참고\n참고기성 LLM 관련 역할분담 예시 ※ 「AI 프라이버시 리스크 관리 모델」(’24.12.) 50쪽 참고‣ (모델개발자) 학습단계에서 인지하지 못했던 리스크를 모델 출시 이후 인식할 경우, 모델 배포 플랫폼(허깅페이스 등)을 통해 해당 리스크를 공지하고, 기술적·경제적으로 합리적인 기간 내에 모델을 업데이트하여 재배포, 이전 모델 비활성화 등 보완 조치  - 프라이버시를 고려한 이용방법, 조건 등을 명시한 라이선스 약관을 수립·배포‣ (모델이용자) 모델카드 등을 통해 모델 개발자가 적용한 리스크 경감조치 등을 검토하는 등 안전성이 확보된 범용모델을 활용하기 위해 노력\n- 22 -\n자체 개발LLM시스템을자체개발하는기업․기관은AI수명주기모든과정을전적으로책임지게됩니다.이방식은대규모학습데이터기반의사전학습(pre-train),미세조정,배포및운영,사후관리에이르는모든단계에서개인정보리스크요인을파악하고,이를경감하기위한조치를취할것을요구합니다.참고LLM 자체 개발시 참고 가능한 안내서▪ 공개된 개인정보 처리 안내서(‘24. 7)  - 대규모 학습데이터 수집·이용의 적법기준으로서 ‘정당한 이익’ 조항(§15①6)의 적용 기준 제시(▲목적의 정당성 ▲처리의 필요성 ▲이익형량)  - 이익형량 기준 충족 위한 기술적·관리적 안전조치 및 정보주체 권리보장 방안 안내▪AI 프라이버시 리스크 관리 모델(‘24."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료]\n\n12.)  - AI 모델 사전학습 및 추가학습, AI 시스템 개발 및 제공 등 AI 전 주기를 망라하는 리스크 관리 체계 안내  ※  (리스크 유형) AI 가치망의 데이터 흐름 및 정보주체 권리보장 책임 복잡화(리스크 경감방안) AI 가치망 참여자 간 역할 명확화, 허용되는 이용방침 작성·공개참고SLM 자체개발 및 AI 에이전트에서의 SLM 활용 체계 예시21)▪(SLM 자체개발) 소형언어모델(SLM, Small Language Model)22)은 대형언어모델(LLM)에 비해 더 한정·집중적인 작업을 처리하도록 설계된 경량 모델▪ (SLM 활용) AI 에이전트23) 개발 시 LLM의 한계를 보완하기 위해 SLM 병행 활용 가능   - (효율성) 광범위한 언어 능력이 아닌 도메인 특화 작업을 처리하는 경우 SLM 활용  - (프라이버시) AI 에이전트에서 SLM은 중앙집중식 LLM 비중을 낮추고 데이터를 분야·영역별로 처리하여 개인정보 보호를 강화하는데 활용 가능▪ (모델 오케스트레이션24)) AI 에이전트에서 SLM과 LLM의 장점을 살리기 위해서 모델별 처리할 작업을 동적으로 배분·관리하는 체계  - 처리할 작업에 가장 적합한 모델(LLM 또는 SLM)을 결정하고, 프롬프트 등 입력을 선택된 모델에 전달하며, 모델의 결과값을 받아 통합해 최종적인 결과를 도출21) \"AI Privacy Risks & Mitigations - Large Language Models\" (EDPB), “Emerging LLM Technologies: The Rise of Agentic AI” 절 참고22) Cabalar, R., ‘What are small language models?’ (2024) https://www.ibm.com/think/topics/small-language-models\n  - 미세조정 등을 위해 추가로 투입한 데이터에 대해 리스크를 관리하고, 서비스의 의도된 용례 등에 따라 리스크를 경감  - 모델 개발자가 배포 이후 발견된 리스크를 공지할 경우, 추가적인 경감조치를 검토·시행하고 모델 버전의 최신 업데이트를 유지\n- 23 -\n3 AI 학습 및 개발❏AI학습․개발은왜중요한가요?생성형AI의학습․개발단계에서는사전에설정한목적을효과적으로달성하고,그과정에서의도치않은리스크를완화하기위한데이터전처리,모델에대한미세조정(fine-tuning),정렬(alignment)등의활동이수행됩니다25).생성형AI는학습데이터에포함된정보를‘암기’(memorization)하고일종의‘영구기억’(memory)형태로내재화하는기술적특성이있어,원본정보가출력결과에그대로노출되거나민감정보추론목적으로운용되는등정보주체의권익이침해될가능성이존재합니다.따라서생성형AI의학습․개발단계에서부터데이터․모델․시스템수준에서의프라이버시안전조치를강화할필요가있습니다26). 23) 목표 달성을 위해 스스로 문제를 분석하여 필요한 작업을 자율적으로 결정하고, 직접 수행할 수 있는 AI 시스템24) Windland, V. et al."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용]\n\n12.)  - AI 모델 사전학습 및 추가학습, AI 시스템 개발 및 제공 등 AI 전 주기를 망라하는 리스크 관리 체계 안내  ※  (리스크 유형) AI 가치망의 데이터 흐름 및 정보주체 권리보장 책임 복잡화(리스크 경감방안) AI 가치망 참여자 간 역할 명확화, 허용되는 이용방침 작성·공개참고SLM 자체개발 및 AI 에이전트에서의 SLM 활용 체계 예시21)▪(SLM 자체개발) 소형언어모델(SLM, Small Language Model)22)은 대형언어모델(LLM)에 비해 더 한정·집중적인 작업을 처리하도록 설계된 경량 모델▪ (SLM 활용) AI 에이전트23) 개발 시 LLM의 한계를 보완하기 위해 SLM 병행 활용 가능   - (효율성) 광범위한 언어 능력이 아닌 도메인 특화 작업을 처리하는 경우 SLM 활용  - (프라이버시) AI 에이전트에서 SLM은 중앙집중식 LLM 비중을 낮추고 데이터를 분야·영역별로 처리하여 개인정보 보호를 강화하는데 활용 가능▪ (모델 오케스트레이션24)) AI 에이전트에서 SLM과 LLM의 장점을 살리기 위해서 모델별 처리할 작업을 동적으로 배분·관리하는 체계  - 처리할 작업에 가장 적합한 모델(LLM 또는 SLM)을 결정하고, 프롬프트 등 입력을 선택된 모델에 전달하며, 모델의 결과값을 받아 통합해 최종적인 결과를 도출21) \"AI Privacy Risks & Mitigations - Large Language Models\" (EDPB), “Emerging LLM Technologies: The Rise of Agentic AI” 절 참고22) Cabalar, R., ‘What are small language models?’ (2024) https://www.ibm.com/think/topics/small-language-models\n  - 미세조정 등을 위해 추가로 투입한 데이터에 대해 리스크를 관리하고, 서비스의 의도된 용례 등에 따라 리스크를 경감  - 모델 개발자가 배포 이후 발견된 리스크를 공지할 경우, 추가적인 경감조치를 검토·시행하고 모델 버전의 최신 업데이트를 유지\n- 23 -\n3 AI 학습 및 개발❏AI학습․개발은왜중요한가요?생성형AI의학습․개발단계에서는사전에설정한목적을효과적으로달성하고,그과정에서의도치않은리스크를완화하기위한데이터전처리,모델에대한미세조정(fine-tuning),정렬(alignment)등의활동이수행됩니다25).생성형AI는학습데이터에포함된정보를‘암기’(memorization)하고일종의‘영구기억’(memory)형태로내재화하는기술적특성이있어,원본정보가출력결과에그대로노출되거나민감정보추론목적으로운용되는등정보주체의권익이침해될가능성이존재합니다.따라서생성형AI의학습․개발단계에서부터데이터․모델․시스템수준에서의프라이버시안전조치를강화할필요가있습니다26). 23) 목표 달성을 위해 스스로 문제를 분석하여 필요한 작업을 자율적으로 결정하고, 직접 수행할 수 있는 AI 시스템24) Windland, V. et al."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료]\n\n’What is LLM orchestration’ (2024) https://www.ibm.com/think/topics/llm-orchestration25) 미세조정(fine-tuning) 및 정렬(alignment)은 생성형 AI의 성능을 개선하기 위한 기법이나, 미세조정은 모델을 특정 목적이나 도메인에 맞게 학습시키는데 초점이 있고, 정렬은 그 모델이 인간의 가치와 사회적 기준에 부합하도록 출력 행동을 다듬는 기법으로 모델 정렬을 위해 미세조정이 선행되거나 병행될 수 있음26) GDS, ｢AI Playbook for the UK Government｣, “Principle 3: You know how to use AI securely\" 및 ”Principle 5: You understand how to manage the AI life cycle\" 참고\n참고LLM의 개인정보 암기·저장 관련 국제 논의▪ (독일 함부르크 개인정보 감독기구(HmbBfDI) 의견서(’24.7.)) LLM에는 개인정보가 저장되지 않기 때문에 LLM을 단순히 저장하는 것은 GDPR에 따른 개인정보 처리에 해당하지 않음. 단, LLM 등으로 구성된 AI 시스템이 쿼리나 출력 등을 통해 개인정보를 처리하는 경우, 해당 처리는 GDPR의 요구사항을 준수해야 함▪ (유럽데이터보호위원회(EDPB) 의견서(Opinion 28/2024)(’24.12.)) 개인정보를 학습한 AI 모델의 익명성 여부는 사안별(case-by-case) 판단이 필요하며, 합리적으로 가능한 모든 수단으로 직·간접적으로 추출하는 개인정보를 획득할 가능성이 무시가능한(insignificant) 수준이 아니면 익명성이 있다고 보기 어렵다는 의견   ※ 따라서 오픈소스 모델 등을 활용해서 AI를 개발할 경우, 활용자가 별도 익명처리를 적용하지 않는 한 GDPR 의무 준수를 위해 모델의 적법성 여부를 확인하는 평가 수행할 것을 요구하며 AI 생태계에 포함된 모든 행위자에 책임이 있다는 의견▪ (미국 캘리포니아 개인정보 보호법(CCPA) 개정(AB-1008)(’25.1.1. 시행)) LLM이 특정 개인에 관한 정보를 생성하거나 그 생성 결과가 개인 식별로 이어질 수 있는 경우에는 해당 모델 자체를 개인정보로 간주할 수 있도록 규정\n- 24 -\n❏프라이버시관점에서고려할내용은무엇인가요?1. 데이터 수준데이터는AI모델의성능과신뢰도에결정적인영향을미치므로,데이터를악의적으로손상시키는데이터오염(datapoisoning),데이터편향성․부정확성등의문제에체계적으로대응할필요가있습니다27).공개된데이터를학습용도로수집할때는,명시적인스크래핑거부의사를표시한콘텐츠는제외하는것이바람직합니다.▲이용약관등에학습배제가명시된경우▲로봇배제표준(robots.txt)28)적용된경우,▲캡차(CAPTCHA)29)와같은기술적차단조치가적용된경우등이이에해당합니다30).참고웹 콘텐츠 제공자가 적용할 수 있는 AI 학습 및 스크래핑 방지 방안31)▪ IP 주소 기반 차단 정책 운영  - HTTP 헤더, 요청 횟수 등을 분석해 스크래핑 봇으로 판단되면 차단▪ 동적 페이지 로딩 기법 등을 적용해 HTML 콘텐츠 스크래핑 방지▪ 메타 태그(meta tag)로 학습 배제 표기  ※ \"noai\" 혹은 “noimageai\" 메타 태그 등\n27) Joint Cybersecurity Information Sheet (CSI), ｢AI Data Security: Best Practices for Securing Data Used to Train & Operate AI Systems｣ (’25.5.) 참고28) ｢공개된 개인정보 처리 안내서｣(‘24. 7.), “Ⅲ."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용]\n\n’What is LLM orchestration’ (2024) https://www.ibm.com/think/topics/llm-orchestration25) 미세조정(fine-tuning) 및 정렬(alignment)은 생성형 AI의 성능을 개선하기 위한 기법이나, 미세조정은 모델을 특정 목적이나 도메인에 맞게 학습시키는데 초점이 있고, 정렬은 그 모델이 인간의 가치와 사회적 기준에 부합하도록 출력 행동을 다듬는 기법으로 모델 정렬을 위해 미세조정이 선행되거나 병행될 수 있음26) GDS, ｢AI Playbook for the UK Government｣, “Principle 3: You know how to use AI securely\" 및 ”Principle 5: You understand how to manage the AI life cycle\" 참고\n참고LLM의 개인정보 암기·저장 관련 국제 논의▪ (독일 함부르크 개인정보 감독기구(HmbBfDI) 의견서(’24.7.)) LLM에는 개인정보가 저장되지 않기 때문에 LLM을 단순히 저장하는 것은 GDPR에 따른 개인정보 처리에 해당하지 않음. 단, LLM 등으로 구성된 AI 시스템이 쿼리나 출력 등을 통해 개인정보를 처리하는 경우, 해당 처리는 GDPR의 요구사항을 준수해야 함▪ (유럽데이터보호위원회(EDPB) 의견서(Opinion 28/2024)(’24.12.)) 개인정보를 학습한 AI 모델의 익명성 여부는 사안별(case-by-case) 판단이 필요하며, 합리적으로 가능한 모든 수단으로 직·간접적으로 추출하는 개인정보를 획득할 가능성이 무시가능한(insignificant) 수준이 아니면 익명성이 있다고 보기 어렵다는 의견   ※ 따라서 오픈소스 모델 등을 활용해서 AI를 개발할 경우, 활용자가 별도 익명처리를 적용하지 않는 한 GDPR 의무 준수를 위해 모델의 적법성 여부를 확인하는 평가 수행할 것을 요구하며 AI 생태계에 포함된 모든 행위자에 책임이 있다는 의견▪ (미국 캘리포니아 개인정보 보호법(CCPA) 개정(AB-1008)(’25.1.1. 시행)) LLM이 특정 개인에 관한 정보를 생성하거나 그 생성 결과가 개인 식별로 이어질 수 있는 경우에는 해당 모델 자체를 개인정보로 간주할 수 있도록 규정\n- 24 -\n❏프라이버시관점에서고려할내용은무엇인가요?1. 데이터 수준데이터는AI모델의성능과신뢰도에결정적인영향을미치므로,데이터를악의적으로손상시키는데이터오염(datapoisoning),데이터편향성․부정확성등의문제에체계적으로대응할필요가있습니다27).공개된데이터를학습용도로수집할때는,명시적인스크래핑거부의사를표시한콘텐츠는제외하는것이바람직합니다.▲이용약관등에학습배제가명시된경우▲로봇배제표준(robots.txt)28)적용된경우,▲캡차(CAPTCHA)29)와같은기술적차단조치가적용된경우등이이에해당합니다30).참고웹 콘텐츠 제공자가 적용할 수 있는 AI 학습 및 스크래핑 방지 방안31)▪ IP 주소 기반 차단 정책 운영  - HTTP 헤더, 요청 횟수 등을 분석해 스크래핑 봇으로 판단되면 차단▪ 동적 페이지 로딩 기법 등을 적용해 HTML 콘텐츠 스크래핑 방지▪ 메타 태그(meta tag)로 학습 배제 표기  ※ \"noai\" 혹은 “noimageai\" 메타 태그 등\n27) Joint Cybersecurity Information Sheet (CSI), ｢AI Data Security: Best Practices for Securing Data Used to Train & Operate AI Systems｣ (’25.5.) 참고28) ｢공개된 개인정보 처리 안내서｣(‘24. 7.), “Ⅲ."}
{"meta": {"section_type": "guideline", "article_ref": "제34조", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료] [제34조]\n\n안전성 확보 조치 기준” 참고29) CAPTCHA(Completely Automated Public Turing test to tell Computers and Humans Apart): 웹사이트에 접근하는 클라이언트가 사람인지 아니면 컴퓨터 봇(bot)인지 판단하기 위하여 사용하는 테스트 기법으로 표시된 텍스트나 숫자를 입력하는 것 혹은 설명에 부합하는 이미지를 선택하는 등 다양한 방식 존재30) CNIL, \"La base légale de l’intérêt légitime : fiche focus sur les mesures à prendre en cas de collecte des données par moissonnage (web scraping)\", ('25. 6.), “Les mesures obligatoires\" 및 \"Respecter les attentes raisonnables\" 참고. 특히 CNIL은 기업·기관이 robots.txt 혹은 CAPTCHA로 데이터 스크래핑에 명확한 거부 의사를 표시한 콘텐츠를 학습하면 정보주체의 합리적인 예견가능성을 인정하기 어렵다는 입장으로 ‘정당한 이익’ 인정을 위한 요건 중 하나인 ‘이익형량’ 판단에 영향이 있을 수 있음31) 위 CNIL 문서의 “Comment les éditeurs de site peuvent-ils protéger leur contenu du moissonnage?” 참고\n▪ (연구동향) LLM이 개인정보를 포함한 데이터를 손실 없이 압축·복원하는 고성능 무손실 압축기(lossless compressor)로 기능할 수 있다는 연구   ※ Delétang, Grégoire, et al. Language Modeling Is Compression. International Conference on Learning Representations (ICLR), 2024. arXiv:2309.10668. https://arxiv.org/abs/2309.10668\n- 25 -\n학습데이터의전처리는개인정보의유․노출등리스크를방지하기위한출발점이됩니다.이를위해생성형AI의의도된용도․성능을고려할때,학습데이터를가명․익명처리하여도목적달성이충분한경우에는수집직후가명․익명처리하는것이권장됩니다.참고비정형데이터 가명처리 기술 및 예시 ※ ｢가명정보 처리 가이드라인｣ (’24.2.)규칙, 정규표현식 등을 통한 개인정보 검출 및 마스킹은 정확도 측면에서 한계가 있을 수 있으며, 이를 보완하기 위해 LLM 모델을 통해 사전에 정의되지 않은 패턴의 개인정보를 검출하고 마스킹할 수 있음학습방법에 따라 다양한 형태의 AI 기반 개인 정 보  검 출  기법  존 재(HMM, MEM, CRFs 등)\nAI 기반 텍스트정보 가명처리영상정보 가명처리 예시(이미지 필터링 기술 적용)특히,개인식별위험이크고범죄등에악용될경우국민에게큰피해를야기할수있어개인정보보호법에서특별히보호하고있는주민등록번호와그밖의고유식별정보,계좌번호,신용카드번호등은AI학습전삭제하거나가명․익명화해야합니다. 참고주요 AI 서비스 사전 실태점검 결과(’24.3.)▪ 사전 학습단계(pre-training)에서 한국 정보주체를 식별할 위험이 크고 유·노출 시 2차 피해를 야기할 우려가 큰 정보 항목(주민번호 등 고유식별정보, 계좌정보, 신용카드정보, 휴대전화번호 등) 제거 권고▪ 개인정보위-한국인터넷진흥원(KISA)에서 탐지한 한국 정보주체의 개인정보 노출 페이지(URL)를 AI 서비스 제공사업자에 제공(신청 기반)\n법률 제34조의2(노출된 개인정보의 삭제ㆍ차단) ① 개인정보처리자는 고유식별정보, 계좌정보, 신용카드정보 등 개인정보가 정보통신망을 통하여 공중(公衆)에 노출되지 아니하도록 하여야 한다. ② 개인정보처리자는 공중에 노출된 개인정보에 대하여 보호위원회 또는 대통령령으로 지정한 전문기관의 요청이 있는 경우에는 해당 정보를 삭제하거나 차단하는 등 필요한 조치를 하여야 한다."}
{"meta": {"section_type": "guideline", "article_ref": "제34조", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용] [제34조]\n\n안전성 확보 조치 기준” 참고29) CAPTCHA(Completely Automated Public Turing test to tell Computers and Humans Apart): 웹사이트에 접근하는 클라이언트가 사람인지 아니면 컴퓨터 봇(bot)인지 판단하기 위하여 사용하는 테스트 기법으로 표시된 텍스트나 숫자를 입력하는 것 혹은 설명에 부합하는 이미지를 선택하는 등 다양한 방식 존재30) CNIL, \"La base légale de l’intérêt légitime : fiche focus sur les mesures à prendre en cas de collecte des données par moissonnage (web scraping)\", ('25. 6.), “Les mesures obligatoires\" 및 \"Respecter les attentes raisonnables\" 참고. 특히 CNIL은 기업·기관이 robots.txt 혹은 CAPTCHA로 데이터 스크래핑에 명확한 거부 의사를 표시한 콘텐츠를 학습하면 정보주체의 합리적인 예견가능성을 인정하기 어렵다는 입장으로 ‘정당한 이익’ 인정을 위한 요건 중 하나인 ‘이익형량’ 판단에 영향이 있을 수 있음31) 위 CNIL 문서의 “Comment les éditeurs de site peuvent-ils protéger leur contenu du moissonnage?” 참고\n▪ (연구동향) LLM이 개인정보를 포함한 데이터를 손실 없이 압축·복원하는 고성능 무손실 압축기(lossless compressor)로 기능할 수 있다는 연구   ※ Delétang, Grégoire, et al. Language Modeling Is Compression. International Conference on Learning Representations (ICLR), 2024. arXiv:2309.10668. https://arxiv.org/abs/2309.10668\n- 25 -\n학습데이터의전처리는개인정보의유․노출등리스크를방지하기위한출발점이됩니다.이를위해생성형AI의의도된용도․성능을고려할때,학습데이터를가명․익명처리하여도목적달성이충분한경우에는수집직후가명․익명처리하는것이권장됩니다.참고비정형데이터 가명처리 기술 및 예시 ※ ｢가명정보 처리 가이드라인｣ (’24.2.)규칙, 정규표현식 등을 통한 개인정보 검출 및 마스킹은 정확도 측면에서 한계가 있을 수 있으며, 이를 보완하기 위해 LLM 모델을 통해 사전에 정의되지 않은 패턴의 개인정보를 검출하고 마스킹할 수 있음학습방법에 따라 다양한 형태의 AI 기반 개인 정 보  검 출  기법  존 재(HMM, MEM, CRFs 등)\nAI 기반 텍스트정보 가명처리영상정보 가명처리 예시(이미지 필터링 기술 적용)특히,개인식별위험이크고범죄등에악용될경우국민에게큰피해를야기할수있어개인정보보호법에서특별히보호하고있는주민등록번호와그밖의고유식별정보,계좌번호,신용카드번호등은AI학습전삭제하거나가명․익명화해야합니다. 참고주요 AI 서비스 사전 실태점검 결과(’24.3.)▪ 사전 학습단계(pre-training)에서 한국 정보주체를 식별할 위험이 크고 유·노출 시 2차 피해를 야기할 우려가 큰 정보 항목(주민번호 등 고유식별정보, 계좌정보, 신용카드정보, 휴대전화번호 등) 제거 권고▪ 개인정보위-한국인터넷진흥원(KISA)에서 탐지한 한국 정보주체의 개인정보 노출 페이지(URL)를 AI 서비스 제공사업자에 제공(신청 기반)\n법률 제34조의2(노출된 개인정보의 삭제ㆍ차단) ① 개인정보처리자는 고유식별정보, 계좌정보, 신용카드정보 등 개인정보가 정보통신망을 통하여 공중(公衆)에 노출되지 아니하도록 하여야 한다. ② 개인정보처리자는 공중에 노출된 개인정보에 대하여 보호위원회 또는 대통령령으로 지정한 전문기관의 요청이 있는 경우에는 해당 정보를 삭제하거나 차단하는 등 필요한 조치를 하여야 한다."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료]\n\n- 26 -\n아울러,적절하게생성된합성데이터32)(syntheticdata)는개인정보에대해요구되는법적제약없이모델성능을유지할수있는실용적인대안으로주목받고있습니다.합성데이터생성시에는실제데이터의구조적정보를최대한유지하여유용성을확보하면서도,원본데이터에포함된개인이식별되지않도록균형점을찾는것이중요합니다.참고합성데이터 생성·활용 시 고려사항 ※｢합성데이터 생성·활용 안내서｣(’24.12.)▪ (안전기준 설정) 합성데이터의 유용성과 안전성 간 상충관계를 고려하여, 활용목적 등에 따른 안전기준을 먼저 설정▪ (원본데이터 전처리) 원본데이터에 대한 분석을 바탕으로 합성에 불필요한 영역 삭제 및 데이터 정제 등을 수행할 것 권장▪ (안전성 검증) 생성된 합성데이터가 개인 식별가능정보가 존재하는 상황에 대비하여 정량·정성적 차원의 안전성 검증 수행▪ (안전한 관리) 합성데이터를 일반에 공개하는 경우 재식별 등 잔여위험 가능성에 대비하여 관리계획 마련·이행\n(예시1 : 유용성에 중점을 둔 경우) 안전한 내부 폐쇄환경에서의 활용, 침해 위험성이 낮은 환경에서 LLM의 토큰 단위 AI 학습에 활용(예시2 : 안전성에 중점을 둔 경우) IT 시스템 위탁 개발 시 활용, 합성데이터의 외부 공개 \n마지막으로개인정보강화기술(PET,privacyenhancingtechnology)의일환으로차분프라이버시,연합학습등프라이버시를보존하는다양한학습기법들이연구․개발되고있고,생성형AI개발에직접적용되면서AI의안전성을높이고있습니다.사례 PET 기술(연합학습) 적용 사례▪ 민감성 높은 의료데이터의 안전한 이용 위한 연합학습(federated learning) 수행 각 병원이 보유한 의료데이터는 병원 내 클라우드 환경에 안전하게 보관되며, 각 병원에서 로컬로 모델학습을 수행한 후 학습된 모델의 결과값만 중앙서버로 전송·활용 데이터 정밀성·정확성 확보가 중요한 의료분야의 경우, 연합학습 기법 적용이 유용할 수 있음  ※ 단, 연합학습한 LLM에 대해서도 유·노출 위험성 평가하여, 연합학습 외 추가적인 PET 기법 결합 필요성 검토하는 것이 바람직\n32) 특정 목적을 위해 원본데이터의 형식과 구조 및 통계적 분포 특성과 패턴을 학습하여 생성한 모의(simulated) 또는 가상(artificial) 데이터\n- 27 -\n2. 모델 수준데이터수준에서사전적으로제거하지못한개인정보리스크를보완하고,AI모델이안전하고바람직한답변을생성하도록하기위해,미세조정(fine-tuning)및정렬(alignment)등기법을활용한추가적인안전조치적용이권장됩니다.이와관련해,최근실증연구에따르면미세조정방식,범위,위치등다양한요소가모델의암기리스크를현저히증가시킬수있는것으로나타나,미세조정단계에서의안전조치중요성을보여준바있습니다33).미세조정기법으로는SFT(SupervisedFine-tuning),RLHF(ReinforcementLearningfromHumanFeedback),DPO(DirectPreferenceOptimization)등이연구․적용되어왔으며,최근에는GRPO(GroupRelativePolicyOptimization)와같은고도화된학습방식이주목받고있습니다.이들은단순응답정확도를넘어서모델판단과정의투명성과안전성을동시에향상시킬수있는방식으로활용될수있습니다."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용]\n\n- 26 -\n아울러,적절하게생성된합성데이터32)(syntheticdata)는개인정보에대해요구되는법적제약없이모델성능을유지할수있는실용적인대안으로주목받고있습니다.합성데이터생성시에는실제데이터의구조적정보를최대한유지하여유용성을확보하면서도,원본데이터에포함된개인이식별되지않도록균형점을찾는것이중요합니다.참고합성데이터 생성·활용 시 고려사항 ※｢합성데이터 생성·활용 안내서｣(’24.12.)▪ (안전기준 설정) 합성데이터의 유용성과 안전성 간 상충관계를 고려하여, 활용목적 등에 따른 안전기준을 먼저 설정▪ (원본데이터 전처리) 원본데이터에 대한 분석을 바탕으로 합성에 불필요한 영역 삭제 및 데이터 정제 등을 수행할 것 권장▪ (안전성 검증) 생성된 합성데이터가 개인 식별가능정보가 존재하는 상황에 대비하여 정량·정성적 차원의 안전성 검증 수행▪ (안전한 관리) 합성데이터를 일반에 공개하는 경우 재식별 등 잔여위험 가능성에 대비하여 관리계획 마련·이행\n(예시1 : 유용성에 중점을 둔 경우) 안전한 내부 폐쇄환경에서의 활용, 침해 위험성이 낮은 환경에서 LLM의 토큰 단위 AI 학습에 활용(예시2 : 안전성에 중점을 둔 경우) IT 시스템 위탁 개발 시 활용, 합성데이터의 외부 공개 \n마지막으로개인정보강화기술(PET,privacyenhancingtechnology)의일환으로차분프라이버시,연합학습등프라이버시를보존하는다양한학습기법들이연구․개발되고있고,생성형AI개발에직접적용되면서AI의안전성을높이고있습니다.사례 PET 기술(연합학습) 적용 사례▪ 민감성 높은 의료데이터의 안전한 이용 위한 연합학습(federated learning) 수행 각 병원이 보유한 의료데이터는 병원 내 클라우드 환경에 안전하게 보관되며, 각 병원에서 로컬로 모델학습을 수행한 후 학습된 모델의 결과값만 중앙서버로 전송·활용 데이터 정밀성·정확성 확보가 중요한 의료분야의 경우, 연합학습 기법 적용이 유용할 수 있음  ※ 단, 연합학습한 LLM에 대해서도 유·노출 위험성 평가하여, 연합학습 외 추가적인 PET 기법 결합 필요성 검토하는 것이 바람직\n32) 특정 목적을 위해 원본데이터의 형식과 구조 및 통계적 분포 특성과 패턴을 학습하여 생성한 모의(simulated) 또는 가상(artificial) 데이터\n- 27 -\n2. 모델 수준데이터수준에서사전적으로제거하지못한개인정보리스크를보완하고,AI모델이안전하고바람직한답변을생성하도록하기위해,미세조정(fine-tuning)및정렬(alignment)등기법을활용한추가적인안전조치적용이권장됩니다.이와관련해,최근실증연구에따르면미세조정방식,범위,위치등다양한요소가모델의암기리스크를현저히증가시킬수있는것으로나타나,미세조정단계에서의안전조치중요성을보여준바있습니다33).미세조정기법으로는SFT(SupervisedFine-tuning),RLHF(ReinforcementLearningfromHumanFeedback),DPO(DirectPreferenceOptimization)등이연구․적용되어왔으며,최근에는GRPO(GroupRelativePolicyOptimization)와같은고도화된학습방식이주목받고있습니다.이들은단순응답정확도를넘어서모델판단과정의투명성과안전성을동시에향상시킬수있는방식으로활용될수있습니다."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료]\n\n33) Mireshghallah, Fatemehsadat, et al(2022)의 연구결과를 인용한 김병필(2025), 「범용AI 모델의 프라이버시 리스크 진단 및 인증 방안」 연구보고서 발췌\n참고안전성 강화 위한 미세조정 기법 예시  ※ 「공개된 개인정보 처리 안내서」(’24.7.)「AI 프라이버시 리스크 관리 모델」(’24.12.)▪ SFT(Supervised Fine-Tuning)  - 비지도학습 기반의 생성형 AI를 지도학습적으로 미세조정하는 과정으로, 바람직한 답변을 생성하도록 미리 정제되거나 라벨링된 데이터를 추가 학습   ※ (예) 개인의 사생활을 묻는 프롬프트에 대해 답변을 거부하는 내용의 답안을 학습시킴▪ RLHF(Reinforcement Learning with Human Feedback)  - 보상모델 생성(Reward Model Creation) : AI 모델이 생성한 출력물에 사람(라벨러)이 점수 또는 순위를 부여하고, 이를 토대로 보상모델을 훈련   ※ (예) 개인의 사생활을 묻는 프롬프트에 대하여 사생활이 포함된 답변에는 (-1)의 보상을, 회피하는 답변에는 (+1)의 보상을 제공 - 정책 최적화(Policy Optimization): 보상모델을 사용하여 AI 모델의 정책을 최적화하는 단계로, 주로 정책 그라디언트 강화학습 알고리즘인 PPO(Proximal Policy Optimization)을 활용하여 미세조정\n- 28 -\n생성형AI모델로부터학습데이터에포함된개인정보를추출하는적대적공격(adversarialattack)에대한대응도모델수준에서다루어져야하는중요한과제입니다.AI모델을대상으로한적대적공격은이미현존하는위협이며,최근에는의료특화LLM을대상으로한실험에서모델보안장치를우회해민감정보에접근할수있는확률이80%에달하고,모델응답을통해원본정보가그대로노출될가능성도20%를상회하는등프라이버시측면에서의심각한취약성이확인되었습니다34).참고생성형 AI 모델에 대한 프라이버시 공격 사례35)\nStable Diffusion 모델에서 추출한 이미지36)  사진과 캡션이 포함된 데이터셋을 학습한 AI 모델에 이름(Ann Graham Lotz)만 입력해도 이미지를 재생성한 사례 LLM으로부터 추출한 개인정보37)  특 정  문 장(“East Stroudsburg Stroudsburg...”)을 입력하자 GPT2가 학습데이터에 포함된 주소, 이메일 등 개인정보를 출력한 사례34) Kim, Minsu, et al. \"Fine-Tuning LLMs with Medical Data: Can Safety Be Ensured?\" NEJM AI, vol. 2, no. 1, 2025, https://doi.org/10.1056/AIcs2400390. (문자를 인코딩하는 방식인 ASCII (미국정보교환표준코드)를 활용해 프롬프트를 변형하여 악의적인 질문을 하는 방식으로 프라이버시 위험성 평가)35) 손수엘, 생성형 AI 모델의 잠재적인 프라이버시 위협과 정보주체의 권리 보호, 2024 개인정보 이슈 심층분석 보고서(’24."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용]\n\n33) Mireshghallah, Fatemehsadat, et al(2022)의 연구결과를 인용한 김병필(2025), 「범용AI 모델의 프라이버시 리스크 진단 및 인증 방안」 연구보고서 발췌\n참고안전성 강화 위한 미세조정 기법 예시  ※ 「공개된 개인정보 처리 안내서」(’24.7.)「AI 프라이버시 리스크 관리 모델」(’24.12.)▪ SFT(Supervised Fine-Tuning)  - 비지도학습 기반의 생성형 AI를 지도학습적으로 미세조정하는 과정으로, 바람직한 답변을 생성하도록 미리 정제되거나 라벨링된 데이터를 추가 학습   ※ (예) 개인의 사생활을 묻는 프롬프트에 대해 답변을 거부하는 내용의 답안을 학습시킴▪ RLHF(Reinforcement Learning with Human Feedback)  - 보상모델 생성(Reward Model Creation) : AI 모델이 생성한 출력물에 사람(라벨러)이 점수 또는 순위를 부여하고, 이를 토대로 보상모델을 훈련   ※ (예) 개인의 사생활을 묻는 프롬프트에 대하여 사생활이 포함된 답변에는 (-1)의 보상을, 회피하는 답변에는 (+1)의 보상을 제공 - 정책 최적화(Policy Optimization): 보상모델을 사용하여 AI 모델의 정책을 최적화하는 단계로, 주로 정책 그라디언트 강화학습 알고리즘인 PPO(Proximal Policy Optimization)을 활용하여 미세조정\n- 28 -\n생성형AI모델로부터학습데이터에포함된개인정보를추출하는적대적공격(adversarialattack)에대한대응도모델수준에서다루어져야하는중요한과제입니다.AI모델을대상으로한적대적공격은이미현존하는위협이며,최근에는의료특화LLM을대상으로한실험에서모델보안장치를우회해민감정보에접근할수있는확률이80%에달하고,모델응답을통해원본정보가그대로노출될가능성도20%를상회하는등프라이버시측면에서의심각한취약성이확인되었습니다34).참고생성형 AI 모델에 대한 프라이버시 공격 사례35)\nStable Diffusion 모델에서 추출한 이미지36)  사진과 캡션이 포함된 데이터셋을 학습한 AI 모델에 이름(Ann Graham Lotz)만 입력해도 이미지를 재생성한 사례 LLM으로부터 추출한 개인정보37)  특 정  문 장(“East Stroudsburg Stroudsburg...”)을 입력하자 GPT2가 학습데이터에 포함된 주소, 이메일 등 개인정보를 출력한 사례34) Kim, Minsu, et al. \"Fine-Tuning LLMs with Medical Data: Can Safety Be Ensured?\" NEJM AI, vol. 2, no. 1, 2025, https://doi.org/10.1056/AIcs2400390. (문자를 인코딩하는 방식인 ASCII (미국정보교환표준코드)를 활용해 프롬프트를 변형하여 악의적인 질문을 하는 방식으로 프라이버시 위험성 평가)35) 손수엘, 생성형 AI 모델의 잠재적인 프라이버시 위협과 정보주체의 권리 보호, 2024 개인정보 이슈 심층분석 보고서(’24."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료]\n\nVol.1)36) Nicholas Carlini et al., Extracting Training Data from Diffusion Models, USENIX Security, 202337) Nicholas Carlini et al., Extracting Training Data from Large Language Models, USENIX Security, 2021\n▪ DPO(Direct Preference Optimization) - 전통적인 RLHF의 단점을 극복하기 위해 제안된 선호 기반 미세조정(Preference-based Fine-tuning)의 기법 중 하나로, 사용자 응답쌍에 대한 상대적 선호를 직접 모델 파라미터에 반영   ※ (예) A와 B라는 응답 쌍이 있을 때 사람이 A를 더 선호한 경우, 응답 A를 생성할 확률이 응답 B를 생성할 확률보다 높아지도록 모델 파라미터를 업데이트▪ GRPO(Group Relative Policy Optimization) - DPO의 한계를 개선하기 위한 기법으로 개별 응답 쌍 비교 대신, 여러 행동을 그룹화하고 상대적 성능을 평가해 정책을 업데이트   ※ 개별 응답쌍 비교 방식에서 발생할 수 있는 노이즈 등을 그룹 단위 학습 방식으로 완화\n- 29 -\n이러한결과는생성형AI의안전성확보를위해모델수준에서의안전장치보강이필요하다는점을시사합니다.가장널리검토되는방식으로AI모델의학습과정과가중치를조절해안전성을강화하는차분프라이버시기반경사하강법(DifferentiallyPrivateStochasticGradientDescent,DP-SGD)이있습니다.참고차분 프라이버시(DP, Differential Privacy) 기반 경사하강법38)▪ 데이터 학습 과정에 발생하는 기울기(gradient)를 일정 범위로 잘라내고(clipping), 여기에 무작위 노이즈를 추가하는 방식으로, 원본 데이터 암기 리스크를 낮추고 외부의 모델 공격에 강한 특징▪ 모델이 클수록(예: 고성능 이미지 분류용 모델) 정확도 저하 현상이 있어 기법 적용에 한계가 있으나, 최근 기울기(gradient)가 잘 정규화되어 있는 경우 DP-SGD 적용이 효과적일 수 있다는 연구도 진행 중\n이외에도모델경량화,성능강화등의목적으로주로활용되는지식증류(knowledgedistillation)기술을프라이버시분야에접목하여일종의PET기술로활용하는방안등다양한연구가진행되고있습니다.참고PET 기술로서의 지식증류(knowledge distillation) 기법 연구 동향39) ▪지식증류는 사전학습 모델의 성능은 유지하면서 모델을 경량화하기 위한 목적으로 주로 활용되는 기법▪최근 지식증류의 기술적 특성(학습·추론 방식을 정교하게 조정 가능)을 활용해 개인정보 학습 또는 출력을 제한하는 방안이 일종의 PET 기법으로 연구 중  - 특히 지식증류에 차분프라이버시, 합성데이터를 병행하는 연구가 활발  ※ Flemings, James, et al. Differentially Private Knowledge Distillation via Synthetic Text Generation (2024)  - 지식증류를 활용하여 개인정보만을 AI 모델에서 언러닝(unlearning)하는 기법도 등장  ※ Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models (2024)\n38) ｢AI 프라이버시 리스크 관리 모델｣(’24.12.)에 부록으로 포함된 ‘생성형 AI 관련 프라이버시 리스크 경감기술 평가연구’, (’24.5.~’10., ㈜)제이씨레이다 및 경북대학교) 참고, 구글 딥마인드 연구(Unlocking High-Accuracy Differentially Private Image Classification through Scale, ’22.6.) 참고39) “지식증류를 이용하여 개인정보 보호 문제를 해결하는 해외 동향 - 제3기 개인정보 기술포럼 워크숍”(’25.5, 법무법인 DLG 황규호)\n- 30 -\n3."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용]\n\nVol.1)36) Nicholas Carlini et al., Extracting Training Data from Diffusion Models, USENIX Security, 202337) Nicholas Carlini et al., Extracting Training Data from Large Language Models, USENIX Security, 2021\n▪ DPO(Direct Preference Optimization) - 전통적인 RLHF의 단점을 극복하기 위해 제안된 선호 기반 미세조정(Preference-based Fine-tuning)의 기법 중 하나로, 사용자 응답쌍에 대한 상대적 선호를 직접 모델 파라미터에 반영   ※ (예) A와 B라는 응답 쌍이 있을 때 사람이 A를 더 선호한 경우, 응답 A를 생성할 확률이 응답 B를 생성할 확률보다 높아지도록 모델 파라미터를 업데이트▪ GRPO(Group Relative Policy Optimization) - DPO의 한계를 개선하기 위한 기법으로 개별 응답 쌍 비교 대신, 여러 행동을 그룹화하고 상대적 성능을 평가해 정책을 업데이트   ※ 개별 응답쌍 비교 방식에서 발생할 수 있는 노이즈 등을 그룹 단위 학습 방식으로 완화\n- 29 -\n이러한결과는생성형AI의안전성확보를위해모델수준에서의안전장치보강이필요하다는점을시사합니다.가장널리검토되는방식으로AI모델의학습과정과가중치를조절해안전성을강화하는차분프라이버시기반경사하강법(DifferentiallyPrivateStochasticGradientDescent,DP-SGD)이있습니다.참고차분 프라이버시(DP, Differential Privacy) 기반 경사하강법38)▪ 데이터 학습 과정에 발생하는 기울기(gradient)를 일정 범위로 잘라내고(clipping), 여기에 무작위 노이즈를 추가하는 방식으로, 원본 데이터 암기 리스크를 낮추고 외부의 모델 공격에 강한 특징▪ 모델이 클수록(예: 고성능 이미지 분류용 모델) 정확도 저하 현상이 있어 기법 적용에 한계가 있으나, 최근 기울기(gradient)가 잘 정규화되어 있는 경우 DP-SGD 적용이 효과적일 수 있다는 연구도 진행 중\n이외에도모델경량화,성능강화등의목적으로주로활용되는지식증류(knowledgedistillation)기술을프라이버시분야에접목하여일종의PET기술로활용하는방안등다양한연구가진행되고있습니다.참고PET 기술로서의 지식증류(knowledge distillation) 기법 연구 동향39) ▪지식증류는 사전학습 모델의 성능은 유지하면서 모델을 경량화하기 위한 목적으로 주로 활용되는 기법▪최근 지식증류의 기술적 특성(학습·추론 방식을 정교하게 조정 가능)을 활용해 개인정보 학습 또는 출력을 제한하는 방안이 일종의 PET 기법으로 연구 중  - 특히 지식증류에 차분프라이버시, 합성데이터를 병행하는 연구가 활발  ※ Flemings, James, et al. Differentially Private Knowledge Distillation via Synthetic Text Generation (2024)  - 지식증류를 활용하여 개인정보만을 AI 모델에서 언러닝(unlearning)하는 기법도 등장  ※ Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models (2024)\n38) ｢AI 프라이버시 리스크 관리 모델｣(’24.12.)에 부록으로 포함된 ‘생성형 AI 관련 프라이버시 리스크 경감기술 평가연구’, (’24.5.~’10., ㈜)제이씨레이다 및 경북대학교) 참고, 구글 딥마인드 연구(Unlocking High-Accuracy Differentially Private Image Classification through Scale, ’22.6.) 참고39) “지식증류를 이용하여 개인정보 보호 문제를 해결하는 해외 동향 - 제3기 개인정보 기술포럼 워크숍”(’25.5, 법무법인 DLG 황규호)\n- 30 -\n3."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료]\n\n시스템 수준데이터․모델수준에서프라이버시보호조치가이행되어도,생성형AI가배포․운영되는시스템운영환경에서의취약점은여전히개인정보유․노출의통로가될수있습니다.따라서AI시스템의학습및추론결과의안전성을담보하기위해서는시스템수준에서의보호체계구축이필수적입니다.AI시스템이외부와API로연동되는경우,접근제어는프라이버시보호의핵심방어수단이될수있습니다.예를들어API호출권한은사전에인증된주체에게만부여하고,세분화된접근통제를적용하여악의적요청과비인가접근을효과적으로차단할수있습니다.또한,생성형AI시스템의입․출력단계에필터를적용하여안전장치를강화할수있습니다.입력필터(inputfilter)는사용자가제공하는프롬프트가악의적이거나민감정보생성을유도하는지분석․차단하는역할을합니다.예를들어,특정인의고유식별정보형식의텍스트를요청하는입력이탐지될경우해당입력값을제거하도록AI시스템을설계할수있습니다.또한,출력필터(outputfilter)를통해AI출력값에포함된개인정보가감지된경우에도이를제거또는치환하거나경고문구를삽입하는방식으로제어할수있습니다40).사례 부적절한 이용자 프롬프트에 대한 필터 사례41)\n40) ｢Llama Developer Use Guide: AI Protections｣ (Meta, '25. 4.), “Mitigating risks at the input level\" 및 ”Develop potential mitigation methods at output\" 참고\n- 31 -\n최근에는생성형AI의성능향상을위해외부지식베이스를검색하고이를입력값에결합하는검색증강생성(RAG,Retrieval-AugmentedGeneration)기법이널리활용되고있습니다.AI에이전트(agent)또한그연속선상에서자율적이고목표지향적인탐색을통해여러환경에서의지식베이스를지속적으로탐색․검색하고생성하는방향으로발전중입니다.이를통해AI모델이과거지식에만한정해서응답을생성하는한계를보완하고환각(Hallucination)을줄이면서출력결과의정확성,최신성,신뢰성을향상시킬수있습니다.다만,RAG나AI에이전트는외부지식베이스에있는문장을검색하고이를프롬프트에결합하여생성하는과정에서해당문장에포함된개인정보를그대로유․노출하는리스크가상대적으로커질수있습니다.이에,앞서논의한데이터전처리,필터링등안전조치적용을이들의구현과정에서고려할것이권장됩니다."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용]\n\n시스템 수준데이터․모델수준에서프라이버시보호조치가이행되어도,생성형AI가배포․운영되는시스템운영환경에서의취약점은여전히개인정보유․노출의통로가될수있습니다.따라서AI시스템의학습및추론결과의안전성을담보하기위해서는시스템수준에서의보호체계구축이필수적입니다.AI시스템이외부와API로연동되는경우,접근제어는프라이버시보호의핵심방어수단이될수있습니다.예를들어API호출권한은사전에인증된주체에게만부여하고,세분화된접근통제를적용하여악의적요청과비인가접근을효과적으로차단할수있습니다.또한,생성형AI시스템의입․출력단계에필터를적용하여안전장치를강화할수있습니다.입력필터(inputfilter)는사용자가제공하는프롬프트가악의적이거나민감정보생성을유도하는지분석․차단하는역할을합니다.예를들어,특정인의고유식별정보형식의텍스트를요청하는입력이탐지될경우해당입력값을제거하도록AI시스템을설계할수있습니다.또한,출력필터(outputfilter)를통해AI출력값에포함된개인정보가감지된경우에도이를제거또는치환하거나경고문구를삽입하는방식으로제어할수있습니다40).사례 부적절한 이용자 프롬프트에 대한 필터 사례41)\n40) ｢Llama Developer Use Guide: AI Protections｣ (Meta, '25. 4.), “Mitigating risks at the input level\" 및 ”Develop potential mitigation methods at output\" 참고\n- 31 -\n최근에는생성형AI의성능향상을위해외부지식베이스를검색하고이를입력값에결합하는검색증강생성(RAG,Retrieval-AugmentedGeneration)기법이널리활용되고있습니다.AI에이전트(agent)또한그연속선상에서자율적이고목표지향적인탐색을통해여러환경에서의지식베이스를지속적으로탐색․검색하고생성하는방향으로발전중입니다.이를통해AI모델이과거지식에만한정해서응답을생성하는한계를보완하고환각(Hallucination)을줄이면서출력결과의정확성,최신성,신뢰성을향상시킬수있습니다.다만,RAG나AI에이전트는외부지식베이스에있는문장을검색하고이를프롬프트에결합하여생성하는과정에서해당문장에포함된개인정보를그대로유․노출하는리스크가상대적으로커질수있습니다.이에,앞서논의한데이터전처리,필터링등안전조치적용을이들의구현과정에서고려할것이권장됩니다."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료]\n\n41) 해당 사례는 필터가 성공적으로 동작한 결과를 보여주나, 안전조치를 우회하는 새로운 종류의 탈옥 공격이 포함된 프롬프트 등이 입력되는 경우 추가적인 안전조치가 필요할 수 있음\n참고LLM 기반 에이전트 서비스와 프라이버시 침해 위험42)▪ LLM기반 에이전트가 높은 수준의 자율성과 적응성을 바탕으로 고도화되면서 개인정보 처리 방식이 복잡해지고 프라이버시 리스크 또한 다양하게 분화 중▪ LLM 기반 에이전트 유형은 개인정보 처리의 흐름에 따라 3가지로 구분  ➊ 검색형 에이전트   · 외부 DB에서 정보를 가져오는 과정에서 공개된 개인정보의 검색 및 조합을 통해 개인정보 침해 리스크 야기   · RAG 방식 등에 의한 DB 활용시 비식별화되지 않은 문서나 민감정보가 사용자 질의에 반응하여 출력될 수 있으므로, 외부 데이터 접근 통제, 사용자 인증, 응답 필터링 및 기록 관리 등 적절한 안전조치 필요  ➋ 기억형 에이전트   · 단·장기 메모리를 바탕으로 지속적 학습과 개인화된 서비스 제공하나, 사용자 행위, 선호, 심리 상태 등에 대한 장기 추적 및 프로파일링 위험 존재   · 비대칭적 정보 구조와 기술 복잡성으로 정보주체 동의와 같은 통제권 행사가 어려운 한계 → 구조적 리스크 평가 및 사전적 통제 장치 필요\n- 32 -\n지속적인 평가 체계생성형AI는단발성의학습․배포만으로는안전성과신뢰성을담보하기어렵습니다.이에,생성형AI의개발과정에서학습․평가를병렬적으로설계하여안전성을지속적으로점검․보완하는피드백루프(feedbackloop)가내재화되어야합니다.개인정보보호,편향성,출력신뢰도등의요소는학습데이터나모델구조의변화에따라민감하게영향을받기에벤치마크테스트등에기반한평가작업을주기적으로수행할것이권장됩니다.최근,LLM등을대상으로안전조치를우회하는비정상적요청을차단하는능력(‘탈옥저항성’)등안전성을평가하는벤치마크․프레임워크또한지속연구되고있습니다.기업․기관은이러한도구를활용하여정량적인평가체계를수립할수있습니다.사례 프라이버시 보호 포함한 AI 안전성 특화 벤치마크 사례▪ Future of Life Institute의 AI Safe Index43)  리스크 평가, 안전을 위한 프레임워크, 전략, 거버넌스, 책임성 등 AI 안전 평가 체계▪ HarmBench 오픈소스 평가 프레임워크44)  자동화된 레드티밍 도구를 제공하는 평가 프레임워크▪ JailBreakBench 오픈소스 평가 벤치마크45)  LLM 등 언어모델을 대상으로 탈옥에 대한 저항성을 측정하는 벤치마크 데이터셋▪ MIT AI Risk Repository46)  AI와 관련된 리스크를 검토하고 망라한 데이터베이스로 프라이버시 및 보안 리스크 포함42) \"AI Privacy Risks & Mitigations - Large Language Models\", (EDPB) 및 김병필(2025), 「범용AI 모델의 프라이버시 리스크 진단 및 인증 방안」 연구보고서 발췌\n  ➌ 멀티 에이전트   · 사용자 입력을 받은 메인 에이전트가 외부의 다양한 서브 에이전트(sub-agent)에게 전달하고, 각 에이전트는 외부 애플리케이션과 연동하거나 외부 서비스를 호출하는 과정에서 개인정보가 다수의 시스템에 공유될 가능성   · 개별 에이전트의 행위가 적절히 통제되지 못한다면, 정보 집적 및 재식별 가능성이 누적되며 그 책임 귀속 또한 모호해질 우려 → 구조적 리스크 평가 및 사전적 통제 장치 필요\n- 33 -\n4 시스템 적용 및 관리❏개발이완료되면어떻게시스템을적용하나요?AI시스템개발을완료하고난후에는최종점검을거쳐활용환경에배포․적용하게됩니다.최종점검과정에는실제환경에서시스템이의도한목적을달성하는지확인하는절차를포함합니다.배포․적용이후에는시스템성능과안전성등을계속확인하며유지․관리를수행합니다.이때정보주체권리침해가발생하는지지속적으로모니터링하고47),개인정보처리과정을투명하게알리는등정보주체권리보장노력을수행해야합니다.❏적용전과운영중에고려할프라이버시요소는무엇인가요?"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용]\n\n41) 해당 사례는 필터가 성공적으로 동작한 결과를 보여주나, 안전조치를 우회하는 새로운 종류의 탈옥 공격이 포함된 프롬프트 등이 입력되는 경우 추가적인 안전조치가 필요할 수 있음\n참고LLM 기반 에이전트 서비스와 프라이버시 침해 위험42)▪ LLM기반 에이전트가 높은 수준의 자율성과 적응성을 바탕으로 고도화되면서 개인정보 처리 방식이 복잡해지고 프라이버시 리스크 또한 다양하게 분화 중▪ LLM 기반 에이전트 유형은 개인정보 처리의 흐름에 따라 3가지로 구분  ➊ 검색형 에이전트   · 외부 DB에서 정보를 가져오는 과정에서 공개된 개인정보의 검색 및 조합을 통해 개인정보 침해 리스크 야기   · RAG 방식 등에 의한 DB 활용시 비식별화되지 않은 문서나 민감정보가 사용자 질의에 반응하여 출력될 수 있으므로, 외부 데이터 접근 통제, 사용자 인증, 응답 필터링 및 기록 관리 등 적절한 안전조치 필요  ➋ 기억형 에이전트   · 단·장기 메모리를 바탕으로 지속적 학습과 개인화된 서비스 제공하나, 사용자 행위, 선호, 심리 상태 등에 대한 장기 추적 및 프로파일링 위험 존재   · 비대칭적 정보 구조와 기술 복잡성으로 정보주체 동의와 같은 통제권 행사가 어려운 한계 → 구조적 리스크 평가 및 사전적 통제 장치 필요\n- 32 -\n지속적인 평가 체계생성형AI는단발성의학습․배포만으로는안전성과신뢰성을담보하기어렵습니다.이에,생성형AI의개발과정에서학습․평가를병렬적으로설계하여안전성을지속적으로점검․보완하는피드백루프(feedbackloop)가내재화되어야합니다.개인정보보호,편향성,출력신뢰도등의요소는학습데이터나모델구조의변화에따라민감하게영향을받기에벤치마크테스트등에기반한평가작업을주기적으로수행할것이권장됩니다.최근,LLM등을대상으로안전조치를우회하는비정상적요청을차단하는능력(‘탈옥저항성’)등안전성을평가하는벤치마크․프레임워크또한지속연구되고있습니다.기업․기관은이러한도구를활용하여정량적인평가체계를수립할수있습니다.사례 프라이버시 보호 포함한 AI 안전성 특화 벤치마크 사례▪ Future of Life Institute의 AI Safe Index43)  리스크 평가, 안전을 위한 프레임워크, 전략, 거버넌스, 책임성 등 AI 안전 평가 체계▪ HarmBench 오픈소스 평가 프레임워크44)  자동화된 레드티밍 도구를 제공하는 평가 프레임워크▪ JailBreakBench 오픈소스 평가 벤치마크45)  LLM 등 언어모델을 대상으로 탈옥에 대한 저항성을 측정하는 벤치마크 데이터셋▪ MIT AI Risk Repository46)  AI와 관련된 리스크를 검토하고 망라한 데이터베이스로 프라이버시 및 보안 리스크 포함42) \"AI Privacy Risks & Mitigations - Large Language Models\", (EDPB) 및 김병필(2025), 「범용AI 모델의 프라이버시 리스크 진단 및 인증 방안」 연구보고서 발췌\n  ➌ 멀티 에이전트   · 사용자 입력을 받은 메인 에이전트가 외부의 다양한 서브 에이전트(sub-agent)에게 전달하고, 각 에이전트는 외부 애플리케이션과 연동하거나 외부 서비스를 호출하는 과정에서 개인정보가 다수의 시스템에 공유될 가능성   · 개별 에이전트의 행위가 적절히 통제되지 못한다면, 정보 집적 및 재식별 가능성이 누적되며 그 책임 귀속 또한 모호해질 우려 → 구조적 리스크 평가 및 사전적 통제 장치 필요\n- 33 -\n4 시스템 적용 및 관리❏개발이완료되면어떻게시스템을적용하나요?AI시스템개발을완료하고난후에는최종점검을거쳐활용환경에배포․적용하게됩니다.최종점검과정에는실제환경에서시스템이의도한목적을달성하는지확인하는절차를포함합니다.배포․적용이후에는시스템성능과안전성등을계속확인하며유지․관리를수행합니다.이때정보주체권리침해가발생하는지지속적으로모니터링하고47),개인정보처리과정을투명하게알리는등정보주체권리보장노력을수행해야합니다.❏적용전과운영중에고려할프라이버시요소는무엇인가요?"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료]\n\n배포 전 점검 사항배포전AI가기존목적대로안전하게동작하는지최종점검해야합니다.AI모델,소스코드,학습데이터등을전반적으로검토․분석하며정확도,안정성을평가해야합니다.기업․기관이외부모델을사용하거나시스템을발주하는경우등분석대상에직접접근할수없는경우,외부모델에쿼리를입력해서산출물을평가하거나시스템공급업체에평가데이터를제공하는등으로대체검사방법을사용하는것을고려할수있습니다48).이와같이최종배포전에실제동작환경에서AI의정확도,안전조치우회시도에대한저항성,학습데이터유·노출가능성등프라이버시리스크를점검하고그결과를문서로관리하여배포전시스템의안전성을높일수있습니다.43) https://futureoflife.org/index, FLI AI Safety Index 2024 (2024. 12. 11.)44) https://github.com/centerforaisafety/HarmBench45) https://jailbreakbench.github.io46) https://airisk.mit.edu47)GDS, ｢AI Playbook for the UK Government｣, “Principle 4: You have meaningful human control at the right stages” 참고48)OMB(Office of Management and Budget) Memorandum(M-25-21), ｢Accelerating Federal Use of AI through Innovation, Governance, and Public Trust｣, Section 4(b)의 i. \"Conduct Pre-Deployment Testing\" 참고\n- 34 -\n기업․기관은배포전테스트단계에서최종적으로확인한프라이버시리스크를고려해서생성형AI의사용목적,금지행위등을‘허용되는이용방침’(AUP,acceptableusepolicy)등에작성․공개할수있습니다.이때,전략수립단계에서검토한내용을참고할수있습니다.PbD원칙,개인정보영향평가등사전예방적접근으로식별한생성형AI의프라이버시리스크와예견가능한오․남용을고려해AUP를작성할수있습니다.작성․공개한AUP는생성형AI가배포된후오․남용을미연에방지하고,이를위반하는이용행위에대하여서비스제한또는계정차단등의조치를취할수있는근거가될수있습니다.사례 허용되는 이용 방침(Acceptable Use Policy) 활용 사례▪ 네이버 CLOVA X 서비스 이용정책 일부(‘24.7월 기준)  ※ 최종이용자 대상 작성사용자는 CLOVA X 서비스를 사용함에 있어 아래 의무를 부담합니다.① 사용자는 CLOVA X 서비스를 악의적으로 사용하는 것이 금지됩니다. 악의적 사용에는 아래와 같은 행위 및 이와 유사한 목적을 가진 행위가 포함되며, 아래 예시에 한정되지 아니합니다. 1. 불법적인 행위나 범죄 및 유해한 행동에 대한 콘텐츠 생성    • 아동 성적 학대 또는 착취와 관련된 콘텐츠    • 불법 약품(마약 등) 또는 상품(무기 등)의 판매를 조장/촉진 또는 이를 제조하는 방법에 대한 콘텐츠    • …  6. 악성코드 및 해킹, 공격, 서비스 어뷰징 코드 등의 생성 등    • 정보처리장치 등에 접근권한 없이 액세스하는 등 침입하거나…  8. 본인이나 타인의 민감정보, 고유식별정보 등 개인정보를 입력하거나 개인정보 및 사생활 침해를 야기할 수 있는 대화의 유도 및 콘텐츠 생성\n참고해외 AI 시스템 배포 전 테스트 사례▪ US CAISI 및 UK AISI의 Anthropic 모델 대상 합동 배포 전 테스트(24."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용]\n\n배포 전 점검 사항배포전AI가기존목적대로안전하게동작하는지최종점검해야합니다.AI모델,소스코드,학습데이터등을전반적으로검토․분석하며정확도,안정성을평가해야합니다.기업․기관이외부모델을사용하거나시스템을발주하는경우등분석대상에직접접근할수없는경우,외부모델에쿼리를입력해서산출물을평가하거나시스템공급업체에평가데이터를제공하는등으로대체검사방법을사용하는것을고려할수있습니다48).이와같이최종배포전에실제동작환경에서AI의정확도,안전조치우회시도에대한저항성,학습데이터유·노출가능성등프라이버시리스크를점검하고그결과를문서로관리하여배포전시스템의안전성을높일수있습니다.43) https://futureoflife.org/index, FLI AI Safety Index 2024 (2024. 12. 11.)44) https://github.com/centerforaisafety/HarmBench45) https://jailbreakbench.github.io46) https://airisk.mit.edu47)GDS, ｢AI Playbook for the UK Government｣, “Principle 4: You have meaningful human control at the right stages” 참고48)OMB(Office of Management and Budget) Memorandum(M-25-21), ｢Accelerating Federal Use of AI through Innovation, Governance, and Public Trust｣, Section 4(b)의 i. \"Conduct Pre-Deployment Testing\" 참고\n- 34 -\n기업․기관은배포전테스트단계에서최종적으로확인한프라이버시리스크를고려해서생성형AI의사용목적,금지행위등을‘허용되는이용방침’(AUP,acceptableusepolicy)등에작성․공개할수있습니다.이때,전략수립단계에서검토한내용을참고할수있습니다.PbD원칙,개인정보영향평가등사전예방적접근으로식별한생성형AI의프라이버시리스크와예견가능한오․남용을고려해AUP를작성할수있습니다.작성․공개한AUP는생성형AI가배포된후오․남용을미연에방지하고,이를위반하는이용행위에대하여서비스제한또는계정차단등의조치를취할수있는근거가될수있습니다.사례 허용되는 이용 방침(Acceptable Use Policy) 활용 사례▪ 네이버 CLOVA X 서비스 이용정책 일부(‘24.7월 기준)  ※ 최종이용자 대상 작성사용자는 CLOVA X 서비스를 사용함에 있어 아래 의무를 부담합니다.① 사용자는 CLOVA X 서비스를 악의적으로 사용하는 것이 금지됩니다. 악의적 사용에는 아래와 같은 행위 및 이와 유사한 목적을 가진 행위가 포함되며, 아래 예시에 한정되지 아니합니다. 1. 불법적인 행위나 범죄 및 유해한 행동에 대한 콘텐츠 생성    • 아동 성적 학대 또는 착취와 관련된 콘텐츠    • 불법 약품(마약 등) 또는 상품(무기 등)의 판매를 조장/촉진 또는 이를 제조하는 방법에 대한 콘텐츠    • …  6. 악성코드 및 해킹, 공격, 서비스 어뷰징 코드 등의 생성 등    • 정보처리장치 등에 접근권한 없이 액세스하는 등 침입하거나…  8. 본인이나 타인의 민감정보, 고유식별정보 등 개인정보를 입력하거나 개인정보 및 사생활 침해를 야기할 수 있는 대화의 유도 및 콘텐츠 생성\n참고해외 AI 시스템 배포 전 테스트 사례▪ US CAISI 및 UK AISI의 Anthropic 모델 대상 합동 배포 전 테스트(24."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료]\n\n11.)  -美 AI 표준 및 혁신 센터(US CAISI) 및 英 AI 안전연구소(UK AISI)에서 합동 테스트를 Anthropic의 ‘Sonnet 3.5 (new)’ 모델을 대상으로 수행  - 소프트웨어 개발 분야를 포함한 전문 분야 문제 해결 능력과 함께 안전조치(safeguard) 효과성*을 기준으로 테스트를 수행   *UK AISI에서 자체 개발한 Criminal Activity, AgentHarm와 공개적으로 알려진 HarmBench 등 여러 탈옥 공격 기법을 적용하며 대상 모델의 응답을 정략적으로 평가\n- 35 -\n▪ OpenAI Business 이용정책 일부(‘25.5월 기준)    ※ 이용사업자 등 대상 작성3.3. Restrictions. Customer will not, and will not permit End Users to: (a) use the Services or Customer Content in a way that violates applicable laws or OpenAI Policies; (b) use the Services or Customer Content in a way that violates third parties’ rights; (c) allow minors to use OpenAI Services without consent from their parent or guardian; (중략) (h) interfere with or disrupt the Services, including circumvent any rate limits or restrictions or bypass any protective measures or safety mitigations for the Services; (이하생략)모니터링, 정보주체 권리보장, 투명성 확보기업․기관은편향․차별적이거나권리를침해하는등부적절한결과물에대응할수있도록정보주체의신고․의견제출기능을시스템에탑재해야합니다.이후,해당창구를상시적으로모니터링하여접수된의견에대해서는운영정책및시스템개선에적극반영해야합니다49).정보주체의열람,정정․삭제등의요청이있는경우에는시간․비용․기술적측면에서합리적으로실현가능한범위에서정보주체의권리보장방안을마련할필요가있습니다.개인정보보호법에서는열람,정정․삭제,처리정지요구권에대하여원칙적으로10일이내대응하도록규정하고있습니다.다만학습데이터셋크기,구성방식․체계등을감안하여전통적인권리행사보장이어려운경우에는그사유를정보주체에게알기쉽게알리고,대체수단등을통해최대한성실하게요구에응하는것이권장됩니다.예를들어출력필터링등을우선적용하여개인정보가발화되지않도록긴급조치하고,추후학습데이터셋에서개인정보를제외하는방안을고려할수있습니다.한편,비용부담이큰모델재학습대신모델내특정항목을삭제하는’머신언러닝‘기술은아직기술성숙도가높지않은한계가있으나,향후정보주체권리보장을위한유용한수단이될수있습니다.49) ｢Llama Developer Use Guide: AI Protections｣ (Meta, '25. 4.), “Feedback & reporting mechanisms\" 참고\n- 36 -\n참고모델 내 개인정보 삭제를 위한 모델 언러닝 기법50)  ※ 「AI 프라이버시 리스크 관리 모델」(’24.12.)‣(개념) 모델이 학습된 정보를 의도적으로 망각하는 것으로 잘못된 정보나 학습에 부적합한 정보(개인정보, 저작권 등)를 삭제하는 기술‣(한계) 최근 위험 경감 기술로서 주목받고 있는 분야로 추가적 연구가 필요하며, 적절한 망각 수준을 찾기 어려운 한계 등 생성형AI시스템을활용하여정보주체에대한의사결정을내리는경우에는해당의사결정이최종적인결정으로서개인정보보호법상자동화된결정에해당하는지여부를확인하고,거부권,설명요구권,검토요구권등정보주체의권리를보장해야합니다."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용]\n\n11.)  -美 AI 표준 및 혁신 센터(US CAISI) 및 英 AI 안전연구소(UK AISI)에서 합동 테스트를 Anthropic의 ‘Sonnet 3.5 (new)’ 모델을 대상으로 수행  - 소프트웨어 개발 분야를 포함한 전문 분야 문제 해결 능력과 함께 안전조치(safeguard) 효과성*을 기준으로 테스트를 수행   *UK AISI에서 자체 개발한 Criminal Activity, AgentHarm와 공개적으로 알려진 HarmBench 등 여러 탈옥 공격 기법을 적용하며 대상 모델의 응답을 정략적으로 평가\n- 35 -\n▪ OpenAI Business 이용정책 일부(‘25.5월 기준)    ※ 이용사업자 등 대상 작성3.3. Restrictions. Customer will not, and will not permit End Users to: (a) use the Services or Customer Content in a way that violates applicable laws or OpenAI Policies; (b) use the Services or Customer Content in a way that violates third parties’ rights; (c) allow minors to use OpenAI Services without consent from their parent or guardian; (중략) (h) interfere with or disrupt the Services, including circumvent any rate limits or restrictions or bypass any protective measures or safety mitigations for the Services; (이하생략)모니터링, 정보주체 권리보장, 투명성 확보기업․기관은편향․차별적이거나권리를침해하는등부적절한결과물에대응할수있도록정보주체의신고․의견제출기능을시스템에탑재해야합니다.이후,해당창구를상시적으로모니터링하여접수된의견에대해서는운영정책및시스템개선에적극반영해야합니다49).정보주체의열람,정정․삭제등의요청이있는경우에는시간․비용․기술적측면에서합리적으로실현가능한범위에서정보주체의권리보장방안을마련할필요가있습니다.개인정보보호법에서는열람,정정․삭제,처리정지요구권에대하여원칙적으로10일이내대응하도록규정하고있습니다.다만학습데이터셋크기,구성방식․체계등을감안하여전통적인권리행사보장이어려운경우에는그사유를정보주체에게알기쉽게알리고,대체수단등을통해최대한성실하게요구에응하는것이권장됩니다.예를들어출력필터링등을우선적용하여개인정보가발화되지않도록긴급조치하고,추후학습데이터셋에서개인정보를제외하는방안을고려할수있습니다.한편,비용부담이큰모델재학습대신모델내특정항목을삭제하는’머신언러닝‘기술은아직기술성숙도가높지않은한계가있으나,향후정보주체권리보장을위한유용한수단이될수있습니다.49) ｢Llama Developer Use Guide: AI Protections｣ (Meta, '25. 4.), “Feedback & reporting mechanisms\" 참고\n- 36 -\n참고모델 내 개인정보 삭제를 위한 모델 언러닝 기법50)  ※ 「AI 프라이버시 리스크 관리 모델」(’24.12.)‣(개념) 모델이 학습된 정보를 의도적으로 망각하는 것으로 잘못된 정보나 학습에 부적합한 정보(개인정보, 저작권 등)를 삭제하는 기술‣(한계) 최근 위험 경감 기술로서 주목받고 있는 분야로 추가적 연구가 필요하며, 적절한 망각 수준을 찾기 어려운 한계 등 생성형AI시스템을활용하여정보주체에대한의사결정을내리는경우에는해당의사결정이최종적인결정으로서개인정보보호법상자동화된결정에해당하는지여부를확인하고,거부권,설명요구권,검토요구권등정보주체의권리를보장해야합니다."}
{"meta": {"section_type": "guideline", "article_ref": "제37조", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료] [제37조]\n\n참고자동화된 결정에 대한 정보주체 권리와 조치 의무※ ｢자동화된 결정에 대한 정보주체 권리안내서｣ (’24.9.)정보주체 권리개인정보처리자 조치거부권▸ 자동화된 결정 적용 정지 또는 인적 개입에 의한 재처리 후 결과 고지설명 요구권▸ 자동화된 결정에 대한 간결하고 의미있는 설명을 제공  ※ 중대한 영향을 미치는 경우가 아닌 때에는 공개된 사항 등을 활용하여 설명검토 요구권▸ 제출한 의견 반영 여부 검토 등 조치 후 그 결과를 통지50) Shrishak, K., “AI-Complex Algorithms and effective Data Protection Supervision Effective implementation of data subjects’ rights”, Support Pool of Experts Programme EDPB (2024), 다양한 종류의 언러닝 기법들 확인 가능\n참고모델개발자 및 이용자의 정보주체 권리 보장 책임 예시※ 「AI 프라이버시 리스크 관리 모델」(’24.12.)‣ (모델이용자) 삭제 요청 수령시, ①입력 및 출력 필터링 등 기업·기관이 실행 가능한 경감조치를 취하고 ②가능한 경우 모델개발자에게 삭제·정정 요구를 전달 후 그 결과를 정보주체에게 통보‣ (모델개발자) 모델이용자로부터 정보주체의 삭제 요구 수령 시, 학습데이터에 개인정보 존재 여부를 확인하고, 종국적으로 해당 데이터가 삭제되도록 합리적인 기간 내에 모델 업데이트\n법률 제37조의2(자동화된 결정에 대한 정보주체의 권리 등) ① 정보주체는 완전히 자동화된 시스템(인공지능 기술을 적용한 시스템을 포함한다)으로 개인정보를 처리하여 이루어지는 결정(「행정기본법」 제20조에 따른 행정청의 자동적 처분은 제외하며, 이하 이 조에서 “자동화된 결정”이라 한다)이 자신의 권리 또는 의무에 중대한 영향을 미치는 경우에는 해당 개인정보처리자에 대하여 해당 결정을 거부할 수 있는 권리를 가진다. 다만, 자동화된 결정이 제15조제1항제1호ㆍ제2호 및 제4호에 따라 이루어지는 경우에는 그러하지 아니하다. ② 정보주체는 개인정보처리자가 자동화된 결정을 한 경우에는 그 결정에 대하여 설명 등을 요구할 수 있다. ③ 개인정보처리자는 제1항 또는 제2항에 따라 정보주체가 자동화된 결정을 거부하거나 이에 대한 설명 등을 요구한 경우에는 정당한 사유가 없는 한 자동화된 결정을 적용하지 아니하거나 인적 개입에 의한 재처리ㆍ설명 등 필요한 조치를 하여야 한다."}
{"meta": {"section_type": "guideline", "article_ref": "제37조", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용] [제37조]\n\n참고자동화된 결정에 대한 정보주체 권리와 조치 의무※ ｢자동화된 결정에 대한 정보주체 권리안내서｣ (’24.9.)정보주체 권리개인정보처리자 조치거부권▸ 자동화된 결정 적용 정지 또는 인적 개입에 의한 재처리 후 결과 고지설명 요구권▸ 자동화된 결정에 대한 간결하고 의미있는 설명을 제공  ※ 중대한 영향을 미치는 경우가 아닌 때에는 공개된 사항 등을 활용하여 설명검토 요구권▸ 제출한 의견 반영 여부 검토 등 조치 후 그 결과를 통지50) Shrishak, K., “AI-Complex Algorithms and effective Data Protection Supervision Effective implementation of data subjects’ rights”, Support Pool of Experts Programme EDPB (2024), 다양한 종류의 언러닝 기법들 확인 가능\n참고모델개발자 및 이용자의 정보주체 권리 보장 책임 예시※ 「AI 프라이버시 리스크 관리 모델」(’24.12.)‣ (모델이용자) 삭제 요청 수령시, ①입력 및 출력 필터링 등 기업·기관이 실행 가능한 경감조치를 취하고 ②가능한 경우 모델개발자에게 삭제·정정 요구를 전달 후 그 결과를 정보주체에게 통보‣ (모델개발자) 모델이용자로부터 정보주체의 삭제 요구 수령 시, 학습데이터에 개인정보 존재 여부를 확인하고, 종국적으로 해당 데이터가 삭제되도록 합리적인 기간 내에 모델 업데이트\n법률 제37조의2(자동화된 결정에 대한 정보주체의 권리 등) ① 정보주체는 완전히 자동화된 시스템(인공지능 기술을 적용한 시스템을 포함한다)으로 개인정보를 처리하여 이루어지는 결정(「행정기본법」 제20조에 따른 행정청의 자동적 처분은 제외하며, 이하 이 조에서 “자동화된 결정”이라 한다)이 자신의 권리 또는 의무에 중대한 영향을 미치는 경우에는 해당 개인정보처리자에 대하여 해당 결정을 거부할 수 있는 권리를 가진다. 다만, 자동화된 결정이 제15조제1항제1호ㆍ제2호 및 제4호에 따라 이루어지는 경우에는 그러하지 아니하다. ② 정보주체는 개인정보처리자가 자동화된 결정을 한 경우에는 그 결정에 대하여 설명 등을 요구할 수 있다. ③ 개인정보처리자는 제1항 또는 제2항에 따라 정보주체가 자동화된 결정을 거부하거나 이에 대한 설명 등을 요구한 경우에는 정당한 사유가 없는 한 자동화된 결정을 적용하지 아니하거나 인적 개입에 의한 재처리ㆍ설명 등 필요한 조치를 하여야 한다."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료]\n\n(이하 생략)\n- 37 -\n지금까지살펴본정보주체의권리를충분히보장하기위해서는AI학습데이터에본인의정보가포함되어있는지,개인정보가AI에서어떻게처리되는지등을정보주체에게명확하게안내할필요가있습니다.기업․기관은데이터셋수집사실,주요출처,처리목적등과함께AI시스템개인정보처리과정을개인정보처리방침,기술문서,FAQ등에투명하게공개하여정보주체가권리를행사할수있도록지원할것이권장됩니다.또한,열람및정정․삭제등요구대응에기술적으로제한이있을경우해당사항을사전에정보주체에안내하는것도정보주체권리보호에도움이될수있습니다.최근부상하고있는AI에이전트는기억능력을보유하고자율적으로작업을수행하는AI시스템으로동작과정에서과다한이용자정보수집및프로파일링등에대한우려가늘어나고있습니다.따라서AI에이전트서비스를제공하는기업․기관은프롬프트입력등이용자대화이력의▲학습데이터이용여부▲제3자제공여부▲보관․파기정책▲필터링기준등을명확하게고지하는것이권장됩니다.이때학습전․후로충분한기간을두고안내하여정보주체의실질적인선택권(opt-out등)을보장하는것이바람직합니다.참고생성형 AI 관련 개인정보 처리방침 수립 시 유의사항※ 「개인정보 처리방침 작성지침」(’25.4.)‣ AI 개발 및 서비스를 제공하는 개인정보처리자의 경우 최소 수집, 목적 명확화 등 고려하여 학습데이터 수집·이용 기준*을 미리 정하고 기재하는 것을 권장  * AI 시스템 개발에 필요한 데이터 양(volume), 범주(민감정보, 행태정보 등) 등을 고려하여 공개된 개인정보의 주요 수집 출처, 수집 방법, 안전성 확보 조치 방안 등 포함‣ 개인정보 처리방침을 변경하는 경우 변경 및 시행 시기, 변경된 내용을 지속적으로 공개하고 이전의 개인정보 처리방침이 있을 시 그간의 변경 이력 기재  * 주요 변경 사항을 별도로 안내하는 경우 웹페이지 팝업창, 공지사항 등을 통해 정보주체가 쉽게 확인할 수 있는 방법으로 알릴 것을 권장‣ 자동화된 결정을 하는 경우에는 그 기준과 절차, 개인정보가 처리되는 방식 등을 정보주체가 쉽게 확인할 수 있도록 기재\n- 38 -\n사례 정보주체 권리 보장 강화 조치 사항 예시※ 개인정보보호위원회 제2025–011–031호(비공개) 결정※ 개인정보보호위원회 제2024–006–169∼174호(공개) 결정▪이용자가 입력하는 데이터를 AI 학습하는 경우에는 이러한 사실을 분명하게 알리고 정보주체의 실질적인 선택권 보장 - (예시. 챗봇 서비스) 대화방 입력 상단에 ‘대화 내용이 학습데이터로 수집된다’는 사실과 ‘데이터 수집 거부 방법(옵트아웃 등)’을 고지▪신규 이용자에게 충분한 기간 동안 최초 고지, 옵트아웃(opt-out)하지 않은 이용자는 일정 회수 이상 추가 고지, 이후 정기적으로 재고지▪학습데이터 수집·거부·파기 정책을 투명하게 공개   ※ 학습데이터에 포함되는 개인정보 항목, 수집·이용 목적(예. LLM 학습), 개인정보 필터링 정책, 보유기간 및 파기 방법(예. 이용자의 opt-out 설정 이후부터 데이터 수집 중지) 등▪이용자가 입력한 데이터를 언제든지 손쉽게 제거‧삭제할 수 있는 기능을 제공, 해당 기능 접근성을 제고하는 것을 포함하여 개인정보 침해 최소화 조치 이행▪개인정보 침해 관련 취약점에 대해 LLM 수정 및 재배포, 문의 창구 개설, 자신의 LLM을 사용하는 기업에 대한 취약점 조치 방안 안내 절차 등을 마련사례 옵트아웃(opt-out) 보장 사례 \n학습된다는 사실과 옵트아웃을 알기 쉽게 프롬프트 입력 란에 안내(충분한 기간 최초 고지, 이후 정기적 재고지)옵트아웃을 쉽게 수행할 수 있도록직관적인 설정 인터페이스를 구성\n- 39 -\n5 AI 프라이버시 거버넌스 체계생성형AI의데이터처리흐름이복잡해지면서리스크관리의중요성이커지고있습니다.이러한환경변화에따라각기업․기관에서는개인정보관련법규준수와리스크관리를총괄하는개인정보보호책임자(CPO)중심으로내부관리체계를구축․운영할필요가있습니다."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용]\n\n(이하 생략)\n- 37 -\n지금까지살펴본정보주체의권리를충분히보장하기위해서는AI학습데이터에본인의정보가포함되어있는지,개인정보가AI에서어떻게처리되는지등을정보주체에게명확하게안내할필요가있습니다.기업․기관은데이터셋수집사실,주요출처,처리목적등과함께AI시스템개인정보처리과정을개인정보처리방침,기술문서,FAQ등에투명하게공개하여정보주체가권리를행사할수있도록지원할것이권장됩니다.또한,열람및정정․삭제등요구대응에기술적으로제한이있을경우해당사항을사전에정보주체에안내하는것도정보주체권리보호에도움이될수있습니다.최근부상하고있는AI에이전트는기억능력을보유하고자율적으로작업을수행하는AI시스템으로동작과정에서과다한이용자정보수집및프로파일링등에대한우려가늘어나고있습니다.따라서AI에이전트서비스를제공하는기업․기관은프롬프트입력등이용자대화이력의▲학습데이터이용여부▲제3자제공여부▲보관․파기정책▲필터링기준등을명확하게고지하는것이권장됩니다.이때학습전․후로충분한기간을두고안내하여정보주체의실질적인선택권(opt-out등)을보장하는것이바람직합니다.참고생성형 AI 관련 개인정보 처리방침 수립 시 유의사항※ 「개인정보 처리방침 작성지침」(’25.4.)‣ AI 개발 및 서비스를 제공하는 개인정보처리자의 경우 최소 수집, 목적 명확화 등 고려하여 학습데이터 수집·이용 기준*을 미리 정하고 기재하는 것을 권장  * AI 시스템 개발에 필요한 데이터 양(volume), 범주(민감정보, 행태정보 등) 등을 고려하여 공개된 개인정보의 주요 수집 출처, 수집 방법, 안전성 확보 조치 방안 등 포함‣ 개인정보 처리방침을 변경하는 경우 변경 및 시행 시기, 변경된 내용을 지속적으로 공개하고 이전의 개인정보 처리방침이 있을 시 그간의 변경 이력 기재  * 주요 변경 사항을 별도로 안내하는 경우 웹페이지 팝업창, 공지사항 등을 통해 정보주체가 쉽게 확인할 수 있는 방법으로 알릴 것을 권장‣ 자동화된 결정을 하는 경우에는 그 기준과 절차, 개인정보가 처리되는 방식 등을 정보주체가 쉽게 확인할 수 있도록 기재\n- 38 -\n사례 정보주체 권리 보장 강화 조치 사항 예시※ 개인정보보호위원회 제2025–011–031호(비공개) 결정※ 개인정보보호위원회 제2024–006–169∼174호(공개) 결정▪이용자가 입력하는 데이터를 AI 학습하는 경우에는 이러한 사실을 분명하게 알리고 정보주체의 실질적인 선택권 보장 - (예시. 챗봇 서비스) 대화방 입력 상단에 ‘대화 내용이 학습데이터로 수집된다’는 사실과 ‘데이터 수집 거부 방법(옵트아웃 등)’을 고지▪신규 이용자에게 충분한 기간 동안 최초 고지, 옵트아웃(opt-out)하지 않은 이용자는 일정 회수 이상 추가 고지, 이후 정기적으로 재고지▪학습데이터 수집·거부·파기 정책을 투명하게 공개   ※ 학습데이터에 포함되는 개인정보 항목, 수집·이용 목적(예. LLM 학습), 개인정보 필터링 정책, 보유기간 및 파기 방법(예. 이용자의 opt-out 설정 이후부터 데이터 수집 중지) 등▪이용자가 입력한 데이터를 언제든지 손쉽게 제거‧삭제할 수 있는 기능을 제공, 해당 기능 접근성을 제고하는 것을 포함하여 개인정보 침해 최소화 조치 이행▪개인정보 침해 관련 취약점에 대해 LLM 수정 및 재배포, 문의 창구 개설, 자신의 LLM을 사용하는 기업에 대한 취약점 조치 방안 안내 절차 등을 마련사례 옵트아웃(opt-out) 보장 사례 \n학습된다는 사실과 옵트아웃을 알기 쉽게 프롬프트 입력 란에 안내(충분한 기간 최초 고지, 이후 정기적 재고지)옵트아웃을 쉽게 수행할 수 있도록직관적인 설정 인터페이스를 구성\n- 39 -\n5 AI 프라이버시 거버넌스 체계생성형AI의데이터처리흐름이복잡해지면서리스크관리의중요성이커지고있습니다.이러한환경변화에따라각기업․기관에서는개인정보관련법규준수와리스크관리를총괄하는개인정보보호책임자(CPO)중심으로내부관리체계를구축․운영할필요가있습니다."}
{"meta": {"section_type": "guideline", "article_ref": "제31조", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료] [제31조]\n\nCPO가생성형AI의목적설정부터배포․관리에이르기까지전과정에서개인정보처리의적법성과안전성을확보하기위한관리․감독책임을수행할수있는체계를구축해야합니다.이를기반으로개인정보영향평가,레드티밍51)등점검도구를활용해생성형AI의프라이버시리스크를지속적으로평가하고,이를경감하기위한다층적안전조치를수립․이행하며,관련절차와결과를체계적으로문서화해야합니다.이과정에서중대한취약점이발견될경우기업․기관내이사회등최고의사결정기구에보고하고,관련정부부서와신속히정보를공유하는것이바람직합니다. 51) AI시스템이 어떤 위험을 일으킬 수 있는지 사전에 점검하는 일종의 ‘모의 공격 테스트’로서, 사람 또는 다른 AI가 AI모델을 의도적으로 시험해보며 유해 콘텐츠 생성, 편향된 답변, 보안 우회(jailbreak) 등 문제를 식별하는 도구·활동\n참고개인정보 보호법상 CPO 제도(法 제31조) 개인정보 보호법은 개인정보 관련 법규 준수, 오남용 방지 등 개인정보처리자의 개인정보 보호 활동을 촉진하고 책임을 부과하기 위한 CPO 제도 규정  - (CPO의 정의) 개인정보 처리에 관한 업무를 총괄하여 책임지는 자  - (CPO 지정 의무) 소상공인을 제외한 개인정보처리자는 CPO를 지정해야 함\n참고주요 AI 기업의 레드티밍 방법론 등 사례▪OpenAI: 외부 전문인력 레드티밍과 자동화 레드티밍 병행52) 전문인력 레드티밍은 ①주요 테스트 영역 선정 및 ②대상 모델 선정, ③테스트 인터페이스 및 문서화 지침 제공, ④레드티밍 결과 사후 평가 및 검토 단계로 수행  자동화 레드티밍은 대규모 공격 사례를 생성하고 테스트하는 데 효율적이며, 전문인력 레드티밍 단계에서 식별된 중요 시나리오를 반영하며 보완\n- 40 -\n아울러,CPO는최고인공지능책임자(CAIO,ChiefArtificialIntelligenceOfficer),정보보호최고책임자(CISO,ChiefInformationSecurityOfficer)등과긴밀한협력체계를유지하며,조직내개인정보처리가수반되는생성형AI개발․활용에적극적으로관여할수있는권한과역할을보장받아야합니다.특히AI기획․개발초기단계부터개입하여개인정보처리에대한충분한정보를확보하고,관련부서에적시피드백을제공하여개인정보안심설계(PbD)관점이생성형AI서비스에내재화될수있도록해야합니다. 52) \"Advancing red teaming with people and AI\", OpenAI, ('24. 11. 21.) 참고    https://openai.com/index/advancing-red-teaming-with-people-and-ai/53) \"Planning red teaming for large language models (LLMs) and their applications, Microsoft, ('25. 2. 7.) 참고54) GenAI Red Teaming Guide, OWASP, ('25. 1. 22.), “5. GenAI Red Teaming Strategy\" 및 ”6."}
{"meta": {"section_type": "guideline", "article_ref": "제31조", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용] [제31조]\n\nCPO가생성형AI의목적설정부터배포․관리에이르기까지전과정에서개인정보처리의적법성과안전성을확보하기위한관리․감독책임을수행할수있는체계를구축해야합니다.이를기반으로개인정보영향평가,레드티밍51)등점검도구를활용해생성형AI의프라이버시리스크를지속적으로평가하고,이를경감하기위한다층적안전조치를수립․이행하며,관련절차와결과를체계적으로문서화해야합니다.이과정에서중대한취약점이발견될경우기업․기관내이사회등최고의사결정기구에보고하고,관련정부부서와신속히정보를공유하는것이바람직합니다. 51) AI시스템이 어떤 위험을 일으킬 수 있는지 사전에 점검하는 일종의 ‘모의 공격 테스트’로서, 사람 또는 다른 AI가 AI모델을 의도적으로 시험해보며 유해 콘텐츠 생성, 편향된 답변, 보안 우회(jailbreak) 등 문제를 식별하는 도구·활동\n참고개인정보 보호법상 CPO 제도(法 제31조) 개인정보 보호법은 개인정보 관련 법규 준수, 오남용 방지 등 개인정보처리자의 개인정보 보호 활동을 촉진하고 책임을 부과하기 위한 CPO 제도 규정  - (CPO의 정의) 개인정보 처리에 관한 업무를 총괄하여 책임지는 자  - (CPO 지정 의무) 소상공인을 제외한 개인정보처리자는 CPO를 지정해야 함\n참고주요 AI 기업의 레드티밍 방법론 등 사례▪OpenAI: 외부 전문인력 레드티밍과 자동화 레드티밍 병행52) 전문인력 레드티밍은 ①주요 테스트 영역 선정 및 ②대상 모델 선정, ③테스트 인터페이스 및 문서화 지침 제공, ④레드티밍 결과 사후 평가 및 검토 단계로 수행  자동화 레드티밍은 대규모 공격 사례를 생성하고 테스트하는 데 효율적이며, 전문인력 레드티밍 단계에서 식별된 중요 시나리오를 반영하며 보완\n- 40 -\n아울러,CPO는최고인공지능책임자(CAIO,ChiefArtificialIntelligenceOfficer),정보보호최고책임자(CISO,ChiefInformationSecurityOfficer)등과긴밀한협력체계를유지하며,조직내개인정보처리가수반되는생성형AI개발․활용에적극적으로관여할수있는권한과역할을보장받아야합니다.특히AI기획․개발초기단계부터개입하여개인정보처리에대한충분한정보를확보하고,관련부서에적시피드백을제공하여개인정보안심설계(PbD)관점이생성형AI서비스에내재화될수있도록해야합니다. 52) \"Advancing red teaming with people and AI\", OpenAI, ('24. 11. 21.) 참고    https://openai.com/index/advancing-red-teaming-with-people-and-ai/53) \"Planning red teaming for large language models (LLMs) and their applications, Microsoft, ('25. 2. 7.) 참고54) GenAI Red Teaming Guide, OWASP, ('25. 1. 22.), “5. GenAI Red Teaming Strategy\" 및 ”6."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "의료", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 의료]\n\nBlueprint for GenAI Red Teaming“ 참고\n▪Microsoft: ①사전준비, ②테스트수행, ③결과점검 순으로 반복53) 사전준비 : 레드티밍을 수행할 다양한 분야의 전문가를 모집, 테스트 대상 선정(대상 모델, AI 시스템 인터페이스 등) 및 점검할 리스크 영역 선정 테스트수행 : 레드팀 인력에 테스트 절차 및 접근권한 등 상시 제공·지원, 상황 확인 결과점검 : 테스트 결과를 정기적으로 주요 이해관계자에게 보고 (식별된 주요 이슈 목록, 향후 레드티밍 계획 검토 및 관련 자료 등)참고레드티밍 시 모델·개발·시스템·운영 영역별 점검항목 사례54) ▪모델: 추론공격(파라미터, 학습데이터 유도 등), 추출공격(모델 정책, 시스템 프롬프트 등), 모델조정(탈옥공격, 프롬프트 인젝션, 안전성 정책 우회 등) 등 점검▪개발: 데이터오염(외부 벡터형 DB 오염, 검색증강생성결과 조작, 캐시 오염 등), 프록시/방화벽 정책 우회, 콘텐츠 필터 우회(다언어 공격, 문맥 우회 공격 등), 접근제어(세션 관리, API 접근권한, RBAC 정책, 토큰 관리 정책 등) 점검▪시스템: 원격 코드 실행 취약점(결과로 생성된 코드 실행, 시스템 커맨드 실행 등), 공급망 취약점(의존성 무결성, 업데이트 체계, 배포 파이프라인 등) 점검▪운영: 개인별 요청 빈도 제한, 허가되지 않은 데이터 접근, 입·출력 필터링 누락, 보안 감사 및 로그 기록 정보가 충분한지 등 점검\n- 41 -\n사례 프라이버시 및 보안 협력 내부 관리체계 下 AI 프라이버시 레드팀 운영 사례55)▪체계적인 테스트 방법론 사용 OWASP(OpenWorldwideApplicationSecurityProject) 등의 표준 보안·프라이버시 진단 프레임워크 활용  정형화된 공격 시나리오 및 테스트 케이스 개발▪공격자 시나리오 기반 테스트 멤버십 추론 공격(Membership Inference Attack), 모델 역전 공격(Model Inversion Attack)   민감정보 추출 시도, 프롬프트 인젝션 공격 등 테스트▪지속적인 테스트 수행  개발 초기부터 테스트 병행하며, 새로운 기능 추가 시 반드시 테스트 수행  정기(분기별, 반기별) 테스트 일정 수립▪문서화 및 보고  심각도 및 영향 범위 평가를 포함한 발견된 취약점에 대한 상세한 문서화▪최신 동향 모니터링  새로운 AI 취약점 및 공격 기법 확인       관련 법규 및 규제 변화 모니터링▪역량 강화 및 훈련  모의 해킹 훈련 및 워크숍 개최 등\n55) ｢Llama Developer Use Guide: AI Protections｣ (Meta, '25. 4.), “Red teaming best practices\" 참고56) Wang, S. et al, \"Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey\", 2024, arXiv:2406.07973, Figure 1.을 바탕으로 일부 수정\n참고생성형 AI에서의 데이터, 개발·활용 단계, 시나리오 관련 리스크 사례56)"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "신용", "guideline_source": "PIPC"}, "text": "[GUIDELINE] PIPC [Domain: 신용]\n\nBlueprint for GenAI Red Teaming“ 참고\n▪Microsoft: ①사전준비, ②테스트수행, ③결과점검 순으로 반복53) 사전준비 : 레드티밍을 수행할 다양한 분야의 전문가를 모집, 테스트 대상 선정(대상 모델, AI 시스템 인터페이스 등) 및 점검할 리스크 영역 선정 테스트수행 : 레드팀 인력에 테스트 절차 및 접근권한 등 상시 제공·지원, 상황 확인 결과점검 : 테스트 결과를 정기적으로 주요 이해관계자에게 보고 (식별된 주요 이슈 목록, 향후 레드티밍 계획 검토 및 관련 자료 등)참고레드티밍 시 모델·개발·시스템·운영 영역별 점검항목 사례54) ▪모델: 추론공격(파라미터, 학습데이터 유도 등), 추출공격(모델 정책, 시스템 프롬프트 등), 모델조정(탈옥공격, 프롬프트 인젝션, 안전성 정책 우회 등) 등 점검▪개발: 데이터오염(외부 벡터형 DB 오염, 검색증강생성결과 조작, 캐시 오염 등), 프록시/방화벽 정책 우회, 콘텐츠 필터 우회(다언어 공격, 문맥 우회 공격 등), 접근제어(세션 관리, API 접근권한, RBAC 정책, 토큰 관리 정책 등) 점검▪시스템: 원격 코드 실행 취약점(결과로 생성된 코드 실행, 시스템 커맨드 실행 등), 공급망 취약점(의존성 무결성, 업데이트 체계, 배포 파이프라인 등) 점검▪운영: 개인별 요청 빈도 제한, 허가되지 않은 데이터 접근, 입·출력 필터링 누락, 보안 감사 및 로그 기록 정보가 충분한지 등 점검\n- 41 -\n사례 프라이버시 및 보안 협력 내부 관리체계 下 AI 프라이버시 레드팀 운영 사례55)▪체계적인 테스트 방법론 사용 OWASP(OpenWorldwideApplicationSecurityProject) 등의 표준 보안·프라이버시 진단 프레임워크 활용  정형화된 공격 시나리오 및 테스트 케이스 개발▪공격자 시나리오 기반 테스트 멤버십 추론 공격(Membership Inference Attack), 모델 역전 공격(Model Inversion Attack)   민감정보 추출 시도, 프롬프트 인젝션 공격 등 테스트▪지속적인 테스트 수행  개발 초기부터 테스트 병행하며, 새로운 기능 추가 시 반드시 테스트 수행  정기(분기별, 반기별) 테스트 일정 수립▪문서화 및 보고  심각도 및 영향 범위 평가를 포함한 발견된 취약점에 대한 상세한 문서화▪최신 동향 모니터링  새로운 AI 취약점 및 공격 기법 확인       관련 법규 및 규제 변화 모니터링▪역량 강화 및 훈련  모의 해킹 훈련 및 워크숍 개최 등\n55) ｢Llama Developer Use Guide: AI Protections｣ (Meta, '25. 4.), “Red teaming best practices\" 참고56) Wang, S. et al, \"Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey\", 2024, arXiv:2406.07973, Figure 1.을 바탕으로 일부 수정\n참고생성형 AI에서의 데이터, 개발·활용 단계, 시나리오 관련 리스크 사례56)"}
