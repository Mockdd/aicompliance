{"meta": {"section_type": "article", "article_ref": "Article 6", "node_types": ["Sanction", "Requirement"]}, "text": "[ARTICLE] Article 6\n\nArticle 6\nClassification rules for high-risk AI systems\n1.   Irrespective of whether an AI system is placed on the market or put into service independently of the products referred to in points (a) and (b), that AI system shall be considered to be high-risk where both of the following conditions are fulfilled:\n(a) the AI system is intended to be used as a safety component of a product, or the AI system is itself a product, covered by the Union harmonisation legislation listed in Annex I;\n(b) the product whose safety component pursuant to point (a) is the AI system, or the AI system itself as a product, is required to undergo a third-party conformity assessment, with a view to the placing on the market or the putting into service of that product pursuant to the Union harmonisation legislati\nArticle 6\nClassification rules for high-risk AI systems\n1."}
{"meta": {"section_type": "article", "article_ref": "Article 7", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 7\n\nArticle 7\nAmendments to Annex III\n1.   The Commission is empowered to adopt delegated acts in accordance with Article 97 to amend Annex III by adding or modifying use-cases of high-risk AI systems where both of the following conditions are fulfilled:\n(a) the AI systems are intended to be used in any of the areas listed in Annex III;\n(b) the AI systems pose a risk of harm to health and safety, or an adverse impact on fundamental rights, and that risk is equivalent to, or greater than, the risk of harm or of adverse impact posed by the high-risk AI systems already referred to in Annex III.\n2."}
{"meta": {"section_type": "article", "article_ref": "Article 8", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 8\n\nArticle 8\nCompliance with the requirements\n1.   High-risk AI systems shall comply with the requirements laid down in this Section, taking into account their intended purpose as well as the generally acknowledged state of the art on AI and AI-related technologies. The risk management system referred to in Article 9 shall be taken into account when ensuring compliance with those requirements.\n2.   Where a product contains an AI system, to which the requirements of this Regulation as well as requirements of the Union harmonisation legislation listed in Section A of Annex I apply, providers shall be responsible for ensuring that their product is fully compliant with all applicable requirements under applicable Union harmonisation legislation."}
{"meta": {"section_type": "article", "article_ref": "Article 9", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 9\n\nArticle 9\nRisk management system\n1.   A risk management system shall be established, implemented, documented and maintained in relation to high-risk AI systems.\n2.   The risk management system shall be understood as a continuous iterative process planned and run throughout the entire lifecycle of a high-risk AI system, requiring regular systematic review and updating."}
{"meta": {"section_type": "article", "article_ref": "Article 10", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 10\n\nArticle 10\nData and data governance\n1.   High-risk AI systems which make use of techniques involving the training of AI models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5 whenever such data sets are used.\n2.   Training, validation and testing data sets shall be subject to data governance and management practices appropriate for the intended purpose of the high-risk AI system."}
{"meta": {"section_type": "article", "article_ref": "Article 11", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 11\n\nArticle 11\nTechnical documentation\n1.   The technical documentation of a high-risk AI system shall be drawn up before that system is placed on the market or put into service and shall be kept up-to date.\nThe technical documentation shall be drawn up in such a way as to demonstrate that the high-risk AI system complies with the requirements set out in this Section and to provide national competent authorities and notified bodies with the necessary information in a clear and comprehensive form to assess the compliance of the AI system with those requirements. It shall contain, at a minimum, the elements set out in Annex IV. SMEs, including start-ups, may provide the elements of the technical documentation specified in Annex IV in a simplified manner."}
{"meta": {"section_type": "article", "article_ref": "Article 12", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 12\n\nArticle 12\nRecord-keeping\n1.   High-risk AI systems shall technically allow for the automatic recording of events (logs) over the lifetime of the system.\n2.   In order to ensure a level of traceability of the functioning of a high-risk AI system that is appropriate to the intended purpose of the system, logging capabilities shall enable the recording of events relevant for:\n(a) identifying situations that may result in the high-risk AI system presenting a risk within the meaning of Article 79(1) or in a substantial modification;\n(b) facilitating the post-market monitoring referred to in Article 72; and\n(c) monitoring the operation of high-risk AI systems referred to in Article 26(5).\n3."}
{"meta": {"section_type": "article", "article_ref": "Article 13", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 13\n\nArticle 13\nTransparency and provision of information to deployers\n1.   High-risk AI systems shall be designed and developed in such a way as to ensure that their operation is sufficiently transparent to enable deployers to interpret a system’s output and use it appropriately. An appropriate type and degree of transparency shall be ensured with a view to achieving compliance with the relevant obligations of the provider and deployer set out in Section 3.\n2.   High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to deployers.\n3."}
{"meta": {"section_type": "article", "article_ref": "Article 14", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 14\n\nArticle 14\nHuman oversight\n1.   High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which they are in use.\n2.   Human oversight shall aim to prevent or minimise the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular where such risks persist despite the application of other requirements set out in this Section.\n3."}
{"meta": {"section_type": "article", "article_ref": "Article 15", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 15\n\nArticle 15\nAccuracy, robustness and cybersecurity\n1.   High-risk AI systems shall be designed and developed in such a way that they achieve an appropriate level of accuracy, robustness, and cybersecurity, and that they perform consistently in those respects throughout their lifecycle.\n2.   To address the technical aspects of how to measure the appropriate levels of accuracy and robustness set out in paragraph 1 and any other relevant performance metrics, the Commission shall, in cooperation with relevant stakeholders and organisations such as metrology and benchmarking authorities, encourage, as appropriate, the development of benchmarks and measurement methodologies.\n3."}
{"meta": {"section_type": "article", "article_ref": "Article 17", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 17\n\nArticle 17\nQuality management system\n1.   Providers of high-risk AI systems shall put a quality management system in place that ensures compliance with this Regulation."}
{"meta": {"section_type": "article", "article_ref": "Article 18", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 18\n\nArticle 18\nDocumentation keeping\n1.   The provider shall, for a period ending 10 years after the high-risk AI system has been placed on the market or put into service, keep at the disposal of the national competent authorities:\n(a) the technical documentation referred to in Article 11;\n(b) the documentation concerning the quality management system referred to in Article 17;\n(c) the documentation concerning the changes approved by notified bodies, where applicable;\n(d) the decisions and other documents issued by the notified bodies, where applicable;\n(e) the EU declaration of conformity referred to in Article 47.\n2."}
{"meta": {"section_type": "article", "article_ref": "Article 19", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 19\n\nArticle 19\nAutomatically generated logs\n1.   Providers of high-risk AI systems shall keep the logs referred to in Article 12(1), automatically generated by their high-risk AI systems, to the extent such logs are under their control. Without prejudice to applicable Union or national law, the logs shall be kept for a period appropriate to the intended purpose of the high-risk AI system, of at least six months, unless provided otherwise in the applicable Union or national law, in particular in Union law on the protection of personal data.\n2."}
{"meta": {"section_type": "article", "article_ref": "Article 20", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 20\n\nArticle 20\nCorrective actions and duty of information\n1.   Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system that they have placed on the market or put into service is not in conformity with this Regulation shall immediately take the necessary corrective actions to bring that system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They shall inform the distributors of the high-risk AI system concerned and, where applicable, the deployers, the authorised representative and importers accordingly.\n2."}
{"meta": {"section_type": "article", "article_ref": "Article 21", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 21\n\nArticle 21\nCooperation with competent authorities\n1.   Providers of high-risk AI systems shall, upon a reasoned request by a competent authority, provide that authority all the information and documentation necessary to demonstrate the conformity of the high-risk AI system with the requirements set out in Section 2, in a language which can be easily understood by the authority in one of the official languages of the institutions of the Union as indicated by the Member State concerned.\n2.   Upon a reasoned request by a competent authority, providers shall also give the requesting competent authority, as applicable, access to the automatically generated logs of the high-risk AI system referred to in Article 12(1), to the extent such logs are under their control.\n3."}
{"meta": {"section_type": "article", "article_ref": "Article 22", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 22\n\nArticle 22\nAuthorised representatives of providers of high-risk AI systems\n1.   Prior to making their high-risk AI systems available on the Union market, providers established in third countries shall, by written mandate, appoint an authorised representative which is established in the Union.\n2.   The provider shall enable its authorised representative to perform the tasks specified in the mandate received from the provider.\n3.   The authorised representative shall perform the tasks specified in the mandate received from the provider. It shall provide a copy of the mandate to the market surveillance authorities upon request, in one of the official languages of the institutions of the Union, as indicated by the competent authority."}
{"meta": {"section_type": "article", "article_ref": "Article 23", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 23\n\nArticle 23\nObligations of importers\n1.   Before placing a high-risk AI system on the market, importers shall ensure that the system is in conformity with this Regulation by verifying that:\n(a) the relevant conformity assessment procedure referred to in Article 43 has been carried out by the provider of the high-risk AI system;\n(b) the provider has drawn up the technical documentation in accordance with Article 11 and Annex IV;\n(c) the system bears the required CE marking and is accompanied by the EU declaration of conformity referred to in Article 47 and instructions for use;\n(d) the provider has appointed an authorised representative in accordance with Article 22(1).\n2."}
{"meta": {"section_type": "article", "article_ref": "Article 24", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 24\n\nArticle 24\nObligations of distributors\n1.   Before making a high-risk AI system available on the market, distributors shall verify that it bears the required CE marking, that it is accompanied by a copy of the EU declaration of conformity referred to in Article 47 and instructions for use, and that the provider and the importer of that system, as applicable, have complied with their respective obligations as laid down in Article 16, points (b) and (c) and Article 23(3).\n2."}
{"meta": {"section_type": "article", "article_ref": "Article 26", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 26\n\nArticle 26\nObligations of deployers of high-risk AI systems\n1.   Deployers of high-risk AI systems shall take appropriate technical and organisational measures to ensure they use such systems in accordance with the instructions for use accompanying the systems, pursuant to paragraphs 3 and 6.\n2.   Deployers shall assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support.\n3.   The obligations set out in paragraphs 1 and 2, are without prejudice to other deployer obligations under Union or national law and to the deployer’s freedom to organise its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider.\n4."}
{"meta": {"section_type": "article", "article_ref": "Article 27", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 27\n\nArticle 27\nFundamental rights impact assessment for high-risk AI systems\n1.   Prior to deploying a high-risk AI system referred to in Article 6(2), with the exception of high-risk AI systems intended to be used in the area listed in point 2 of Annex III, deployers that are bodies governed by public law, or are private entities providing public services, and deployers of high-risk AI systems referred to in points 5 (b) and (c) of Annex III, shall perform an assessment of the impact on fundamental rights that the use of such system may produce."}
{"meta": {"section_type": "article", "article_ref": "Article 31", "node_types": ["Sanction", "Requirement"]}, "text": "[ARTICLE] Article 31\n\nArticle 31\nRequirements relating to notified bodies\n1. A notified body shall be established under the national law of a Member State and shall have legal personality. 2. Notified bodies shall satisfy the organisational, quality management, resources and process requirements that are necessary to fulfil their tasks, as well as suitable cybersecurity requirements. 3. The organisational structure, allocation of responsibilities, reporting lines and operation of notified bodies shall ensure confidence in their performance, and in the results of the conformity assessment activities that the notified bodies conduct."}
{"meta": {"section_type": "article", "article_ref": "Article 31", "node_types": ["Sanction", "Requirement"]}, "text": "[ARTICLE] Article 31\n\n4. Notified bodies shall be independent of the provider of a high-risk AI system in relation to which they perform conformity assessment activities. Notified bodies shall als\nArticle 31\nRequirements relating to notified bodies\n1. A notified body shall be established under the national law of a Member State and shall have legal personality. 2. Notified bodies shall satisfy the organisational, quality management, resources and process requirements that are necessary to fulfil their tasks, as well as suitable cybersecurity requirements. 3."}
{"meta": {"section_type": "article", "article_ref": "Article 34", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 34\n\nArticle 34\nOperational obligations of notified bodies\n1.   Notified bodies shall verify the conformity of high-risk AI systems in accordance with the conformity assessment procedures set out in Article 43.\n2.   Notified bodies shall avoid unnecessary burdens for providers when performing their activities, and take due account of the size of the provider, the sector in which it operates, its structure and the degree of complexity of the high-risk AI system concerned, in particular in view of minimising administrative burdens and compliance costs for micro- and small enterprises within the meaning of Recommendation 2003/361/EC."}
{"meta": {"section_type": "article", "article_ref": "Article 38", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 38\n\nArticle 38\nCoordination of notified bodies\n1.   The Commission shall ensure that, with regard to high-risk AI systems, appropriate coordination and cooperation between notified bodies active in the conformity assessment procedures pursuant to this Regulation are put in place and properly operated in the form of a sectoral group of notified bodies.\n2.   Each notifying authority shall ensure that the bodies notified by it participate in the work of a group referred to in paragraph 1, directly or through designated representatives.\n3.   The Commission shall provide for the exchange of knowledge and best practices between notifying authorities."}
{"meta": {"section_type": "article", "article_ref": "Article 40", "node_types": ["Sanction", "Requirement"]}, "text": "[ARTICLE] Article 40\n\nArticle 40\nHarmonised standards and standardisation deliverables\n1. High-risk AI systems or general-purpose AI models which are in conformity with harmonised standards or parts thereof the references of which have been published in the\nOfficial Journal of the European Union in accordance with Regulation (EU) No 1025/2012 shall be presumed to be in conformity with the requirements set out in Section 2 of this Chapter or, as applicable, with the obligations set out in of Chapter V, Sections 2 and 3, of this Regulation, to the extent that those standards cover those requirements or obligations."}
{"meta": {"section_type": "article", "article_ref": "Article 40", "node_types": ["Sanction", "Requirement"]}, "text": "[ARTICLE] Article 40\n\n2. In accordance with Article 10 of Regulation (EU) No 1025/2012, the Commission shall issue, without undue delay, standardisation requests covering all requirements set out in Section 2 of this Ch\nArticle 40\nHarmonised standards and standardisation deliverables\n1. High-risk AI systems or general-purpose AI models which are in conformity with harmonised standards or parts thereof the references of which have been published in the\nOfficial Journal of the European Union in accordance with Regulation (EU) No 1025/2012 shall be presumed to be in conformity with the requirements set out in Section 2 of this Chapter or, as applicable, with the obligations set out in of Chapter V, Sections 2 and 3, of this Regulation, to the extent that those standards cover those requirements or obligations."}
{"meta": {"section_type": "article", "article_ref": "Article 40", "node_types": ["Sanction", "Requirement"]}, "text": "[ARTICLE] Article 40\n\n2. In accordance with Article 10 of Regulation (EU) No 1025/2012, the Commission shall issue, without undue delay, standardisation requests covering all requirements set out in Section 2 of this Ch [Default Stakeholder: no Stakeholder could be extracted from the text; Provider assigned as default per §3.1.]\nArticle 40\nHarmonised standards and standardisation deliverables\n1. High-risk AI systems or general-purpose AI models which are in conformity with harmonised standards or parts thereof the references of which have been published in the\nOfficial Journal of the European Union in accordance with Regulation (EU) No 1025/2012 shall be presumed to be in conformity with the requirements set out in Section 2 of this Chapter or, as applicable, with the obligations set out in of Chapter V, Sections 2 and 3, of this Regulation, to the extent that those standards cover those requirements or obligations."}
{"meta": {"section_type": "article", "article_ref": "Article 42", "node_types": ["Sanction", "Requirement"]}, "text": "[ARTICLE] Article 42\n\nArticle 42\nPresumption of conformity with certain requirements\n1. High-risk AI systems that have been trained and tested on data reflecting the specific geographical, behavioural, contextual or functional setting within which they are intended to be used shall be presumed to comply with the relevant requirements laid down in Article 10(4). 2. High-risk AI systems that have been certified or for which a statement of conformity has been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019/881 and the references of which have been published in the\nOfficial Journal of the European Union shall be presumed to comply with the cybersecurity requirements set out in Article 15 of this Regulation in so far as the cybersecurity certificate or statement of conformity or parts thereo\nArticle 42\nPresumption of conformity with certain requirements\n1."}
{"meta": {"section_type": "article", "article_ref": "Article 42", "node_types": ["Sanction", "Requirement"]}, "text": "[ARTICLE] Article 42\n\nHigh-risk AI systems that have been trained and tested on data reflecting the specific geographical, behavioural, contextual or functional setting within which they are intended to be used shall be presumed to comply with the relevant requirements laid down in Article 10(4). 2. High-risk AI systems that have been certified or for which a statement of conformity has been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019/881 and the references of which have been published in the\nOfficial Journal of the European Union shall be presumed to comply with the cybersecurity requirements set out in Article 15 of this Regulation in so far as the cybersecurity certificate or statement of conformity or parts thereo [Default Stakeholder: no Stakeholder could be extracted from the text; Provider assigned as default per §3.1.]\nArticle 42\nPresumption of conformity with certain requirements\n1."}
{"meta": {"section_type": "article", "article_ref": "Article 43", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 43\n\nArticle 43\nConformity assessment\n1.   For high-risk AI systems listed in point 1 of Annex III, where, in demonstrating the compliance of a high-risk AI system with the requirements set out in Section 2, the provider has applied harmonised standards referred to in Article 40, or, where applicable, common specifications referred to in Article 41, the provider shall opt for one of the following conformity assessment procedures based on:\n(a) the internal control referred to in Annex VI; or\n(b) the assessment of the quality management system and the assessment of the technical documentation, with the involvement of a notified body, referred to in Annex VII."}
{"meta": {"section_type": "article", "article_ref": "Article 46", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 46\n\nArticle 46\nDerogation from conformity assessment procedure\n1.   By way of derogation from Article 43 and upon a duly justified request, any market surveillance authority may authorise the placing on the market or the putting into service of specific high-risk AI systems within the territory of the Member State concerned, for exceptional reasons of public security or the protection of life and health of persons, environmental protection or the protection of key industrial and infrastructural assets. That authorisation shall be for a limited period while the necessary conformity assessment procedures are being carried out, taking into account the exceptional reasons justifying the derogation. The completion of those procedures shall be undertaken without undue delay.\n2."}
{"meta": {"section_type": "article", "article_ref": "Article 47", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 47\n\nArticle 47\nEU declaration of conformity\n1.   The provider shall draw up a written machine readable, physical or electronically signed EU declaration of conformity for each high-risk AI system, and keep it at the disposal of the national competent authorities for 10 years after the high-risk AI system has been placed on the market or put into service. The EU declaration of conformity shall identify the high-risk AI system for which it has been drawn up. A copy of the EU declaration of conformity shall be submitted to the relevant national competent authorities upon request.\n2.   The EU declaration of conformity shall state that the high-risk AI system concerned meets the requirements set out in Section 2."}
{"meta": {"section_type": "article", "article_ref": "Article 48", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 48\n\nArticle 48\nCE marking\n1.   The CE marking shall be subject to the general principles set out in Article 30 of Regulation (EC) No 765/2008.\n2.   For high-risk AI systems provided digitally, a digital CE marking shall be used, only if it can easily be accessed via the interface from which that system is accessed or via an easily accessible machine-readable code or other electronic means.\n3.   The CE marking shall be affixed visibly, legibly and indelibly for high-risk AI systems. Where that is not possible or not warranted on account of the nature of the high-risk AI system, it shall be affixed to the packaging or to the accompanying documentation, as appropriate.\n4."}
{"meta": {"section_type": "article", "article_ref": "Article 49", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 49\n\nArticle 49\nRegistration\n1.   Before placing on the market or putting into service a high-risk AI system listed in Annex III, with the exception of high-risk AI systems referred to in point 2 of Annex III, the provider or, where applicable, the authorised representative shall register themselves and their system in the EU database referred to in Article 71.\n2.   Before placing on the market or putting into service an AI system for which the provider has concluded that it is not high-risk according to Article 6(3), that provider or, where applicable, the authorised representative shall register themselves and that system in the EU database referred to in Article 71.\n3."}
{"meta": {"section_type": "article", "article_ref": "Article 60", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 60\n\nArticle 60\nTesting of high-risk AI systems in real world conditions outside AI regulatory sandboxes\n1.   Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes may be conducted by providers or prospective providers of high-risk AI systems listed in Annex III, in accordance with this Article and the real-world testing plan referred to in this Article, without prejudice to the prohibitions under Article 5.\nThe Commission shall, by means of implementing acts, specify the detailed elements of the real-world testing plan. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 98(2)."}
{"meta": {"section_type": "article", "article_ref": "Article 63", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 63\n\nArticle 63\nDerogations for specific operators\n1.   Microenterprises within the meaning of Recommendation 2003/361/EC may comply with certain elements of the quality management system required by Article 17 of this Regulation in a simplified manner, provided that they do not have partner enterprises or linked enterprises within the meaning of that Recommendation. For that purpose, the Commission shall develop guidelines on the elements of the quality management system which may be complied with in a simplified manner considering the needs of microenterprises, without affecting the level of protection or the need for compliance with the requirements in respect of high-risk AI systems.\n2."}
{"meta": {"section_type": "article", "article_ref": "Article 71", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 71\n\nArticle 71\nEU database for high-risk AI systems listed in Annex III\n1.   The Commission shall, in collaboration with the Member States, set up and maintain an EU database containing information referred to in paragraphs 2 and 3 of this Article concerning high-risk AI systems referred to in Article 6(2) which are registered in accordance with Articles 49 and 60 and AI systems that are not considered as high-risk pursuant to Article 6(3) and which are registered in accordance with Article 6(4) and Article 49. When setting the functional specifications of such database, the Commission shall consult the relevant experts, and when updating the functional specifications of such database, the Commission shall consult the Board.\n2."}
{"meta": {"section_type": "article", "article_ref": "Article 72", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 72\n\nArticle 72\nPost-market monitoring by providers and post-market monitoring plan for high-risk AI systems\n1.   Providers shall establish and document a post-market monitoring system in a manner that is proportionate to the nature of the AI technologies and the risks of the high-risk AI system.\n2.   The post-market monitoring system shall actively and systematically collect, document and analyse relevant data which may be provided by deployers or which may be collected through other sources on the performance of high-risk AI systems throughout their lifetime, and which allow the provider to evaluate the continuous compliance of AI systems with the requirements set out in Chapter III, Section 2."}
{"meta": {"section_type": "article", "article_ref": "Article 73", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 73\n\nArticle 73\nReporting of serious incidents\n1.   Providers of high-risk AI systems placed on the Union market shall report any serious incident to the market surveillance authorities of the Member States where that incident occurred.\n2.   The report referred to in paragraph 1 shall be made immediately after the provider has established a causal link between the AI system and the serious incident or the reasonable likelihood of such a link, and, in any event, not later than 15 days after the provider or, where applicable, the deployer, becomes aware of the serious incident.\nThe period for the reporting referred to in the first subparagraph shall take account of the severity of the serious incident.\n3."}
{"meta": {"section_type": "article", "article_ref": "Article 77", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 77\n\nArticle 77\nPowers of authorities protecting fundamental rights\n1.   National public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights, including the right to non-discrimination, in relation to the use of high-risk AI systems referred to in Annex III shall have the power to request and access any documentation created or maintained under this Regulation in accessible language and format when access to that documentation is necessary for effectively fulfilling their mandates within the limits of their jurisdiction. The relevant public authority or body shall inform the market surveillance authority of the Member State concerned of any such request.\n2."}
{"meta": {"section_type": "article", "article_ref": "Article 80", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 80\n\nArticle 80\nProcedure for dealing with AI systems classified by the provider as non-high-risk in application of Annex III\n1.   Where a market surveillance authority has sufficient reason to consider that an AI system classified by the provider as non-high-risk pursuant to Article 6(3) is indeed high-risk, the market surveillance authority shall carry out an evaluation of the AI system concerned in respect of its classification as a high-risk AI system based on the conditions set out in Article 6(3) and the Commission guidelines.\n2."}
{"meta": {"section_type": "article", "article_ref": "Article 82", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 82\n\nArticle 82\nCompliant AI systems which present a risk\n1.   Where, having performed an evaluation under Article 79, after consulting the relevant national public authority referred to in Article 77(1), the market surveillance authority of a Member State finds that although a high-risk AI system complies with this Regulation, it nevertheless presents a risk to the health or safety of persons, to fundamental rights, or to other aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure that the AI system concerned, when placed on the market or put into service, no longer presents that risk without undue delay, within a period it may prescribe.\n2."}
{"meta": {"section_type": "article", "article_ref": "Article 86", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 86\n\nArticle 86\nRight to explanation of individual decision-making\n1.   Any affected person subject to a decision which is taken by the deployer on the basis of the output from a high-risk AI system listed in Annex III, with the exception of systems listed under point 2 thereof, and which produces legal effects or similarly significantly affects that person in a way that they consider to have an adverse impact on their health, safety or fundamental rights shall have the right to obtain from the deployer clear and meaningful explanations of the role of the AI system in the decision-making procedure and the main elements of the decision taken.\n2."}
{"meta": {"section_type": "article", "article_ref": "Article 95", "node_types": ["Requirement"]}, "text": "[ARTICLE] Article 95\n\nArticle 95\nCodes of conduct for voluntary application of specific requirements\n1.   The AI Office and the Member States shall encourage and facilitate the drawing up of codes of conduct, including related governance mechanisms, intended to foster the voluntary application to AI systems, other than high-risk AI systems, of some or all of the requirements set out in Chapter III, Section 2 taking into account the available technical solutions and industry best practices allowing for the application of such requirements.\n2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "guideline_source": "EU Commission"}, "text": "[Article-specific] [Article: Article 3]\n\n2 Its aim is to promote innovation in and the uptake of\nAI, while ensuring a high level of protection of health, safety, and fundamental rights in\nthe Union, including democracy and the rule of law.\n(2) The AI Act does not apply to all systems, but only to those systems that fulfil the\ndefinition of an ‘AI system’ within the meaning of Article 3(1) AI Act. The definition of\nan AI system is therefore key to understanding the scope of application of the AI Act.\n(3) Article 96(1)(f) AI Act requires the Commission to develop guidelines on the application\nof the definition o f an AI system as set out in Article 3(1) of that Act. By issuing these\nGuidelines, the Commission aims to assist providers and other relevant persons,\nincluding market and institutional stakeholders, in determining whether a system\nconstitutes an AI syste m within the meaning of the AI Act, thereby facilitating the\neffective application and enforcement of that Act.\n(4) The definition of an AI system entered into application on 2 February 2025 3, together\nwith other provisions set out in Chapters I and II AI A ct, notably Article 5 AI Act on\nprohibited AI practices."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nCONTENTS\n1. Background and objectives ............................................................................................................. 1\n2. Overview of prohibited AI practices ............................................................................................... 2\n2.1. Prohibitions listed in Article 5 AI Act .................................................................................... 2\n2.2. Legal basis of the prohibitions ................................................................................................ 3\n2.3."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nMaterial scope: practices related to the ‘placing on the market’, ‘putting into service’ or\n‘use’ of an AI system .......................................................................................................................... 4\n2.4. Personal scope: responsible actors .......................................................................................... 5\n2.5. Exclusion from the scope of the AI Act .................................................................................. 7\n2.5.1."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nNational security, defence and military purposes ........................................................... 7\n2.5.2. Judicial and law enforcement cooperation with third countries ...................................... 9\n2.5.3. Research & Development ............................................................................................... 9\n2.5.4. Personal non-professional activity ................................................................................ 10\n2.5.5. AI systems released under free and open source licences ............................................. 11\n2.6."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n17\n2.9.1. Market Surveillance Authorities ................................................................................... 17\n2.9.2. Penalties ........................................................................................................................ 17\n3. Article 5(1)(a) and (b) AI Act – harmful manipulation, deception and exploitation .................... 18\n3.1. Rationale and objectives ....................................................................................................... 18\n3.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nMain components of the prohibition in Article 5(1)(a) AI Act – harmful manipulation ...... 19\n3.2.1. Subliminal, purposefully manipulative or deceptive techniques .................................. 19\n3.2.2. With the objective or the effect of materially distorting the behaviour of a person or a\ngroup of persons ............................................................................................................................ 24\n3.2.3. (Reasonably likely to) cause significant harm .............................................................. 28\n3.3."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nMain components of the prohibition in Article 5(1)(b) AI Act – harmful exploitation of\nvulnerabilities .................................................................................................................................... 33\n3.3.1. Exploitation of vulnerabilities due to age, disability, or a specific socio-economic\nsituation 33\n3.3.2. With the objective or the effect of materially distorting behaviour .............................. 38\n3.3.3. (Reasonably likely to) cause significant harm .............................................................. 38\n3.4."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\n6.2.2. Through untargeted scraping of facial images .............................................................. 78\n6.2.3. From the Internet and CCTV footage ........................................................................... 79\n6.3. Out of scope .......................................................................................................................... 79\n6.4. Interplay with other Union legal acts .................................................................................... 80\n7."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n6.2.2. Through untargeted scraping of facial images .............................................................. 78\n6.2.3. From the Internet and CCTV footage ........................................................................... 79\n6.3. Out of scope .......................................................................................................................... 79\n6.4. Interplay with other Union legal acts .................................................................................... 80\n7."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nArticle 5(1)(f) AI Act emotion recognition ................................................................................... 80\n7.1. Rationale and objectives ....................................................................................................... 80\n7.2. Main concepts and components of the prohibition ............................................................... 81\n7.2.1. AI systems to infer emotions ........................................................................................ 82\n7.2.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nArticle 5(1)(f) AI Act emotion recognition ................................................................................... 80\n7.1. Rationale and objectives ....................................................................................................... 80\n7.2. Main concepts and components of the prohibition ............................................................... 81\n7.2.1. AI systems to infer emotions ........................................................................................ 82\n7.2.2."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\nLimitation of the prohibition to workplace and educational institutions ...................... 84\n7.2.3. Exceptions for medical and safety reasons ................................................................... 87\n7.3. More favourable Member State law ...................................................................................... 88\n7.4. Out of scope .......................................................................................................................... 89\n8."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nLimitation of the prohibition to workplace and educational institutions ...................... 84\n7.2.3. Exceptions for medical and safety reasons ................................................................... 87\n7.3. More favourable Member State law ...................................................................................... 88\n7.4. Out of scope .......................................................................................................................... 89\n8."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\n93\n8.2.3. To deduce or infer their race, political opinions, trade union membership, religious or\nphilosophical beliefs, sex-life or sexual orientation ...................................................................... 93\n8.3. Out of scope .......................................................................................................................... 94\n8.4. Interplay with other Union law ............................................................................................. 95\n9."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n93\n8.2.3. To deduce or infer their race, political opinions, trade union membership, religious or\nphilosophical beliefs, sex-life or sexual orientation ...................................................................... 93\n8.3. Out of scope .......................................................................................................................... 94\n8.4. Interplay with other Union law ............................................................................................. 95\n9."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nArticle 5(1)(h) AI Act - Real-time Remote Biometric Identification (RBI) Systems for Law\nEnforcement Purposes .......................................................................................................................... 95\n9.1. Rationale and objectives ....................................................................................................... 96\n9.2. Main concepts and components of the prohibition ............................................................... 97\n9.2.1."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nArticle 5(1)(h) AI Act - Real-time Remote Biometric Identification (RBI) Systems for Law\nEnforcement Purposes .......................................................................................................................... 95\n9.1. Rationale and objectives ....................................................................................................... 96\n9.2. Main concepts and components of the prohibition ............................................................... 97\n9.2.1."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\nThe Notion of Remote Biometric Identification ........................................................... 97\n9.2.2. Real-time ..................................................................................................................... 100\n9.2.3. In publicly accessible spaces . ..................................................................................... 101\n9.2.4. For law enforcement purposes .................................................................................... 103\n9.3."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nThe Notion of Remote Biometric Identification ........................................................... 97\n9.2.2. Real-time ..................................................................................................................... 100\n9.2.3. In publicly accessible spaces . ..................................................................................... 101\n9.2.4. For law enforcement purposes .................................................................................... 103\n9.3."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n126\n10.4. Need for national laws within the limits of the AI Act exceptions ................................. 127\n10.4.1. Principle: national law required to provide the legal basis for the authorisation for all\nor some of the exceptions............................................................................................................ 127\n10.4.2. National law shall respect the limits and conditions of Article 5(1)(h) AI Act ...... 127\n10.4.3. Detailed national law on the authorisation request, the issuance and the exercise . 128\n10.4.4."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nDetailed national law on the supervision and the reporting relating to the\nauthorisation ................................................................................................................................ 130\n10.5. Annual reports by the national market surveillance authorities and the national data\nprotection authorities of Member States ......................................................................................... 130\n10.6. Annual reports by the Commission ................................................................................. 131\n10.7."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article \n5]\n\n3\nsocial score leading to detrimental or unfavourable\ntreatment when data comes from unrelated social\ncontexts or such treatment is unjustified or\ndisproportionate to the social behaviour\nArticle\n5(1)(d)\nIndividual criminal\noffence risk assessment\nand prediction\nAI systems that assess or predict the risk of people\ncommitting a criminal offence based solely on\nprofiling or personality traits and characteristics;\nexcept to support a human assessment based on\nobjective and verifiable facts directly linked to a\ncriminal activity\nArticle\n5(1)(e)\nUntargeted scraping to\ndevelop facial\nrecognition databases\nAI systems that create or expand facial recognition\ndatabases through untargeted scraping of facial\nimages from the internet or closed-circuit television\n(‘CCTV’) footage\nArticle\n5(1)(f)\nEmotion recognition AI systems that infer emotions at the workplace or\nin education institutions; except for medical or\nsafety reasons\nArticle\n5(1)(g)\nBiometric categorisation AI systems that categorise people based on their\nbiometric data to deduce or infer their race, political\nopinions, trade union membership, religious or\nphilosophical beliefs, sex-life or sexual orientation;\nexcept for labelling or filtering of lawfully acquired\nbiometric datasets, including in the area of law\nenforcement\nArticle\n5(1)(h)\nReal-time remote\nbiometric identification\n(‘RBI’)\nAI systems for real-time remote biometric\nidentification in publicly accessible spaces for the\npurposes of law enforcement; except if necessary\nfor the targeted search of specific victims, the\nprevention of specific threats including terrorist\nattacks, or the search of suspects of specific\noffences (further procedural requirements,\nincluding for authorisation, outlined in Article 5(2-\n7) AI Act)."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article \n5]\n\n3\nsocial score leading to detrimental or unfavourable\ntreatment when data comes from unrelated social\ncontexts or such treatment is unjustified or\ndisproportionate to the social behaviour\nArticle\n5(1)(d)\nIndividual criminal\noffence risk assessment\nand prediction\nAI systems that assess or predict the risk of people\ncommitting a criminal offence based solely on\nprofiling or personality traits and characteristics;\nexcept to support a human assessment based on\nobjective and verifiable facts directly linked to a\ncriminal activity\nArticle\n5(1)(e)\nUntargeted scraping to\ndevelop facial\nrecognition databases\nAI systems that create or expand facial recognition\ndatabases through untargeted scraping of facial\nimages from the internet or closed-circuit television\n(‘CCTV’) footage\nArticle\n5(1)(f)\nEmotion recognition AI systems that infer emotions at the workplace or\nin education institutions; except for medical or\nsafety reasons\nArticle\n5(1)(g)\nBiometric categorisation AI systems that categorise people based on their\nbiometric data to deduce or infer their race, political\nopinions, trade union membership, religious or\nphilosophical beliefs, sex-life or sexual orientation;\nexcept for labelling or filtering of lawfully acquired\nbiometric datasets, including in the area of law\nenforcement\nArticle\n5(1)(h)\nReal-time remote\nbiometric identification\n(‘RBI’)\nAI systems for real-time remote biometric\nidentification in publicly accessible spaces for the\npurposes of law enforcement; except if necessary\nfor the targeted search of specific victims, the\nprevention of specific threats including terrorist\nattacks, or the search of suspects of specific\noffences (further procedural requirements,\nincluding for authorisation, outlined in Article 5(2-\n7) AI Act)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 114", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 114]\n\n2.2. Legal basis of the prohibitions\n(10) The AI Act is supported by two legal bases: Article 114 of the Treaty on the Functioning\nof the European Union (‘TFEU’) (the internal market legal basis) and Article 16 TFEU\n(the data protection legal basis). Article 16 TFEU serves as a legal basis for the specific\nrules on the processing of personal data in relation to the prohibition on the use of\nremote biometric identification (‘RBI’) systems for law enforcement purposes,\nbiometric categorisation systems for law enforcement purposes , and individual risk\nassessments for law enforcement purposes.4 All other prohibitions listed in Article 5 AI\nAct find their legal basis in Article 114 TFEU."}
{"meta": {"section_type": "guideline", "article_ref": "Article 114", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 114]\n\n2.2. Legal basis of the prohibitions\n(10) The AI Act is supported by two legal bases: Article 114 of the Treaty on the Functioning\nof the European Union (‘TFEU’) (the internal market legal basis) and Article 16 TFEU\n(the data protection legal basis). Article 16 TFEU serves as a legal basis for the specific\nrules on the processing of personal data in relation to the prohibition on the use of\nremote biometric identification (‘RBI’) systems for law enforcement purposes,\nbiometric categorisation systems for law enforcement purposes , and individual risk\nassessments for law enforcement purposes.4 All other prohibitions listed in Article 5 AI\nAct find their legal basis in Article 114 TFEU."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n4\n2.3. Material scope: practices related to the ‘placing on the market’, ‘putting\ninto service’ or ‘use’ of an AI system\n(11) The practices prohibited by Article 5 AI Act relate to the placing on the market, the\nputting into service , or the use of specific AI systems.5 As regards real -time remote\nbiometric identification (‘RBI’) systems, the prohibition in Article 5(1)(h) AI Act only\napplies to their use. Article 3(1) AI Act defines what constitutes an A I system. The\nGuidelines on the Definition of an AI system provide the Commission’s interpretation\nof that definition."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 3]\n\n(12) According to Article 3(9) AI Act, the placing on the market of an AI system is ‘the\nfirst making available of an AI system […] on the Union market’. ‘Making available’\nis defined as the supply of the system ‘for distribution or use on the Union market in\nthe course of a commercial activity, whether in return for payment or free of charge.’ 6\nThe making available of an AI system is covered regardless of the means of supply,\nsuch as access to the system and its service through an application programm ing\ninterface ( ‘API’), via cloud, direct downloads, as physical copies , or embedded in\nphysical products."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 3]\n\nFor example, a RBI system developed outside the Union by a third-country provider is\nplaced on the Union market for the first time when it is offered in return for payment or\nfree of charge in one or more Member States. Such placing on the market may occur by\nproviding access to the system online through an API or other user interface. (13) Article 3(11) AI Act defines putting into service as ‘the supply of an AI system for\nfirst use to the deployer or for own use in the Union for its intended purpose’, therefore\ncovering both supply for first use to third parties, as well as in-house development and\ndeployment."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nThe intended purpose of the system is the ‘use for which an AI system is\nintended by the provider, including the specific context and condition s of use, as\nspecified in the information supplied by the provider in the instructions of use,\npromotional or sales materials and statements, as well as in the technical\ndocumentation.’7\nFor example, a provider builds a RBI system outside the Union and supplies that system\nto a law enforcement authority or to a private company established in a Member State\nto be used for the first time, thereby putting it into service."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article \n5]\n\nsecurity and justice (AFSJ) annexed to the TEU and TFEU, Ireland may decide not to apply the rules concerning the prohibition of\nreal-time use of RBIs in publi c spaces for a law enforcement purpose as well as the procedural rules linked to that article (Article\n5(2) to (6) AI Act) (see Recital 40). Denmark benefits from opt-out agreements when applying Protocol No. 22 to the TEU and TFEU\nand may decide not to fully apply the prohibitions based on Article 16 of the TFEU (see Recital 41)."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n6\nbe a public authority that develops the system in -house and puts it into service for its\nown use. (17) Deployers are natural or legal persons, public authorities, agencies or other bodies\nusing AI systems under their authority, unless the use is for a personal non-professional\nactivity.14 ‘Authority’ over an AI system should be understood as assuming\nresponsibility over the decision to deploy the system and over the manner of its actual\nuse."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nDeployers fall within the scope of the AI Act , if their place of establishment or\nlocation is within the Union15 or, if they are located in a third country, the output of the\nAI system is used in the Union.16\n(18) Where the deployer of an AI system is a legal person under whose authority the system\nis used, i.e. a law enforcement authority or a private security company, the individual\nemployees that act within the procedures and under the control of that person should\nnot be considered to be the deployer . A legal person also remains a deployer if it\ninvolves third parties (e.g."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\ncontractors, external staff) in the operation of the system on\nits behalf and under its responsibility and control. (19) Operators may fulfil more than one role concurrently in relation to an AI system. For\nexample, if an operator develops its own AI system that it use s afterwards, it will be\nconsidered both the provider and the deployer of that system, even if that system is also\nused by other deployers to whom the system has been provided in return for payment\nor free of charge. (20) Continuous compliance with the AI Act is required during all phases of the AI lifecycle."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nThis necessitates ongoing monitoring of and updates to AI systems placed on the market\nor put into service in the Union to ensure that an AI system remains compliant with the\nAI Act throughout its lifecycle and that it does not result in a practice prohibited under\nArticle 5 AI Act. Providers and deployers of AI systems have different responsibilities\ndepending on their roles and control over the design , the development and the actual\nuse of the system to avoid a prohibited practice."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 2]\n\nFor each of the prohibitions, these roles\nand responsibilities should be interpreted in a proportionate manner, taking into account\nwho in the value chain is best placed to adopt specific preventive and mitigating\nmeasures and ensure compliant development and use of AI systems in line with the\nobjectives and the approach of the AI Act. 2.5. Exclusion from the scope of the AI Act\n(21) Article 2 AI Act provides for a number of general exclusions from scope which are\nrelevant for a complete understanding of the practical application of the prohibitions\nlisted in Article 5 AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n7\nconcerning national security, regardless of the type of entity entrusted by the Member\nStates with carrying out tasks in relat ion to those competences. The AI Act expressly\nexcludes from its scope AI systems that are ‘placed on the market, put into service, or\nused with or without modification exclusively for military, defence or national security\npurposes, regardless of the type of entity carrying out those activities.’ Whether that\nexclusion applies therefore depends on the purposes or the uses of the AI system, not\nthe entities carrying out the activities with that system, which may also cover private\noperators entrusted by the Member States with carrying out tasks in relation to those\ncompetences."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Administration of Justice", "guideline_source": "EU Commission"}, "text": "[Domain: Administration of Justice]\n\n7\nconcerning national security, regardless of the type of entity entrusted by the Member\nStates with carrying out tasks in relat ion to those competences. The AI Act expressly\nexcludes from its scope AI systems that are ‘placed on the market, put into service, or\nused with or without modification exclusively for military, defence or national security\npurposes, regardless of the type of entity carrying out those activities.’ Whether that\nexclusion applies therefore depends on the purposes or the uses of the AI system, not\nthe entities carrying out the activities with that system, which may also cover private\noperators entrusted by the Member States with carrying out tasks in relation to those\ncompetences."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 2]\n\n(23) According to the CJEU, the term ‘national security’ refers to ‘the primary interest in\nprotecting the essential functions of the State and the fundamental interests of society\nand encompasses the prevention and punishment of activities capable of ser iously\ndestabilising the fundamental constitutional, political, economic or social structures of\na country and, in particular, of directly threatening society, the population or the Stat e\nitself, such as terrorist activities .’17 National security does not cover , for example\nactivities relating to road safety,18 or the organisation or administration of justice.19 As\nstated by the CJEU, ‘it is for the Member States to define their essential security\ninterests and to adopt appropriate measures to ensure their internal and external security,\n[…] a national measure […] taken for the purpose of protecting national security cannot\nrender EU law inapplicable and exempt the Member States from their obligation to\ncomply with that law.’20\n(24) For the exclusion in Article 2(3), second subparagraph, AI Act to apply, the AI system\nmust be placed on the market, put into service or used exclusively for military, defence\nor national security purposes."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Administration of Justice", "guideline_source": "EU Commission"}, "text": "[Domain: Administration of Justice] [Article: Article 2]\n\n(23) According to the CJEU, the term ‘national security’ refers to ‘the primary interest in\nprotecting the essential functions of the State and the fundamental interests of society\nand encompasses the prevention and punishment of activities capable of ser iously\ndestabilising the fundamental constitutional, political, economic or social structures of\na country and, in particular, of directly threatening society, the population or the Stat e\nitself, such as terrorist activities .’17 National security does not cover , for example\nactivities relating to road safety,18 or the organisation or administration of justice.19 As\nstated by the CJEU, ‘it is for the Member States to define their essential security\ninterests and to adopt appropriate measures to ensure their internal and external security,\n[…] a national measure […] taken for the purpose of protecting national security cannot\nrender EU law inapplicable and exempt the Member States from their obligation to\ncomply with that law.’20\n(24) For the exclusion in Article 2(3), second subparagraph, AI Act to apply, the AI system\nmust be placed on the market, put into service or used exclusively for military, defence\nor national security purposes."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nRecital 2 4 AI Act further clarifies how the notion\n‘exclusively’ should be interpreted and when an AI system used for such purposes may\nnevertheless fall within the scope of the AI Act. For example, if an AI system placed on the market, put into service or used for military,\ndefence or national security purposes is used (temporarily or permanently) for other\npurposes, such as for civilian or humanitarian purposes, law enforcement or publi c\nsecurity purposes, that system will fall within the scope of the AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Administration of Justice", "guideline_source": "EU Commission"}, "text": "[Domain: Administration of Justice]\n\nRecital 2 4 AI Act further clarifies how the notion\n‘exclusively’ should be interpreted and when an AI system used for such purposes may\nnevertheless fall within the scope of the AI Act. For example, if an AI system placed on the market, put into service or used for military,\ndefence or national security purposes is used (temporarily or permanently) for other\npurposes, such as for civilian or humanitarian purposes, law enforcement or publi c\nsecurity purposes, that system will fall within the scope of the AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nIn that case, the\nentity using the AI system for the other purposes should ensure compliance of the AI\nsystem with the AI Act, unless the system already complies with that act, which has to\nbe verified before such use."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Administration of Justice", "guideline_source": "EU Commission"}, "text": "[Domain: Administration of Justice]\n\nIn that case, the\nentity using the AI system for the other purposes should ensure compliance of the AI\nsystem with the AI Act, unless the system already complies with that act, which has to\nbe verified before such use."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n8\ncalled ‘ dual use ’ systems), fall within the scope of the AI Act. Providers of those\nsystems should ensure that they comply with the requirements in the AI Act. For example, if a company offers a RBI system for v arious purposes, including law\nenforcement and national security, that company is the provider of that ‘dual use’\nsystem and must ensure its compliance with the requirements in the AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(26) However, the fact that an AI system may fall within the scope of the AI Act should not\naffect the ability of entities carrying out national security, defence and military activities\nto use that system for national security, military and defence purposes, regardless of the\ntype of entity carrying out those activities.21\nFor example, if a national security agency or a private operator is tasked by a national\nintelligence agency to use real -time RBI systems for national security purposes (such\nas to gather intelligence), such use would be excluded from the scope of the AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(27) The clear delineation of the national security exclusion is particularly important where\nAI systems are placed on the market, put into service or used for law enforcement\npurposes that fall within the scope of the AI Act. This is relevant for the prohibitions\nregarding individual crime predictions and assessments and regarding the use of real -\ntime RBI systems for law enforcement purposes laid down in Article 5(1)(d) and (h) AI\nAct respectively."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nPolice and other law enforcement authorities are taske d with the\nprevention, detection, investigation and prosecution of criminal offences or the\nexecution of criminal penalties, including safeguarding against and preventing threats\nto public security .22 Whenever AI systems are used for such purposes, they wi ll fall\nwithin the scope of the AI Act. (28) The activities of Europol and other Union security agencies, such as Frontex, fall within\nthe scope of the AI Act. 2.5.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 2]\n\nJudicial and law enforcement cooperation with third countries\n(29) According to Article 2(4) AI Act, the AI Act does not apply to public authorities in a\nthird country or international organisations, where those authorities or organisations use\nAI systems in the framework of international cooperation or agreements for law\nenforcement and judicial cooperation with the Union or with one or more Member\nStates, provided that such a third country or international organisation provides\nadequate safeguards with respect to the protection of fundamental rights and freedoms\nof individuals."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nWhere relevant, this exclusion may cover the activities of private entities\nentrusted by the third country in question to carry out specific tasks in support of such\nlaw enforcement and judicial cooperation."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n9\nfor the supervision of AI systems used in the area of law enforcement and ju stice.24\nRecital 22 AI Act clarifies that the recipient national authorities and Union institutions,\nbodies, offices and agencies making use of such AI outputs in the Union remain\naccountable to ensure their use complies with Union law. When those international\nagreements are revised or new ones are concluded in the future, the contracting parties\nshould make utmost efforts to align those agreements with the requirements of th e AI\nAct. 2.5.3."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 2]\n\nResearch & Development\n(30) According to Article 2(8) AI Act, the AI Act does not apply ‘to any research, testing or\ndevelopment activity regarding AI systems or AI models prior to their being placed on\nthe market or put into service’. This exclusion is in line with the market-based logic of\nthe AI Act, which applies to AI systems once they are placed on the market or put into\nservice. For example, during the research and development (R&D) phase, AI developers have\nthe freedom to experiment and test new functionalities which might involve techniques\nthat could be seen as manipulative and covered by Article 5(1) (a) AI Act, if used in\nconsumer-facing applications."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nThe AI Act allows for such experimentation by\nrecognising that early-stage R&D is essential for refining AI technologies and ensuring\nthat they meet safety and ethical standards prior to their placing on the market. (31) As clarified in recital 25 AI Act, the AI Act aims to support innovation and recognises\nthe importance of scientific research in advancing AI technologies and contributing to\nscientific progress and innovation."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 2]\n\nArticle 2(6) AI Act therefore provides an exclusion\nfor ‘AI systems or AI models, including their outputs, specifically developed and put\ninto service for the sole purpose of scientific research and development’. For example, research into cognitive and behavioural responses to AI-driven subliminal\nor deceptive stimuli can provide valuable insights into human -AI interactions,\ninforming safer and more effective AI applications in the future. Such research is\npermitted, since it is excluded from the scope of the AI Act, notwithstanding the\nprohibition in Article 5(1)(a) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 2]\n\n(32) The exclusion in Article 2(8) AI Act is, however, without prejudice to the obligation to\ncomply with the AI Act where an AI system is placed on the market or put into service\nas a result of such research and development activity.25 Testing in real-world conditions\nwithin the meaning of the AI Act26 is also not covered by that exclusion."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 2]\n\n10\nin real-world conditions. Because real-world testing does not fall within the exclusion\nof Article 2(8) AI Act, the planned testing must be fully compliant with the\nrequirements for RBI systems in the AI Act , unless the system is tested in an AI\nregulatory sandbox or in accordance with the special regime for testing in real world\nconditions outside the sandbox, as provided for in Articles 60 and 61 AI Act.27\n(33) In any event, any research and development activity (including when excluded from the\nscope of the AI Act) should be carried out in accordance with recognised ethical and\nprofessional standards for scientific research and should be conducted in accordance\nwith applicable Union law28 (e.g., data protection law that remains applicable)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 2]\n\n2.5.4. Personal non-professional activity\n(34) Article 2(10) AI Act provides that the AI Act ‘does not apply to obligations of deployers\nwho are natural persons using systems in the course of a purely personal non-\nprofessional activity’. The definition of deployer also excludes users engaged in such\nactivities (see section 2.4. above). Any activity through which a natural person gains an\neconomic benefit on a regular basis or is otherwise involved in a professional, business,\ntrade, occupational or freelance activity should be considered as a ‘professional’\nactivity."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 2]\n\nThe specification of ‘personal’ is a qualifier of non-professional, meaning that\nthe person should act in both a personal and a non-professional capacity. The exclusion\nshould therefore, for example, not encompass criminal activities since these should not\nbe considered purely personal. For example, an individual using a facial recognition system at home (e.g. to control\naccess and to monitor for safety the entrance to the home) would fall under the exclusion\nof Article 2(10) AI Act and, hence, would not be subject to the obligations for deployers\nunder the AI Act, even in cases where it is required to transmit (parts of) the footage to\nlaw enforcement authorities."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nBy contrast , a natural person using an AI system for professional activities such as\nfreelancers, journalists, doctors , etc. would need to comply with the obligations for\ndeployers of facial recognition systems under the AI Act. Any use by natural persons\nwhere they are acting on behalf or under the authority of a deployer acting in a\nprofessional capacity will also fall within the scope of the AI Act. Furthermore, criminal activities cannot be considered purely personal activities, even if\nno economic benefit is sought or attained ."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 2]\n\nFor other unlawful activities, such as non -\ncompliance with consumer protection or data protection law and national administrative\nlegislation, the exclusion in the AI Act applies, but the other relevant legal frameworks\ncontinue to apply). (35) The exclusion in Article 2(10) AI Act applies only as regards the obligations of\ndeployers when using the system for purely personal non-professional activities."}
{"meta": {"section_type": "guideline", "article_ref": "Article 6", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[Domain: Healthcare] [Article: Article 6]\n\n11\nproviders placing the system on the market or putting it into service, other professional\ndeployers, and other responsible actors, such as importers and distributors. For example, an emotion recognition system, if intended to be used by natural persons\nfor purely personal non -professional activities, remain s a high -risk AI system as\nclassified in Article 6 AI Act and must be fully in compliance with the AI Act ."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[Domain: Healthcare] [Article: Article 2]\n\nAt the\nsame time the deployer that uses it for purely personal non-professional use (e.g., an\nautistic person) will not be covered by specific obligations for deployers under the AI\nAct and the use would be out of scope. 2.5.5. AI systems released under free and open source licences\n(36) According to Article 2(12) AI Act, th e AI Act does not apply to AI systems released\nunder free and open-source licences,29 unless they are placed on the market or put into\nservice as high-risk AI systems or as an AI system that falls under Article 5 (prohibited\nAI practices) or Article 50 AI Act (transparency obligations for certain AI systems) ."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[Domain: Healthcare] [Article: Article 5]\n\nThis means that providers of AI systems cannot benefit from this exclusion if the AI\nsystem they place on the market or put into service constitutes a prohibited practice\nunder Articles 5 AI Act. 2.6. Interplay of the prohibitions with the requirements for high -risk AI\nsystems\n(37) The AI practices prohibited by Article 5 AI Act should be considered in relation to the\nAI systems classified as high -risk in accordance with Article 6 AI Act, in particular\nthose listed in Annex III.30 That is because the use of AI systems classified as high-risk\nmay in some cases qualify as a prohibited practice in specific instances if all conditions\nunder one or more of the prohibitions in Article 5 AI Act are fulfilled."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[Domain: Healthcare] [Article: Article 5]\n\nSimilarly, certain AI-based scoring\nsystem, such as those used for credit -scoring or assessing risk in health and life\ninsurance, will be considered high -risk AI systems where they do not fulfil the\nconditions for the prohibition listed in Article 5(1)(c) AI Act.31 Another example are AI\nsystems evaluating persons and determining if they are entitled to receive essential\npublic assistance benefits and services, such as healthcare services and social security\nbenefits that are classified as high -risk.32 If such systems involve unacceptable social\nscoring and fulfil the conditions of Article 5(1)(c) AI Act, their placing on the market,\nputting into service and use will be prohibited in the Union."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Biometrics", "guideline_source": "EU Commission"}, "text": "[Domain: Biometrics]\n\n29 Recital 102 AI Act describes that a release of software and data under free and open-source licence ‘allows them to be openly shared\nand where users can freely access, use, modify and redistribute them or modified versions thereto’. 30 In this list, AI systems based on biometrics are covered, as well as AI systems used for specific purposes in certain domains such as\nemployment, education, access to public and private services, law enforcement etc. 31 This is expressly mentioned in Recital 58 and in Annex III AI Act. 32 Recital 58 AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\n29 Recital 102 AI Act describes that a release of software and data under free and open-source licence ‘allows them to be openly shared\nand where users can freely access, use, modify and redistribute them or modified versions thereto’. 30 In this list, AI systems based on biometrics are covered, as well as AI systems used for specific purposes in certain domains such as\nemployment, education, access to public and private services, law enforcement etc. 31 This is expressly mentioned in Recital 58 and in Annex III AI Act. 32 Recital 58 AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n29 Recital 102 AI Act describes that a release of software and data under free and open-source licence ‘allows them to be openly shared\nand where users can freely access, use, modify and redistribute them or modified versions thereto’. 30 In this list, AI systems based on biometrics are covered, as well as AI systems used for specific purposes in certain domains such as\nemployment, education, access to public and private services, law enforcement etc. 31 This is expressly mentioned in Recital 58 and in Annex III AI Act. 32 Recital 58 AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n29 Recital 102 AI Act describes that a release of software and data under free and open-source licence ‘allows them to be openly shared\nand where users can freely access, use, modify and redistribute them or modified versions thereto’. 30 In this list, AI systems based on biometrics are covered, as well as AI systems used for specific purposes in certain domains such as\nemployment, education, access to public and private services, law enforcement etc. 31 This is expressly mentioned in Recital 58 and in Annex III AI Act. 32 Recital 58 AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\n13\ndeceive and cause significant harms under Article 5(1) (a) AI Act , the provider is\nexpected to take appropriate and proportionate measures (e.g. , appropriate safe and\nethical design, integration of technical and other safeguards, restrictions of use ,\ntransparency and user control, appropriate information in the instructions of use) before\nthe AI system is placed on the market (Article 5(1) (a) AI Act) to ensure the chat bot\ndoes not cause significant harm to users or other persons or groups of persons (see also\nsection 3.2.3.(c))."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\n13\ndeceive and cause significant harms under Article 5(1) (a) AI Act , the provider is\nexpected to take appropriate and proportionate measures (e.g. , appropriate safe and\nethical design, integration of technical and other safeguards, restrictions of use ,\ntransparency and user control, appropriate information in the instructions of use) before\nthe AI system is placed on the market (Article 5(1) (a) AI Act) to ensure the chat bot\ndoes not cause significant harm to users or other persons or groups of persons (see also\nsection 3.2.3.(c))."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\n(41) In certain cases, in particular where the prohibitions are linked to a very specific purpose\nof the system,36 providers may have limited possibilities to integrate other preventive\nand mitigating measures and will have to rely on primarily providing appropriate\ninstructions and information to the deployers and the required human oversight and\nrestricting prohibited use of the system. Where appropriate, such measures may also\ninclude monitoring for compliance with that restriction, depending on the means\nthrough which the AI system is supplied and the information at the provider’s disposal\nfor possible misuse ."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n(41) In certain cases, in particular where the prohibitions are linked to a very specific purpose\nof the system,36 providers may have limited possibilities to integrate other preventive\nand mitigating measures and will have to rely on primarily providing appropriate\ninstructions and information to the deployers and the required human oversight and\nrestricting prohibited use of the system. Where appropriate, such measures may also\ninclude monitoring for compliance with that restriction, depending on the means\nthrough which the AI system is supplied and the information at the provider’s disposal\nfor possible misuse ."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nAny possible monitoring measures to detect misuse should not\namount to a general monitoring of the activities of the deployers and should be in line\nwith Union law. For example, a general-purpose AI system that can recognise or infer emotions should\nnot be used by deployers at the workplaces or in education institutions , unless an\nexception for medical or safety reasons applies. However, the provider may not be in a\nposition to know the specific context in which the emotion recognition functionality of\nthe system will be used and whether an exception to the prohibition in Article 5(1)(f)\nAI Act may apply."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\nAny possible monitoring measures to detect misuse should not\namount to a general monitoring of the activities of the deployers and should be in line\nwith Union law. For example, a general-purpose AI system that can recognise or infer emotions should\nnot be used by deployers at the workplaces or in education institutions , unless an\nexception for medical or safety reasons applies. However, the provider may not be in a\nposition to know the specific context in which the emotion recognition functionality of\nthe system will be used and whether an exception to the prohibition in Article 5(1)(f)\nAI Act may apply."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\nSuch providers may nevertheless explicitly exclude such prohibited\nuse in their terms of use and include appropriate information in the instructions of use\nto guide deployers. They are also expected to take appropriate measures if they become\naware that the system is misused for this specific prohibited purpose by specific\ndeployers, for example, if such misuse is reported or the provider becomes otherwise\naware, which may be the case if the system is directly operated through a platform under\nthe control of the provider and the provider performs checks. 2.8."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\nSuch providers may nevertheless explicitly exclude such prohibited\nuse in their terms of use and include appropriate information in the instructions of use\nto guide deployers. They are also expected to take appropriate measures if they become\naware that the system is misused for this specific prohibited purpose by specific\ndeployers, for example, if such misuse is reported or the provider becomes otherwise\naware, which may be the case if the system is directly operated through a platform under\nthe control of the provider and the provider performs checks. 2.8."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\nInterplay between the prohibitions and other Union law\n(42) The AI Act is a regulation that applies horizontally across all sectors without prejudice\nto other Union legislation, in particular on the protection of fundamental rights,\nconsumer protection, employment, the protection of workers, and product safety.37 The\nAI Act complements such legislation through its preventative and safety logic (AI\nsystems may not be placed on the market or used in a certain way ) and provides\nadditional protection by addressing specific harmful AI practices which may not be\nprohibited by other laws ."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\nInterplay between the prohibitions and other Union law\n(42) The AI Act is a regulation that applies horizontally across all sectors without prejudice\nto other Union legislation, in particular on the protection of fundamental rights,\nconsumer protection, employment, the protection of workers, and product safety.37 The\nAI Act complements such legislation through its preventative and safety logic (AI\nsystems may not be placed on the market or used in a certain way ) and provides\nadditional protection by addressing specific harmful AI practices which may not be\nprohibited by other laws ."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n14\ndeployment (i.e. the use), the AI Act’s prohibitions enable action to be taken against\nharmful practices involving AI at various points in the AI value chain. (43) At the same time, the AI Act does not affect prohibitions that apply where an AI practice\nfalls within other Union law.38 Thus, even where an AI system is not prohibited by the\nAI Act, its use could still be prohibited or unlawful based on other primary or secondary\nUnion law (e.g., because of the failure to respect fundamental rights in a given case ,\nsuch as the lack of a legal basis for the processing of personal data required under data\nprotection law , discrimination prohibited by Union law , etc.)."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n14\ndeployment (i.e. the use), the AI Act’s prohibitions enable action to be taken against\nharmful practices involving AI at various points in the AI value chain. (43) At the same time, the AI Act does not affect prohibitions that apply where an AI practice\nfalls within other Union law.38 Thus, even where an AI system is not prohibited by the\nAI Act, its use could still be prohibited or unlawful based on other primary or secondary\nUnion law (e.g., because of the failure to respect fundamental rights in a given case ,\nsuch as the lack of a legal basis for the processing of personal data required under data\nprotection law , discrimination prohibited by Union law , etc.)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\nThe respect of the\nprohibitions in the AI Act are therefore not a sufficient condition for compliance with\nother Union legislation that remains applicable to providers and deployers of AI\nsystems. For example, AI-enabled emotion recognition systems used in the workplace that are\nexempted from the prohibition in Article 5(1)(f) AI Act, because they are used for\nmedical or safety reasons, remain subject to data protection law and Union and national\nlaw on employment and working conditions, including health and safety at work, which\nmay foresee other restrictions and safeguards in relation to the use of such systems.39\n(44) When specific activities related to the placing on the market or use of AI systems are\nalso covered under other Union legislation, the AI Act aims to ensure the consistent\nimplementation of the different provisions."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nThe respect of the\nprohibitions in the AI Act are therefore not a sufficient condition for compliance with\nother Union legislation that remains applicable to providers and deployers of AI\nsystems. For example, AI-enabled emotion recognition systems used in the workplace that are\nexempted from the prohibition in Article 5(1)(f) AI Act, because they are used for\nmedical or safety reasons, remain subject to data protection law and Union and national\nlaw on employment and working conditions, including health and safety at work, which\nmay foresee other restrictions and safeguards in relation to the use of such systems.39\n(44) When specific activities related to the placing on the market or use of AI systems are\nalso covered under other Union legislation, the AI Act aims to ensure the consistent\nimplementation of the different provisions."}
{"meta": {"section_type": "guideline", "article_ref": "Article 77", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 77]\n\nMoreover, it enables effective cooperation\nbetween the competent authorities responsible for the enforcement of the AI Act and\nthe authorities protecting fundamental rights pursuant to Article 77 AI Act and other\nprovisions of the AI Act. More generally, in accordance with Article 4(3) TEU, the\nvarious authorities concerned are bound to cooperate sincerely when giving effect to\ntheir respective tasks under Union law."}
{"meta": {"section_type": "guideline", "article_ref": "Article 77", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 77]\n\nMoreover, it enables effective cooperation\nbetween the competent authorities responsible for the enforcement of the AI Act and\nthe authorities protecting fundamental rights pursuant to Article 77 AI Act and other\nprovisions of the AI Act. More generally, in accordance with Article 4(3) TEU, the\nvarious authorities concerned are bound to cooperate sincerely when giving effect to\ntheir respective tasks under Union law."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n(45) In the context of the prohibitions , the interplay between the AI Act and Union data\nprotection law is particularly relevant , since AI systems often process information\nrelating to identified or identifiable natural persons (‘personal data’) .40 Depending on\nthe prohibition and the context, the most relevant legal acts in relation to such systems\nare Regulation (EU) 2016/679 on the protection of natural persons with regard to the\nprocessing of personal data and on the free movement of such data (General Data\nProtection Regulation, hereinafter ‘GDPR’), Directive (EU) 2016/680 on the protection\nof natural persons with regard to the pro cessing of personal data by competent\nauthorities for the purposes of the prevention, investigation, detection or prosecution of\ncriminal offences or the execution of criminal penalties, and on the free movement of\nsuch data (Law Enforcement Directive, hereinafter ‘ LED’), and Regulation (EU)\n2018/1725 which lays down data protection rules for the EU Institutions, bodies, offices\nand agencies (hereinafter ‘EUDPR’)."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(45) In the context of the prohibitions , the interplay between the AI Act and Union data\nprotection law is particularly relevant , since AI systems often process information\nrelating to identified or identifiable natural persons (‘personal data’) .40 Depending on\nthe prohibition and the context, the most relevant legal acts in relation to such systems\nare Regulation (EU) 2016/679 on the protection of natural persons with regard to the\nprocessing of personal data and on the free movement of such data (General Data\nProtection Regulation, hereinafter ‘GDPR’), Directive (EU) 2016/680 on the protection\nof natural persons with regard to the pro cessing of personal data by competent\nauthorities for the purposes of the prevention, investigation, detection or prosecution of\ncriminal offences or the execution of criminal penalties, and on the free movement of\nsuch data (Law Enforcement Directive, hereinafter ‘ LED’), and Regulation (EU)\n2018/1725 which lays down data protection rules for the EU Institutions, bodies, offices\nand agencies (hereinafter ‘EUDPR’)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n15\nprotection rules have been clarified by the CJEU and the European Data Protection\nBoard has adopted a series o f guidelines (e.g. on the noti on of ‘profiling’,41 which is\nparticularly relevant for the prohibition in Article 5(1)(d) AI Act, since it uses the same\nnotion). (46) Concerning the prohibitions/restrictions on the use of biometric categorisation systems\nand real-time RBI systems for law enforcement purposes, the AI Act applies as lex\nspecialis to Article 10 LED, thus regulating such use and the processing of biometric\ndata involved in an exhaustive manner.42 In that context, the AI Act is not intended to\nprovide the legal basis for the processing of personal data under Article 8 of Directive\n(EU) 2016/680."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article \n5]\n\nAll other provisions of that Directive apply in addition to the conditions\nset out in the AI Act , in particular for the use of real-time (RBI) systems for law\nenforcement purposes when permitted , subject to the limited exceptions in Article\n5(1)(h) AI Act. More generally, the LED must also be complied with for any processing\nof personal data by competent law enforcement authorities (i.e. competent authorities\nunder Article 3(7) LED) when they process the data for law enforcement purposes."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 2]\n\n(47) In accordance with Article 2(9) AI Act, EU consumer protection and safety legislation\nalso remain fully applicable to AI systems falling within scope of those acts. For example,\n- Social scoring practices by traders (including natural persons acting in a professional\ncapacity in business -to-consumer relations), subject to case -by-case assessment, may\nalso be considered ‘unfair’ and therefore in breach of consumer law ( i.e."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nDirective\n2005/29/EC);\n- The use of an AI system to infer emotions may also have to comply with Regulation\n(EU) 2017/745 (Medical Device Regulation ) if the AI system is used for medical\ndiagnosis or medical treatment purposes. (48) Furthermore, the AI Act applies in conjunction with relevant obligations for providers\nof intermediary services that embed AI systems or models into their services regulated\nby Regulation (EU) 2022/2065 ( ‘the Digital Services Act ’)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 2]\n\nSpecifically, Article 2(5)\nAI Act indicates that the AI Act does not affect the application of the provisions on the\nliability of such providers as set out in Chapter II of the Digital Services Act. (49) In addition, the prohibitions in the AI Act are without prejudice to any liability that the\nprovider or deployer might incur for the harm caused according to applicable Union or\nnational liability laws."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\n23\nBy contrast, a generative AI system that incidentally presents false or misleading\ninformation and hallucinates66 may not be considered to deploy deceptive techniques\nwithin the meaning of Article 5(1)(a) AI Act , taking into account the limitations and\nthe state of the art of generative AI . In particular , this may be the case where the\nprovider of the system has properly informed users about the system’s limitations and\nintegrated appropriate safeguards into the system to minimise such outcomes and\nprovided that the system is not intended for, nor deployed in, sensitive contexts (e.g.,\nhealth, education, elections) where serious harmful consequences are likely to occur\n(see also considerations in section 3.2.3.c) below)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nd) Combination of techniques\n(74) Article 5(1)(a) AI Act applies to subliminal, purposefully manipulative , or deceptive\ntechniques, or to combination s of such techniques that can have a compound impact. As stated above, purposefully manipulative techniques may be also subliminal in\nnature, if they operate beyond the threshold of conscious awareness. (75) Furthermore, when purposefully manipulative and deceptive techniques are applied in\ncombination, this may significantly influence the behaviour of individuals , leading\nthem to make decisions based on unconscious manipulations and false beliefs ."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nThis\ncombination may create a feedback loop where individuals are less likely to question\nor critically evaluate the information received, since the manipulative elements have\nalready primed their cognitive biases and emotional responses. 3.2.2. With the objective or the effect of materially distorting the behaviour of\na person or a group of persons\n(76) A third condition for the prohibition in Article 5(1) (a) AI Act to apply is that the\ndeployed subliminal, purposefully manipulative, or deceptive technique must have ‘the\nobjective, or the effect of materially distorting the behaviour of a person or a group of\npersons’."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nThis implies a substantial impact on the behaviour where a person’s autonomy\nand free choices are undermined, rather than a minor influence. However, intent is not\na necessary requirement , since Article 5(1)(a) AI Act also covers practices that may\nonly have the ‘effect’ of causing material distortion. There should be a\nplausible/reasonably likely causal link between the potential material distortion of the\nbehaviour and the subliminal, purposefully manipulative or deceptive technique\ndeployed by the AI system."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\n82 See in particular Articles 24, 25 and 26 of the Charter. See also United Nations Educational, Scientific and Cultural Organiz ation\n(UNESCO) Recommendation on the Ethics of Artificial Intelligence (2021) which emphasises incl usivity and fairness in AI\ndevelopment and deployment. It calls for special attention to vulnerable groups, including children, older people, and people with\ndisabilities."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n35\n- A therapeutic chatbot aimed to provide mental health support and coping strategies to\npersons with mental disabilities can exploit their limited intellectual capacities to\ninfluence them to buy expensive medical products or nudge them to behave in ways\nthat are harmful to them or other persons. - AI systems can identify women and young girls with disabilities online with sexually\nabusive content and targets them with more effective grooming practices, thus\nexploiting their impairments and vulnerabilities that make them more susceptible to\nmanipulation and abuse and less capable of protecting themselves."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration]\n\n35\n- A therapeutic chatbot aimed to provide mental health support and coping strategies to\npersons with mental disabilities can exploit their limited intellectual capacities to\ninfluence them to buy expensive medical products or nudge them to behave in ways\nthat are harmful to them or other persons. - AI systems can identify women and young girls with disabilities online with sexually\nabusive content and targets them with more effective grooming practices, thus\nexploiting their impairments and vulnerabilities that make them more susceptible to\nmanipulation and abuse and less capable of protecting themselves."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\nBy contrast, AI applications that are not designed in an accessible manner should not\nbe regarded to exploit vulnerabilities of persons with disabilities since they do not\nspecifically target those vulnerabilities, but are simply inaccessible to the persons with\ndisabilities. c) Specific social or economic situation\n(109) The third category of vulnerabilities which the prohibition in Article 5(1) (b) AI Act\nseeks to protect are those due to a specific social or economic situation that is likely to\nmake the persons concerned more vulnerable to exploitation ."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration] [Article: Article 5]\n\nBy contrast, AI applications that are not designed in an accessible manner should not\nbe regarded to exploit vulnerabilities of persons with disabilities since they do not\nspecifically target those vulnerabilities, but are simply inaccessible to the persons with\ndisabilities. c) Specific social or economic situation\n(109) The third category of vulnerabilities which the prohibition in Article 5(1) (b) AI Act\nseeks to protect are those due to a specific social or economic situation that is likely to\nmake the persons concerned more vulnerable to exploitation ."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n‘Specific’ should not be\ninterpreted in this context as a unique individual characteristic, but rather a legal status\nor membership to a specific vulnerable social or economic group. Recital 29 AI Act\ncontains a non-exhaustive list of examples of such situations, such as persons living in\nextreme poverty and ethnic or religious minorities. The category aims to cover , in\nprinciple, relatively stable and long-term characteristics, but transient circumstances,\nsuch as temporary unemployment, over-indebtedness or migration status, may also be\ncovered as a specific soci al or -economic situa tion."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration]\n\n‘Specific’ should not be\ninterpreted in this context as a unique individual characteristic, but rather a legal status\nor membership to a specific vulnerable social or economic group. Recital 29 AI Act\ncontains a non-exhaustive list of examples of such situations, such as persons living in\nextreme poverty and ethnic or religious minorities. The category aims to cover , in\nprinciple, relatively stable and long-term characteristics, but transient circumstances,\nsuch as temporary unemployment, over-indebtedness or migration status, may also be\ncovered as a specific soci al or -economic situa tion."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\nHowever, situations such as\ngrievances or loneliness that may be experienced by any person are not covered, since\nthey are not s pecific from a socio-economic perspective (their exploitation may be\ncovered though under Article 5(1)(a) AI Act). (110) Persons in disadvantaged social or economic situations are usually more vulnerable and\nhave fewer resources and lower digital literacy than the general population , which\nmakes it harder for them to discern or counteract exploitative AI practices."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration] [Article: Article 5]\n\nHowever, situations such as\ngrievances or loneliness that may be experienced by any person are not covered, since\nthey are not s pecific from a socio-economic perspective (their exploitation may be\ncovered though under Article 5(1)(a) AI Act). (110) Persons in disadvantaged social or economic situations are usually more vulnerable and\nhave fewer resources and lower digital literacy than the general population , which\nmakes it harder for them to discern or counteract exploitative AI practices."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article \n5]\n\nArticle\n5(1)(b) AI Act aims to ensure that AI technologies do not perpetuate or exacerbate\nexisting financial and other social inequalities and injustices by exploiting the\nvulnerabilities of those people. For example, an AI-predictive algorithm can be used to target with advertisements for\npredatory financial products people who live in low-income post-codes and are in a dire\nfinancial situation, thus exploiting their susceptibility to such advertisements because\nof possible despair and causing them significant financial harm."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration] [Article: Article \n5]\n\nArticle\n5(1)(b) AI Act aims to ensure that AI technologies do not perpetuate or exacerbate\nexisting financial and other social inequalities and injustices by exploiting the\nvulnerabilities of those people. For example, an AI-predictive algorithm can be used to target with advertisements for\npredatory financial products people who live in low-income post-codes and are in a dire\nfinancial situation, thus exploiting their susceptibility to such advertisements because\nof possible despair and causing them significant financial harm."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[Domain: Healthcare]\n\n39\nCertain AI-enabled differential pricing practices in key services such as insurance that\nexploit the specific soci al or economic situation and provide higher prices to lower\nincome consumers can lead to a significant financial burden to pay more for the same\ncoverage, leaving them vulnerable to shocks.92\n(118) Persons with disabilities also represent a vulnerable group that exploitative and\nmanipulative AI systems may significantly harm."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[Domain: Healthcare]\n\nFor instance, an AI system that uses emotion recognition to support mentally disabled\nindividuals in their daily life may also manipulate them into making harmful decisions,\nlike purchasing products promising unrealistic mental health benefits . This is likely to\nworsen their mental health condition and financially exploit them through the purchase\nof i neffective and expensive products , which is likely to cause them significant\npsychological and financial harms."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[Domain: Healthcare]\n\n(119) Socially or economically disadvantaged individuals are particularly susceptible to AI\nsystems exploiting their financial desperation and precarious social situation and are\noften less informed and digitally literate. For instance, an AI chatbot could target specific socially or economically disadvantaged\ngroups inciting them to commit acts of violence or injuries of other persons, by\nidentifying their heightened susceptibility to certain types of content, fear -based\nnarratives, or exploitative offers."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[Domain: Healthcare] [Article: Article 5]\n\nThe system’s targeted approach exacerbates the\nexisting vulnerabilities of these socio -economically disadvantaged individuals,\ndeepening their challenges . In certain cases this may lead to increased anxiety,\ndepression, feelings of helplessness, social isolation, or self-harm and radicalisation to\na point that reaches the threshold of significant harm under Article 5(1)(b) AI Act. (120) Unlike Article 5(1)(a) AI Act, Article 5(1)(b) AI Act does not explicitly refer to group\nharms, while recital 29 AI Act refers for both pr ohibitions to harms suffered by both\nspecific persons and groups of individuals ."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[Domain: Healthcare] [Article: Article 5]\n\nThe two prohibitions should thus be\ninterpreted in a consistent manner aligned also with the safety logic of the AI Act and\nthe objective of the prohibition in Article 5(1)(b) to protect all individuals belonging to\nthe specific vulnerable groups due to age, disability and specific social or -economic\nsituation. Harms that can be extern alised and affect other persons, even if not directly\naffected by the system, should therefore also be taken into account in the assessment of\nthe significance of the harm under Article 5(1)(b) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[Domain: Healthcare]\n\nFor instance,\n- The AI-enabled exploitation of children’s vulnerabilities may have long-term societal\nimpacts, including increased prevalence of mental health concerns, healthcare costs,\nand reduced productivity due to chronic health issues."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n95 Particularly relevant in this respect is the Judgment of the Court (Grand Chamber) of 4 July 2023, Case C-252/21 Meta Platforms Inc\nand Others v Bundeskartellamt. Although the CJEU finds, inter alia, that the processing of off-service users’ personal data for direct\nmarketing purposes by a large social network platform may be regarded as carried out for a legitimate interest of the controller, this\ncannot be done without consent from a user as a legal basis due to the interests and fundamental rights of such a user, which under\nthe circumstances of that case, in particular the extensive processing, override the interest of that operator in such personalised\nadvertising through which social platforms finance their activities (see the Meta Platforms judgment, paragraphs 115 to 118)."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n96 E.g. Council Directive 2000/43/EC of 29 June 2000 implementing the principle of equal treatment between persons irrespective of\nracial or ethnic origin OJ L 180, 19.7.2000, p. 22 –26; Council Directive 2000/78/EC of 27 November 2000 establishing a general\nframework for equal treatment in employment and occupation OJ L 303, 2.12.2000, p. 16–22; Directive 2006/54/EC of the European\nParliament and of the Council of 5 July 2006 on the implementation of the principle of equal opportunities and equal treatmen t of\nmen and women in matters of employment and occupation (recast), OJ L 204, 26.7.2006, p."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n52\n(158) The second scenario is where the scoring is based on personal or personality\ncharacteristics, which may or may not involve specific social behavioural aspects. ‘Personal characteristics’ may include a variety of information relating to a person, for\nexample sex, sexual orientation or sexual characteristics, gender, gender identity, race,\nethnicity, family situation, address, income, household members, profession,\nemployment or other legal status, performance at work, economic situation, financial\nliquidity, health, personal preferences, interests, reliability, behaviour, location or\nmovement, level of debt , type of car etc.117 ‘Personality characteristics’ should be in\nprinciple interpreted as synonymous with personal characteristics, but may also imply\nthe creation of specific profiles of individuals as personalities ."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\nPersonality\ncharacteristics may be also based on a number of factors and imply a judgement, which\nmay be made by the individuals themselves, other persons, or generated by AI systems. In the AI Act, personality characteristics are sometimes referred to as personality traits\nand characteristics;118 those concepts should be interpreted consistently. (159) ‘Known, inferred or predicted’ personal or personality characteristics are different types\nof information and personal data that need to be distinguished."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n‘Known\ncharacteristics’ are based on information which has been provided to the AI system as\nan input, and which is in most cases verifiable information . By contrast, ‘inferred\ncharacteristics’ are based on information which has been inferred from other\ninformation, with the inference usually made by an AI system. ‘Predicted\ncharacteristics’ are those which are estimated based on patterns with less than 100%\naccuracy."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\nThe concepts of ‘inferred’ (or derived) data are also used in the context of\nprofiling in Union data protection law and may therefore be a source of inspiration for\ninterpreting those concepts used in Article 5(1)(c) AI Act.119 The use of these different\ntypes of data may have different implications for the accuracy and the fairness of the\nscoring practices and therefore may be taken into account, in particular where the\nprocessing is opaque or relies on data points whose accuracy is more difficult to be\nverified. 4.2.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\nThe social score must lead to detrimental or unfavourable treatment in\nunrelated social context s and/or unjustified or disproportionate\ntreatment to the gravity of the social behaviour\na) Causal link between the social score and the treatment\n(160) For the prohibition in Article 5(1)(c) AI Act to apply, the social score created by or with\nthe assistance of an AI system must lead to a detrimental or unfavourable treatment\nfor the evaluated person or group of persons. In other words, the treatment must be the\nconsequence of the score, and the score the cause of the treatmen t."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\n55\nworkplace, etc.122 By contrast, data that is relevant for the allocation of the benefits\nand lawfully collected could be used to determine the risk of fraud , since public\nauthorities pursue a legitimate aim in verifying if social benefits are correctly\nallocated. - A public labour agency uses an AI system to score unemployed individuals based on\nan interview and a n AI-based assessment for determining whether an individual\nshould benefit from state support for employment ."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n55\nworkplace, etc.122 By contrast, data that is relevant for the allocation of the benefits\nand lawfully collected could be used to determine the risk of fraud , since public\nauthorities pursue a legitimate aim in verifying if social benefits are correctly\nallocated. - A public labour agency uses an AI system to score unemployed individuals based on\nan interview and a n AI-based assessment for determining whether an individual\nshould benefit from state support for employment ."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\nThat score is based on relevant\npersonal characteristics, such as age and education, but also variables collected or\ninferred from data and contexts with no apparent connection to the purpose of\nevaluation, such as marital status, health data for chronic diseases, addiction, etc.123\nThese unacceptable scoring practices may be distinguished from l awful practices that\nevaluate persons for specific purpose in compliance with Union and national law , in\nparticular when such laws, in compliance with EU law, specify the data considered as\nrelevant and necessary for the purposes of evaluation (see section 4.3."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\nThat score is based on relevant\npersonal characteristics, such as age and education, but also variables collected or\ninferred from data and contexts with no apparent connection to the purpose of\nevaluation, such as marital status, health data for chronic diseases, addiction, etc.123\nThese unacceptable scoring practices may be distinguished from l awful practices that\nevaluate persons for specific purpose in compliance with Union and national law , in\nparticular when such laws, in compliance with EU law, specify the data considered as\nrelevant and necessary for the purposes of evaluation (see section 4.3."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nout of scope). Scenario 2: Unfavourable or detrimental treatment disproportionate to the social behaviour\n(167) Another alternative scenario under Article 5(1)(c)(ii) AI Act where an AI scoring\nsystem may be prohibited is if the treatment resulting from the score is unjustified or\ndisproportionate to the gravity of the social behaviour. The severity of the impact and\nthe interference with the fundamental rights of the person concerned resulting from the\nsocial scoring compared to the gravity of the social behaviour of the person should\ndetermine whether such treatment is disproportionate for the legitimate aim pursued ,\ntaking into account the general principle of proportionality."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\nout of scope). Scenario 2: Unfavourable or detrimental treatment disproportionate to the social behaviour\n(167) Another alternative scenario under Article 5(1)(c)(ii) AI Act where an AI scoring\nsystem may be prohibited is if the treatment resulting from the score is unjustified or\ndisproportionate to the gravity of the social behaviour. The severity of the impact and\nthe interference with the fundamental rights of the person concerned resulting from the\nsocial scoring compared to the gravity of the social behaviour of the person should\ndetermine whether such treatment is disproportionate for the legitimate aim pursued ,\ntaking into account the general principle of proportionality."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\nThis requires a case-by-\ncase assessment, which should consider all relevant circumstances of the case, as well\nas general ethical considerations and principles for fairness and social justice related to\nthe assessment of the social behaviour and the proportionality of the detrimental\ntreatment. The treatment may also be ‘unjustified’, such as lacking a legitimate aim. Sectoral Union or national legislation setting specific criteria and procedures that\nregulate such potential detrimental or unfavourable treatment may also be relevant as\npart of this assessment."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\nThis requires a case-by-\ncase assessment, which should consider all relevant circumstances of the case, as well\nas general ethical considerations and principles for fairness and social justice related to\nthe assessment of the social behaviour and the proportionality of the detrimental\ntreatment. The treatment may also be ‘unjustified’, such as lacking a legitimate aim. Sectoral Union or national legislation setting specific criteria and procedures that\nregulate such potential detrimental or unfavourable treatment may also be relevant as\npart of this assessment."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nExamples of unjustified or disproportionate treatment compared to the social behaviour\nprohibited under Article 5(1)(c) ii) AI Act\n- A public agency uses an AI system to profile families for early detection of children\nat risk based on criteria such as parental mental health and unemployment, but also\ninformation on parents’ social behaviour derived from multiple contexts."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\nExamples of unjustified or disproportionate treatment compared to the social behaviour\nprohibited under Article 5(1)(c) ii) AI Act\n- A public agency uses an AI system to profile families for early detection of children\nat risk based on criteria such as parental mental health and unemployment, but also\ninformation on parents’ social behaviour derived from multiple contexts."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\n56\nare taken from their families, including in cases of minor transgressions by the parents,\nsuch as occasionally missing doctors’ appointments or receiving traffic fines. - A municipality use s an AI system to score trustworthiness of residents based on\nmultiple data points related to their social behaviour in a variety of contexts. The\ngenerated score for residents considered ‘less trustworthy’ is used for blacklisting, i.e. withdrawal of public benefits, other serious punitive measures, and increased control\nor surveillance."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\nAmong the factors considered in the assessment are insufficient\nvolunteering and minor misbehaviour, such as not returning books to the library on\ntime, leaving rubbish on the street outside the day of collection , and a delay in the\npayment of local taxes. These unacceptable social scoring practices may be distinguished from lawful practices\nthat evaluate persons for a legitimate specific purpose in compliance with Union and\nnational law, in particular where those laws ensure that detrimental or unfavourable\ntreatment is justified and proportionate to the social behaviour (see section 4.3."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nout of\nscope). (168) Both alternatives under Article 5(1) (c)(i) and (ii) AI Act may also be fulfilled\nsimultaneously. Examples of unjustified or disproportionate treatment under Article 5(1)(c)) i) and ii)\nAI Act\n- A tax authorit y uses an AI system to detect child benefit fraud by profiling and\nassigning beneficiaries suspected of fraud to categories such as ‘deliberate intent/gross\nnegligence’ using criteria such as low income, dual nationality, social behaviour, etc."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\nBased on the risk score, a beneficiary’s file is inspected and, in many cases, their\nchildcare benefit ceased, they receive notice to repay the received benefits, and no\nlonger qualify for standard debt collection arrangements. Such scoring causes many\nfamilies to be heavily indebted and lead s to unjust, discriminatory and detrimental\ntreatment of individuals and groups of individuals ,124 driving many families into\nsevere financial hardship."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\n- A public authority uses an AI system to control fraud in the student housing grant\nprocess that considers among the indicators the internet connections, family status or\nlevel of education of beneficiaries as distinguishing factor s for fraud risk, which do\nnot seem relevant, nor justified. - A government introduces a comprehensive AI -based system that monitors and rates\ncitizens based on their behaviour in various aspects of life, such as social interactions,\nonline activit ies, purchasing habits , and punctuality in paying bills ."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n57\nto the social behaviour used to determine the social score (e.g., job opportunities are\ninfluenced by social media activity), while also imposing excessive penalties for\nminor infractions ( e.g., social and financial disadvantages for relatively minor\noffences). These unacceptable social scoring practices may be distinguished from lawful practices\nevaluating persons for legitimate specific purposes that do not fulfil th ese conditions\nand are in compliance with Union and national law, in particular when those laws ensure\nthat detrimental or unfavourable treatment is justified and proportionate and data from\nrelated social contexts is used (see section 4.3."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\nout of scope). (169) The prohibition under Article 5(1)(c) AI Act may also cover cases where awards or\npreferential treatment are given to certain individuals or groups of persons , since this\nimplies less favourable treatment of other individuals (e.g., in cases of support\nemployment programmes, (de-)prioritisation for housing or resettlement). 4.2.3. Regardless of whether provided or used by public or private persons\n(170) As already noted , Article 5(1)(c) AI Act prohibits unacceptable AI -enabled social\nscoring practices regardless of whether the AI system or the score are provided or used\nby public or private persons."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\nWhile scoring in the public sector may have very\nsignificant consequences for people due to an imbalance of power and a dependence on\npublic services, simil arly harmful consequences may also occur in the private sector,\nwhere scoring practices are also increasingly undertaken by companies and other\nentities. For example,\n- An insurance company collects spending and other financial information from a bank\nwhich is unrelated to the determination of eligibility of candidates for life insurance\nand which is used to determine the price of the premium to be paid for such insurance."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\nAn AI system analyses this information and recommends, on that basis, whether to\nrefuse a contract or set higher life insurance premiums for a particular individual or a\ngroup of customers. - A private credit agency uses an AI system to determine the creditworthiness of people\nand decide whether an individual should obtain a loan for housing based on unrelated\npersonal characteristics. These unacceptable social scoring practices may be distinguished from lawful practices\nevaluating persons for spec ific legitimate purposes that do not fulfil these conditions\nand are in compliance with Union and national law, in particular when those laws ensure\nthat detrimental or unfavourable treatment is justified and proportionate and data from\nrelated social contexts is used (see section 4.3."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration]\n\n58\nensuring that only data related to the social context in which the score is used are\nprocessed for the purpose of the evaluation or classification, the system is performing\nas intended, and any resulting detrimental or unfavourable treatment is justified and\nproportionate to the social behaviour. Compliance with applicable legislation and\nappropriate and proportionate safeguards built in the system and applied during its\noperation will help to avoid the prohibition from applying, while enabling the use of AI\nsystems for the evaluation or classification of persons for legitimate and beneficial\npurposes (e.g., to improve the effectiveness of processes, quality of service, safety, etc.)\n(see section 4.3."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration]\n\nout of scope). (172) Compliance with the requirements for high-risk AI systems (e.g. in the area of essential\npublic services and benefits, credit-scoring and creditworthiness assessment, migration\netc.) may also help to ensure that AI systems used for evaluation and classification\npurposes in those high-risk areas do not constitute unacceptable social scoring practices\nthat providers and deployers should consider when implementi ng their respective\nobligations (e.g."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration] [Article: Article 5]\n\non risk management, transparency, data governance, fundamental\nrights impact assessment, human oversight, monitoring, etc.). 4.3. Out of scope\n(173) The prohibition in Article 5(1)(c) AI Act only applies to the scoring of natural persons\nor groups of persons, thus excluding in principle scoring of legal entities where the\nevaluation is not based on personal or personality characteristics or social behaviour of\nindividuals, even if in some cases individuals may be indirectly impacted by the score\n(e.g., all citizens in a municipality in case of allocation of budget)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration] [Article: Article 5]\n\nHowever, if legal\nentities have been evaluated based on an overall score that aggregates the evaluation or\nclassification of a group of natural persons based on their social behaviour or personal\nor personality characteristics and this score directly affects th ose persons (e.g. all\nemployees in a company, students in a specific school whose behaviour has been\nevaluated), the practice may fall within the scope of Article 5(1) (c) AI Act if all other\nconditions are fulfilled. This will depend on a case-by-case assessment."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration] [Article: Article 5]\n\n(174) AI-based social scoring as a ‘probabilistic value ’ and prognosis should also be\ndistinguished from individual ratings by users which assess the quality of a service\n(such as a driver in an online car-sharing platform or a host in an online platform for\naccommodation). Such ratings are the mere aggregation of individual human scores that\ndo not necessarily involve AI, unless the data are combined with other information and\nanalysed by the AI system for evaluating or classifying individuals fulfilling all\nconditions in Article 5(1)(c) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration] [Article: Article 5]\n\n(175) Furthermore, the scoring of natural persons is not at all times prohibited, but only in the\nlimited cases where all of the conditions of Article 5(1)(c) AI Act are cumulatively\nfulfilled, as analysed above . Recital 31 AI Act , in particular mentions that the\nprohibition ‘should not affect lawful evaluation practices of natural persons that are\ncarried out for a spec ific purpose in accordance wi th Union and national law’. For\nexample, credit scoring and risk scoring and underwriting are essential aspects of the\nservices of financial and insurance businesses."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n59\nlegitimate practices (i.e. to improve the quality and efficiency of services , to ensure\nmore efficient claims handling , to perform specific employee evaluations , fraud\nprevention and detection, law enforcement or scoring of users’ behaviour on online\nplatforms), are not per se prohibited, if lawful and undertaken in line with the AI Act\nand other applicable Union law and national law, which must comply with Union law."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(176) In other words, AI systems which evaluate or classify individuals for the purposes of\ngenerating a social score in a lawful manner and for a specific purpose in the related\ncontext as that in which the personal data used for the score were collected are not\nprohibited, provided that any detrimental or unfavourable treatment from using the\nscore is justified and proportionate to the gravity of the social behaviour.125\n(177) Compliance with sectoral Union legislation, such as in the field of credit-scoring, anti-\nmoney laundering, etc., that specifies the type of data that can be used as relevant and\nnecessary for the specific legitimate purpose of evaluation and ensures that the\ntreatment is justified and proportionate to the social behaviour may thus ensure that the\nAI practice falls outside the scope of the prohibition in Article 5(1)(c) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nExamples of legitimate scoring practices in line with Union and national law that are\noutside the scope of Article 5(1)(c) AI Act:\n- Financial credit scoring systems used by creditors or credit information agencies to\nassess a customer’s financial creditworthiness or outstanding debts, providing a credit\nscore or determining their creditworthiness assessment , which are based on the\ncustomer’s income and expenses and other financial and economic circumstances, are\nout of scope of Article 5(1)(c) AI Act if they are relevant for the legitimate purpose of\nthe credit scoring and if they comply with consumer protection laws126 specifying the\ntype of data and the necessary safeguards to ensure the fair treatment of consumers in\ncreditworthiness assessments."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n- Companies have a legitimate interest to evaluate customers for financial fraud and\nthose practices are not affected by the prohibition, if the evaluation is based on relevant\ndata such as transactional behaviour and metadata in the context of the services, past\nhistory and other factors from sources that are objectively relevant to determine the\nrisk of fraud and if the detrimental treatment is justified and proportionate as a\nconsequence of the fraudulent behaviour."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n- Information collected through telematic devices that show that a driver is speeding or\nnot maintaining safe driving practices used by an insurer that offers telematics-based\ntariffs in relation to a policyholder ’s high -risk driving behaviour may be used to\nincrease the premium of that policyholder due to the higher risk of an accident caused\nby that driving behaviour, provided the increase in the premium is proportionate to the\nrisky behaviour of the driver."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\n60\n- The collection and processing of data that is relevant and necessary for the intended\nlegitimate purpose of the AI systems (e.g. hea lth and schizophrenic data collected\nfrom various sources to diagnose patients) is out of scope of Article 5(1)(c) AI Act, in\nparticular because it process relevant and necessary data and typically does not entail\nunjustified detrimental or unfavourable treatment of certain natural persons."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n60\n- The collection and processing of data that is relevant and necessary for the intended\nlegitimate purpose of the AI systems (e.g. hea lth and schizophrenic data collected\nfrom various sources to diagnose patients) is out of scope of Article 5(1)(c) AI Act, in\nparticular because it process relevant and necessary data and typically does not entail\nunjustified detrimental or unfavourable treatment of certain natural persons."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration] [Article: Article 5]\n\n60\n- The collection and processing of data that is relevant and necessary for the intended\nlegitimate purpose of the AI systems (e.g. hea lth and schizophrenic data collected\nfrom various sources to diagnose patients) is out of scope of Article 5(1)(c) AI Act, in\nparticular because it process relevant and necessary data and typically does not entail\nunjustified detrimental or unfavourable treatment of certain natural persons."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article \n5]\n\n- Online platforms profiling users for safety reasons on their services based on data\nwhich is relevant for the context and purpose of assessment is out of scope of Article\n5(1)(c) AI Act , when the evaluation does not result in detrimental treatment that is\ndisproportionate to the gravity of the user’s misbehaviour. - AI-enabled targeted commercial advertising is out of scope , where it is based on\nrelevant data (e.g., users’ preferences), it is done in line with Union law on consumer\nprotection, data protection and digital services, and it does not result in detrimental or\nunfavourable treatment disproportionate to the gravity of the user’s social behaviour\n(e.g., exploitative and unfair differential pricing)."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article \n5]\n\n- Online platforms profiling users for safety reasons on their services based on data\nwhich is relevant for the context and purpose of assessment is out of scope of Article\n5(1)(c) AI Act , when the evaluation does not result in detrimental treatment that is\ndisproportionate to the gravity of the user’s misbehaviour. - AI-enabled targeted commercial advertising is out of scope , where it is based on\nrelevant data (e.g., users’ preferences), it is done in line with Union law on consumer\nprotection, data protection and digital services, and it does not result in detrimental or\nunfavourable treatment disproportionate to the gravity of the user’s social behaviour\n(e.g., exploitative and unfair differential pricing)."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration] [Article: Article \n5]\n\n- Online platforms profiling users for safety reasons on their services based on data\nwhich is relevant for the context and purpose of assessment is out of scope of Article\n5(1)(c) AI Act , when the evaluation does not result in detrimental treatment that is\ndisproportionate to the gravity of the user’s misbehaviour. - AI-enabled targeted commercial advertising is out of scope , where it is based on\nrelevant data (e.g., users’ preferences), it is done in line with Union law on consumer\nprotection, data protection and digital services, and it does not result in detrimental or\nunfavourable treatment disproportionate to the gravity of the user’s social behaviour\n(e.g., exploitative and unfair differential pricing)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\n- AI systems using data collected in refugee camps ( e.g., behavioural compliance) for\ndecisions about resettlement or employment is not affected by the prohibition , given\nthat this data is relevant for the purpose of assessment and provided that the procedures\nunder applicable Union migration law are fulfilled to ensure the treatment is justified\nand proportionate. - AI-enabled scoring by an online shopping platform which offers privileges to users\nwith a strong purchase history and a low rate of product returns, such as a faster returns\napplication process or returnless refunds are out of scope of Article 5(1)(c) AI Act ,\ngiven that the advantages are justified and proportionate to reward positive behaviour\nand other users continue to have access to the standard return process."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n- AI systems using data collected in refugee camps ( e.g., behavioural compliance) for\ndecisions about resettlement or employment is not affected by the prohibition , given\nthat this data is relevant for the purpose of assessment and provided that the procedures\nunder applicable Union migration law are fulfilled to ensure the treatment is justified\nand proportionate. - AI-enabled scoring by an online shopping platform which offers privileges to users\nwith a strong purchase history and a low rate of product returns, such as a faster returns\napplication process or returnless refunds are out of scope of Article 5(1)(c) AI Act ,\ngiven that the advantages are justified and proportionate to reward positive behaviour\nand other users continue to have access to the standard return process."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration] [Article: Article 5]\n\n- AI systems using data collected in refugee camps ( e.g., behavioural compliance) for\ndecisions about resettlement or employment is not affected by the prohibition , given\nthat this data is relevant for the purpose of assessment and provided that the procedures\nunder applicable Union migration law are fulfilled to ensure the treatment is justified\nand proportionate. - AI-enabled scoring by an online shopping platform which offers privileges to users\nwith a strong purchase history and a low rate of product returns, such as a faster returns\napplication process or returnless refunds are out of scope of Article 5(1)(c) AI Act ,\ngiven that the advantages are justified and proportionate to reward positive behaviour\nand other users continue to have access to the standard return process."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\n- AI-evaluation and scoring of individuals by police and other law enforcement\nauthorities that collect data about individuals’ social behaviour from multiple contexts\nare out of scope of Article 5(1)(c) AI Act where those data are relevant for the specific\npurposes of the prevention, detection, prosecution and punishment of criminal\noffences, and where the detrimental treatment is justified and proportionate in\naccordance with substantive and procedural Union and national criminal and police\nlaw."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n- AI-evaluation and scoring of individuals by police and other law enforcement\nauthorities that collect data about individuals’ social behaviour from multiple contexts\nare out of scope of Article 5(1)(c) AI Act where those data are relevant for the specific\npurposes of the prevention, detection, prosecution and punishment of criminal\noffences, and where the detrimental treatment is justified and proportionate in\naccordance with substantive and procedural Union and national criminal and police\nlaw."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration] [Article: Article 5]\n\n- AI-evaluation and scoring of individuals by police and other law enforcement\nauthorities that collect data about individuals’ social behaviour from multiple contexts\nare out of scope of Article 5(1)(c) AI Act where those data are relevant for the specific\npurposes of the prevention, detection, prosecution and punishment of criminal\noffences, and where the detrimental treatment is justified and proportionate in\naccordance with substantive and procedural Union and national criminal and police\nlaw."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\nIt is also relevant to consider in this context the prohibition in Article 5(1)(d) AI\nAct, which imposes additional and more specific conditions for AI -enabled risk\nassessments and prediction s of the likelihood of a person committing a criminal\noffence which must not be solely based on profiling or the assessment of personality\ntraits (see section 5). 4.4. Interplay with other Union legal acts\n(178) Providers and deployers should carefully assess whether other applicable Union and\nnational legislation applies to any particular AI scoring system used in their activities,\nin particular if there is more specific legislation that strictly regulates the types of data\nthat can be used as relevant and necessary for specific evaluation purposes and if there\nare more specific rules and procedures to ensure justified and fair treatment."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nIt is also relevant to consider in this context the prohibition in Article 5(1)(d) AI\nAct, which imposes additional and more specific conditions for AI -enabled risk\nassessments and prediction s of the likelihood of a person committing a criminal\noffence which must not be solely based on profiling or the assessment of personality\ntraits (see section 5). 4.4. Interplay with other Union legal acts\n(178) Providers and deployers should carefully assess whether other applicable Union and\nnational legislation applies to any particular AI scoring system used in their activities,\nin particular if there is more specific legislation that strictly regulates the types of data\nthat can be used as relevant and necessary for specific evaluation purposes and if there\nare more specific rules and procedures to ensure justified and fair treatment."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration] [Article: Article 5]\n\nIt is also relevant to consider in this context the prohibition in Article 5(1)(d) AI\nAct, which imposes additional and more specific conditions for AI -enabled risk\nassessments and prediction s of the likelihood of a person committing a criminal\noffence which must not be solely based on profiling or the assessment of personality\ntraits (see section 5). 4.4. Interplay with other Union legal acts\n(178) Providers and deployers should carefully assess whether other applicable Union and\nnational legislation applies to any particular AI scoring system used in their activities,\nin particular if there is more specific legislation that strictly regulates the types of data\nthat can be used as relevant and necessary for specific evaluation purposes and if there\nare more specific rules and procedures to ensure justified and fair treatment."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n62\n(184) Article 5(1)(d) AI Act prohibits AI systems assessing or predicting the risk of a natural\nperson committing a criminal offence based solely on profiling or assessing personality\ntraits and characteristics. (185) The provision indicates, in its last phrase, that the prohibition does not apply if the AI\nsystem is used to support the human assessment of the involvement of a person in a\ncriminal activity, which is already based on objective and verifiable facts directly linked\nto that activity."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nSuch AI systems that fall outside the scope of the prohibition intended\nto be used by law enforcement authorities, or on their behalf, or by Union institutions,\nbodies, offices or agencies in support of law enforcement authorities, for assessing the\nrisk of a natural person offending or re-offending not solely on the basis of profiling, or\nthe assessment of personality traits and characteristics or past criminal behaviour are\nclassified as ‘high -risk’ AI systems (Annex III, point 6, letter (d) AI Act) and must\ncomply with all relevant requirements and obligations under the AI Act. 5.1."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nMain concepts and components of the prohibition\nArticle 5(1)(d) AI Act provides\nThe following AI practices shall be prohibited:\nd) the placing on the market, the putting into service for this specific purpose, or the\nuse of an AI system for making risk assessments of natural persons in order to assess\nor predict the risk of a natural person committing a criminal offence , based solely on\nthe profiling of a natural person or on assessing their personality traits and\ncharacteristics; this prohibition shall not apply to AI systems used to support the human\nassessment of the involvement of a person in a criminal activity, which is already based\non objective and verifiable facts directly linked to a criminal activity;\n(187) Several cumulative conditions must be fulfilled for the prohibition in Article 5(1)(d) AI\nAct to apply:\n(i) The practice must constitute the ‘placing on the market’, ‘the putting into service\nfor this specific purpose’ or the ‘use’ of an AI system."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n63\n(188) For the prohibition to apply all three conditions must be simultaneously fulfilled. The\nfirst condition, i.e. the placing on the market, putting into service or use of the AI\nsystem, has been already analysed in section 2.3. The prohibition, therefore, applies to\nboth providers and deployers of AI systems, each within their respective responsibilities\nnot to place on the market, put into service , or use such AI systems for this specific\npurpose. The other two conditions for the prohibition to apply are analysed below. 5.2.1."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nAssessing the risk or predicting the likelihood of a person committing a\ncrime\n(189) Risk assessments to assess or predict the risk of an individual committing a criminal\noffence are often referred to as individual ‘crime prediction’ or ‘crime forecasting’. While there is no general ly agreed definition of ‘crime prediction’ or ‘crime\nforecasting’,129 these terms refer in general to a v ariety of advanced AI technologies\nand analytical methods applied to large amount of often historical data (including socio-\neconomic data, but also police records, etc .) which, in combination with criminology\ntheories, are used to forecast crime as a basis to inform police and law enforcement\nstrategies and action to combat, control, and prevent crime.130\n(190) Crime prediction AI systems identify patterns within historical data, associating\nindicators with the likelihood of a crime occurring, and then generate risk scores as\npredictive outputs."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nFor example, such systems may be used for planning police task\nforces, for monitoring high -risk situatio ns, and for conducting controls of persons\npredicted as likely (re-)offenders. Such systems bring opportunities for law\nenforcement authorities, especially those with scarce resources, increasing efficiency,\nand enabling a proactive approach for detecting, deterring , and anticipating criminal\noffences.131 However, such use of historical data on crimes committed to predict other\npersons’ future behaviour may perpetuate or even reinforce bias es, and may result in\ncrucial individual circumstances being ‘overlooked’ when these circumstances are not\npart of the data set or considered in the algorithms on which the particular AI system\noperates."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nThis may also undermine public trust in law enforcement and the justice\nsystem in general.132\n(191) Such risk assessments and predictions are, in principle, forward-looking and concern\nfuture criminal offences (not yet committed ) or crimes that are assessed as a risk of\nbeing committed at the moment , including in cases of an attempt or preparatory\nactivities undertaken to commit a criminal offence."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n129 For example, see systems mentioned in the EU Fundamental Rights Agency handbook, such as the Criminality Awareness System\n(CAS) in the Netherlands and Precobs in Germany and Switzerland, Handbook, 2018, p.138. Preventing unlawful profiling today\nand in the future: a guide, Handbook, 2018, p.138. 130 See Europol, AI and policing The benefits and challenges of artificial intelligence for law enforcement, An Observatory Report from\nthe Europol Innovation Lab, 23 September 2024 . See also F."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nYang, ‘ Predictive Policing ’ in Oxford Research Encyclopedia,\nCriminology and Criminal Justice, Oxford University Press, 2019. 131 For example, OxRec (Dutch Probation Office, ‘Reclassering Nederland ’) Prediction of violent reoffending in p risoners and\nindividuals on probation: a Dutch validation study (OxRec) - PMC (nih.gov)\n132 See for instance EU Fundamental Rights Agency (8 December 2022) Bias in algorithms - Artificial intelligence and discrimination |\nEuropean Union Agency for Fundamental Rights."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n66\nparticular, the existence of certain pre -established objective and verifiable facts may\njustify that conclusion. For example,\n- A law enforcement authority uses an AI system to predict criminal behaviour for\ncrimes such as terrorism solely based on individuals’ age, nationality, address, type of\ncar, and marital status . With that system, individuals are deemed more likely to\ncommit future offences that they have not yet committed solely based on their personal\ncharacteristics. Such a system may be assumed to be prohibited under Article 5(1)(d)\nAI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n- National tax authorities use an AI predictive tool to review all taxpayers’ tax returns\nto predict potential criminal tax offences to identify cases requiring further\ninvestigation. This is done solely on the basis of the profile built by the AI system,\nwhich uses for its assessment personal ity traits, such as double nationality, place of\nbirth, number of children, and opaque variables, especially inferred information that\nis predictive and therefore non -objective and hard to verify."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nSuch a system will\nnormally fall under the prohibition of Arti cle 5(1)(d) AI Act, since there is no\nreasonable suspicion of the involvement of a particular person in a criminal activity\nor other objective and verifiable facts linking that to that criminal activity. This is also\nan example that falls within the scope of social scoring prohibited under Article 5(1)(c)\nAI Act involving unfavourable treatment with data from unrelated social contexts."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n- A police department uses AI -based risk assessment tool to assess the risk of young\nchildren and adolescents being involv ed in ‘future violent and property offending ’. The system assesses children based on their relationships with other people and their\nsupposed risk levels, meaning that children may be deemed at a higher risk of\noffending simply by being linked to another individual with a high -risk assessment,\nsuch as a sibling or a friend. The parents’ risk levels may also impact a child’s risk\nlevel."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nThe risk assessments result in police ‘registering’ these children in their systems,\nmonitoring them with addition al inspections, and referring them to youth ‘care’\nservices. Such a system is also likely to fall under the prohibition of Article 5(1)(d) AI\nAct. 5.2.3. Exclusion of AI systems to support the human assessment based on\nobjective and verifiable facts directly linked to a criminal activity\n(203) Article 5(1)(d) AI Act provides, in its last phrase, that the prohibition does not apply to\nAI systems used to support the human assessment of the involvement of a person in\na criminal activity, which is already based on objective and verifiable facts directly\nlinked to a criminal activity."}
{"meta": {"section_type": "guideline", "article_ref": "Article 14", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 14]\n\n67\n(204) Where the system falls within the scope of the exclusion and is therefore not prohibited,\nit will be classified as a high-risk AI system (as referred to in Annex III, point 6(d), AI\nAct) if intended to be used by law enforcement authorities or on their behalf and\ntherefore subject to the requirements and safeguards, including human oversight\n(Article 14 and Article 26 AI Act). These requirements include that the human oversight\nmust be assigned to persons with the necessary competence, training and authority who\nshould be able to properly understand the capabilities and limitations of the AI system,\ncorrectly interpret its output and address the risk of automation bias."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nThose persons\nshould have clear procedures, training and the necessary competence and authority to\nmeaningfully assess the outputs of the AI system . In this specific case, their human\nassessment should ensure that any AI prediction or assessment of the risk of a person\ncommitting a crime is based on objective and verifiable facts linked to a criminal\nactivity. Those persons should also intervene in order to avoid negative consequences\nor risks, or stop the use of the AI system if it does not perform as intended."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(205) Furthermore, the concept of ‘human intervention’ has been subject to CJEU case -law,\nin particular in the context of solely automated decision-making predicting the risk of\nair passengers being involved in serious crimes. That case-law may also be relevant for\nthe application of the concept of ‘human assessment’ as used in Article 5(1)(d) AI Act. In the Ligue des droits humains case,138 the CJEU examined the legality of the use of\nan advanced AI system for the systematic processing of passenger name record (PNR)\ndata of air travellers to assess their likelihood of being involved in terrorism and other\nserious crimes."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nThe CJEU interpreted the rule in Directive (EU) 2016/681 ( ‘PNR Directive ’) that\nprohibits adverse legal decisions based solely on automated processing and required\nindividual human assessment and review for any positive matches by non-automated\nmeans to identify false positives and ensure non-discriminatory results. According to the CJEU, that human assessment , subject to which any results of\nautomated processing of PNR data must be based , must rely on objective criteria to\nevaluate whether a positive match concerns someone who might be involved in this\nspecific case in terrorist offenses or serious crime, and to ensure the non-discriminatory\nnature of automated processing."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(206) As to the content of the excl usion, one of its central elements is that the AI system is\nused to support human assessment, rather than involving the AI system itself making\nthe risk assessment as occurs in the situations covered by the prohibition. However, for\nthe exclusion to apply, that human assessment must, in addition, already be based on\nobjective and verifiable facts directly linked to a criminal activity. 5.2.4."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n68\nprohibition in Article 5(1)(d) AI Act in some cases . That follows from the fact that,\nbased on its wording, the prohibition does not apply exclusively to law enforcement\nauthorities. Moreover, otherwise the prohibition might be easily circumvented, which\nwould call into question its effectiveness. (208) That being so, the prohibition may be assumed to apply, in particula r, when private\nactors are entrusted by law to exercise public authority and public powers for the\nprevention, investigation, detection or prosecution of criminal offences or the execution\nof criminal penalties.139 Private actors may be also explicitly requested on a case-by-\ncase basis to act on behalf of law enforcement authorities and carry out individual crime\nrisk predictions."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nIn those cases, the activities of those private actors could also fall\nwithin the scop e of the prohibition , if the applicable conditions are fulfilled and the\nexclusion does not apply. For example, a private company providing advanced AI-based crime analytic software\nmay be asked by a law enforcement authority to analyse a large amount of data from\nmultiple sources and databases , such as national registers, banking transactions,\ncommunication data, geo-spatial data, etc., to predict or assess the risk of individuals as\npotential offenders of human trafficking offences."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nIf all the criteria for Art icle 5(1)(d)\nare met, such a use case could be prohibited. (209) Furthermore, the prohibition may apply to private entities assessing or predicting the\nrisk of a person of committing a crime where this is objectively necessary for\ncompliance with a legal obligation to which that private operator is subject to assess or\npredict the risk of persons committing specific criminal offences ( e.g., in case of anti-\nmoney laundering, terrorism financing)."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nFor example, a banking institution has an obligation under Union anti -money\nlaundering legislation140 to screen and profile customers for money -laundering\noffences. If the bank uses an AI system to fulfil its obligations, that should be done\nbased only on the data a s specified in that law which are objective and verifiable to\nensure that the person s singled out as suspect are reasonably likely to commit anti -\nmoney laundering offences."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nThe predictions must also be subject to human assessment\nand verification in compliance with that legislation 141 in order to ensure the accuracy\nand appropriateness of such assessments. Compliance with that legislation will ensure\nthat the use of individual crime prediction AI system for anti -money laundering\npurposes fall outside the scope of the prohibition in Article 5(1)(d) AI Act. (210) However, having regard to the focus on risk assessments relating specifically and\nexclusively to the commission of criminal offences that is evident from the wording of\nthe prohibition as well as to the purpose of the prohibition as explained in Recital 42, if\na private entity profiles customers for its regular business operations and safety or to\nprotect its financial interests (e.g."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n69\nof assessing or predicting the risk of the customer committing a specific criminal\noffence, the activities of the private entities should not be considered to fall under the\nscope of the prohibition of Article 5(1)(d) AI Act. (211) In other words, in the absence of private parties having been entrusted by law certain\nspecific law enforcement tasks, acting on behalf of law enforcement authorities or being\nsubject to specific legal obligations as described above, the use of AI systems for\nmaking risk assessments in the context of private entities’ ordinary course of business\nand with the aim of protecting their own private interests, whilst the fact that those risk\nassessments may relate to the risk of criminal offences being committed merely as a\npurely accidental and secondary circumstance, is not deemed to be covered by the\nprohibition."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n5.3. Out of scope\n5.3.1. Location-based or geospatial predictive or place -based crime\npredictions\n(212) Location-based or geospatial or place-based crime predictions is based on the place or\nlocation of crime or the likelihood that in those areas a crime would be committed. In\nprinciple, such policing does not involve an assessment of a specific individual . They\ntherefore fall outside the scope of the prohibition."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nExamples of location-based or geospatial predictive or place-based crime predictions\n- AI-based predictive policing system provides a score of the likelihood of criminality\nin different areas in a city based on previous criminality rates by area and other\nsupporting information such as street maps, to highlight elevated risk of specific types\nof criminalities e.g., burglaries, knife crime etc. and help law enforcement authorities\ndetermine where to deploy less or more police patrols/presence to carry out\ncommunity policing to disrupt and deter criminal activity."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n- A customs authority uses AI risk analytic tools to predict the likelihood of the location\nof narcotics or illicit goods, for example on the basis of known trafficking routes. - A police department uses AI-driven systems to detect and locate gunshots in real time. The system employs acoustic sensors in urban areas to identify gunfire sounds and\ntriangulate their location, providing officers with actionable data to aid detection and\ninvestigation of crimes."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(213) It may , however, not always be evident how to distinguish location-based crime\npredictive systems from individual predictive systems assessing the risk of a person\ncommitting a crime . To the extent that an AI system carries out location -based\npredictive policing and then considers the risk score of the location as an aspect in the\nprofiling of a person , that system should be considered person -based and in principle\ncovered by Article 5(1) (d) AI Act , although it may f all outside the scope of the\nprohibition on other grounds."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n71\ntrained on data including past criminal history of individuals in similar cases, as well\nas factors such as age group, social behaviour, income, and employment status. - AI system is used to support the assessment of a human officer to assess the risk of an\nindividual serving a non-custodial sentence violating release conditions or absconding\nbased on past criminal behaviours and objective facts that give grounds for suspicion\nsuch as adherence to conditions of release, psychological ass essment outcomes and\nrecommendations from other community services the individual may be using."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\nBased\non this information, the officer decides whether to maintain the status quo or revise\nthe conditions of release. - AI systems used by custom authorities to assess the risk of goods entering the EU not\ncomplying with the legislation applicable at the border (e.g., which may include bans\non import of illicit drugs, export sanctions contravention or other illegal activity) to\nidentify situations w here a customs control should be carried out."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\nThe AI system\nassesses objective and verifiable information provided to the customs related to the\ngoods and their supply chains (e.g., nature and value of the goods, container number,\nmeans of transport for co ncealment of other goods, prior knowledge relating to the\ncompliance of goods of the particular description and origin with requirements for\ntheir importation to or exportation from the Union)."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\nIn certain cases, it may also\nprocess information about the pr ior involvement of the importer or exporter in\nirregularities related to import of goods, their affiliation to criminal organizations or a\ncriminal record for drug trafficking. Such systems are out of scope of the prohibition\nbecause any prediction of a likelihood of a natural person to be involved in an import\nor export of illicit goods is not solely based on profiling, but on objective and\nverifiable information related to the goods and the importer or exporter’s prior\ninvolvement in criminal activity and subject to a human review to determine whether\nor not the situation requires a customs control or risk mitigation action."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\n5.3.3. AI systems used for crime predictions and assessments in relation to\nlegal entities\n(215) The prohibition in Article 5(1)(d) AI Act applies only to individual predictions and risk\nassessments of natural persons, thus typically excluding crime predictive systems that\nprofile legal entities such as companies or non-governmental organisations. For example,\n- A tax or customs authority is using an AI system to analyse large amounts of data on\ntransactions and tax declarations and customs data of companies for assessing the risk\nof a company committing tax or customs fraud constituting a criminal offence."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\n- AI systems used to assist customs authorities to help identify situations where an\ninstruction not to send illicit goods to the EU should be issued to legal entities. (216) At the same time, there may be borderline cases where a natural person acts via a legal\nentity as a ‘sole tr ader’ or as an independent profession al (e.g., a lawyer) ."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n72\nconditions are fulfilled , since the AI system profiles a specific natural person and\nassesses or predicts their risk of committing a criminal offence, even if this is done for\npurposes in relation to the commercial activity undertaken by the natural person. 5.3.4. AI systems used for individual predictions of administrative offences\n(217) The prohibition in Article 5(1)(d) AI Act applies only for the prediction of criminal\noffences, thus excluding administrative offences from its scope , the prosecution of\nwhich is, in principle, less intrusive for the fundamental rights and freedoms of people."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nFor example, a public authority using AI in the context of an administrative\ninvestigation to assess the risk of potential offenders of committing minor offences\n(such as petty traffic offences ) or irregularities in tax, procurement or expenditure\nprocesses would not fall within the scope of the prohibition in Article 5(1)(d) AI Act,\neven in cases where information might be gathered for possible involvement of the\nnatural persons in criminal offences as a result of the administrative investigations and\nchecks."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(218) Whether an offence is administrative or criminal in nature may depend on Union or\nnational law. For offences that are not directly regulated by Union law , the national\nqualification of the offence is subject to scrutiny by CJEU since ‘criminal offence’ is a\nconcept that has autonomous meaning within the EU law and should be interpreted\nconsistently across Member States . The CJEU has conclu ded, in a different context,\nthat the classification of the offences by the Member States is not conclusive in that\nrespect.142 Relevant criteria used to assess the nature of the offence (criminal or not)\nmay be found in relevant case -law of the CJEU and the European Court of Human\nRights (ECtHR).143\n5.4."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nInterplay with other Union legal acts\n(219) The interplay of the prohibition in Article 5(1)(d) AI Act with the LED and GDPR is\nrelevant when assessing the lawfulness of personal data processing under Union data\nprotection law, such as the GDPR and the LED. In particular, Article 5(1)(d) AI Act\nimposes a specific prohibition for law enforcement authorities, other public authorities\nand private entities falling within the scope of the prohibition to assess or predict the\nrisk of a natural person committing a criminal offence, based solely on the profiling of\na natural person or on assessing their personality traits and characteristics."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(220) The interplay of the prohibition in Article 5(1)(d) AI Act with Directive (EU) 2016/343\non the presumption of innocence is also relevant, since both acts are concerned - directly\nin the case of the Directive and indirectly in the case of the AI Act (see its Recital 42),\nwith the fundamental right to be presumed innocent until proven guilty according to\nlaw.144 While the Directive applies from the moment that a person is suspected or\naccused of having committed a criminal offence,145 the AI Act has a broader scope of\napplication and applies already at the stage of prediction and crime prevention before a\nformal criminal investigation is opened against a particular person and even in cases\nwhen such predictions and risk assessments are made by private actors falling within\nthe scope of Article 5(1)d) AI Act and not by competent law enforcement authorities,\nincluding judicial authorities."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(221) Even in cases where the prohibition in Article 5(1) (d) AI Act does not apply, it is\nimportant to emphasize that applicable Union and national law remains fully applicable,\nincluding in particular data protection, criminal procedural and police law and\nsafeguards that may further restrict or impose additional conditions on the use of\nindividual crime predictive AI systems. 6. ARTICLE 5(1)(E) AI ACT - UNTARGETED SCRAPING OF FACIAL IMAGES\n(222) Article 5(1)(e) AI Act prohibits the placing on the market, putting into service for this\nspecific purpose, or the use of AI systems that create o r expand facial recognition\ndatabases through the untargeted scraping of facial images from the Internet or CCTV\nfootage."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n6.1. Rationale and objectives\n(223) The untargeted scraping of facial images from the internet and from CCTV footage\nseriously interferes with individuals’ rights to privacy and data protection and deny\nthose individuals the right to remain anonymous. Recital 43 AI Act therefore justifies\nthe prohibition established in Article 5(1)(e) AI Act based on the ‘feeling of mass\nsurveillance’ and the risks of ‘gross violations of fundamental rights, including the right\nto privacy’. 6.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\n76\n(234) The prohibition in Article 5(1)(e) AI Act does not apply to the untargeted scraping of\nbiometric data other than facial images (such as voice samples). The prohibition does\nalso not apply where no AI systems are involved in the scraping. Facial image databases\nthat are not used for the recognition of persons are also out of scope , such as facial\nimage databases used for AI model training or testing purposes, where the persons are\nnot identified."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\n(235) The prohibition in Article 5(1)(e) AI Act does not apply to AI systems which harvest\nlarge amounts of facial images from the internet to build AI models that generate new\nimages about fictitious persons because such systems would not result in the recognition\nof real persons. Such AI systems could fall under the transparency requirements of\nArticle 50 AI Act. (236) The prohibition in Article 5(1)(e) AI Act covers AI systems used to create or expand\nfacial recognition databases."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nWhen it comes to existing facial databases built up prior\nto the entry into application of the prohibition, which are not further expanded through\nAI-enabled untargeted scraping, those databases and their use must comply with the\napplicable Union data protection rules. (237) The prohibition in Article 5(1)(e) AI Act is targeted at the creation or expansion of\nfacial recognition databases. The concrete act of biometric identification is subject to\nspecific rules in the AI Act and other relevant Union legislation. 6.4."}
{"meta": {"section_type": "guideline", "article_ref": "ARTICLE 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: ARTICLE 5]\n\nInterplay with other Union legal acts\n(238) In relation to Union data protection law, the untargeted scraping of the internet or CCTV\nmaterial to build -up or expand face recognition databases , i.e. the processing of\npersonal data (collection of data and use of databases) would be unlawful and no legal\nbasis under the GDPR, EUDPR and the LED could be relied upon. 7. ARTICLE 5(1)(F) AI ACT EMOTION RECOGNITION\n(239) Article 5(1)(f) AI Act prohibits AI systems to infer emotions of a natural person in the\nareas of workplace and education institutions, except where the use of the system is\nintended for medical or safety reasons."}
{"meta": {"section_type": "guideline", "article_ref": "Article 50", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 50]\n\nEmotion recognition systems that do not fall\nunder the prohibition are considered high-risk pursuant to point (1)(c) of Annex III AI\nAct. Article 50(3) AI Act lays down certain transparency requirements for the use of\nemotion recognition systems. 7.1. Rationale and objectives\n(240) Emotion recognition technology is quickly evolving and comprehends different\ntechnologies and processing operations to detect, collect, analyse, categori se, re act,\ninteract and learn emotions from persons. Such technology is also referred to as ‘affect\ntechnology’."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\n77\nadvertising and neuromarketing ;148 in the entertainment industry , for example to\nprovide personalised recommendations or to predict reactions to movies ; in medicine\nand healthcare, for example to detect depression, for suicide prevention or to detect\nautism, in education, for example to monitor attention or engagement of learners (pupils\nand students at different ages) ; in employment , for example to accompany the\nrecruitment process, to monitor emotions or boredom of employees, but also well-being\napplications for ‘making workers happier’ ;149 for law enforcement and public safety,\nfor example with lie detectors or emotion screening at big events ; and for many other\npurposes."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n77\nadvertising and neuromarketing ;148 in the entertainment industry , for example to\nprovide personalised recommendations or to predict reactions to movies ; in medicine\nand healthcare, for example to detect depression, for suicide prevention or to detect\nautism, in education, for example to monitor attention or engagement of learners (pupils\nand students at different ages) ; in employment , for example to accompany the\nrecruitment process, to monitor emotions or boredom of employees, but also well-being\napplications for ‘making workers happier’ ;149 for law enforcement and public safety,\nfor example with lie detectors or emotion screening at big events ; and for many other\npurposes."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[Domain: Healthcare]\n\n77\nadvertising and neuromarketing ;148 in the entertainment industry , for example to\nprovide personalised recommendations or to predict reactions to movies ; in medicine\nand healthcare, for example to detect depression, for suicide prevention or to detect\nautism, in education, for example to monitor attention or engagement of learners (pupils\nand students at different ages) ; in employment , for example to accompany the\nrecruitment process, to monitor emotions or boredom of employees, but also well-being\napplications for ‘making workers happier’ ;149 for law enforcement and public safety,\nfor example with lie detectors or emotion screening at big events ; and for many other\npurposes."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n77\nadvertising and neuromarketing ;148 in the entertainment industry , for example to\nprovide personalised recommendations or to predict reactions to movies ; in medicine\nand healthcare, for example to detect depression, for suicide prevention or to detect\nautism, in education, for example to monitor attention or engagement of learners (pupils\nand students at different ages) ; in employment , for example to accompany the\nrecruitment process, to monitor emotions or boredom of employees, but also well-being\napplications for ‘making workers happier’ ;149 for law enforcement and public safety,\nfor example with lie detectors or emotion screening at big events ; and for many other\npurposes."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\n(241) Emotion recognition is frequently doubted as to its effectiveness or as to its accuracy.150\nRecital 44 AI Act explains that there are ‘serious concerns about the scientific basis of\nAI systems aiming to identify or infer emotions, particularly as expression of emotions\nvary considerably across cultures and situations, and even within a single individual. Among the key shortc omings of such systems are the limited reliability, the lack of\nspecificity and the limited generalisability.’ It further explains that emotion recognition\ncan lead to ‘discriminatory outcomes and can be intrusive to the rights and freedoms of\nthe concerned persons’, in particular the rights to privacy, human dignity and freedom\nof thought."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n(241) Emotion recognition is frequently doubted as to its effectiveness or as to its accuracy.150\nRecital 44 AI Act explains that there are ‘serious concerns about the scientific basis of\nAI systems aiming to identify or infer emotions, particularly as expression of emotions\nvary considerably across cultures and situations, and even within a single individual. Among the key shortc omings of such systems are the limited reliability, the lack of\nspecificity and the limited generalisability.’ It further explains that emotion recognition\ncan lead to ‘discriminatory outcomes and can be intrusive to the rights and freedoms of\nthe concerned persons’, in particular the rights to privacy, human dignity and freedom\nof thought."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[Domain: Healthcare]\n\n(241) Emotion recognition is frequently doubted as to its effectiveness or as to its accuracy.150\nRecital 44 AI Act explains that there are ‘serious concerns about the scientific basis of\nAI systems aiming to identify or infer emotions, particularly as expression of emotions\nvary considerably across cultures and situations, and even within a single individual. Among the key shortc omings of such systems are the limited reliability, the lack of\nspecificity and the limited generalisability.’ It further explains that emotion recognition\ncan lead to ‘discriminatory outcomes and can be intrusive to the rights and freedoms of\nthe concerned persons’, in particular the rights to privacy, human dignity and freedom\nof thought."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(241) Emotion recognition is frequently doubted as to its effectiveness or as to its accuracy.150\nRecital 44 AI Act explains that there are ‘serious concerns about the scientific basis of\nAI systems aiming to identify or infer emotions, particularly as expression of emotions\nvary considerably across cultures and situations, and even within a single individual. Among the key shortc omings of such systems are the limited reliability, the lack of\nspecificity and the limited generalisability.’ It further explains that emotion recognition\ncan lead to ‘discriminatory outcomes and can be intrusive to the rights and freedoms of\nthe concerned persons’, in particular the rights to privacy, human dignity and freedom\nof thought."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nThis plays an important role in asymmetric relationships especially in the\ncontext of the workplace and education and training institutions, where both workers\nand students are in particularly vulnerable position s. At the same time, emotion\nrecognition in specific use contexts, such as for safety and medical care (e.g., health\ntreatment and diagnosis) has benefits.151\n7.2. Main concepts and components of the prohibition\nArticle 5(1) (f) AI Act provides:\nThe following AI practices shall be prohibited:\nf) the placing on the market, the putting into service for this specific purpose, or the use\nof AI systems to infer emotions of a natural person in the areas of workplace and\neducation institutions, except where the use of the AI system is intended to be put in\nplace or into the market for medical or safety reasons."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\nThis plays an important role in asymmetric relationships especially in the\ncontext of the workplace and education and training institutions, where both workers\nand students are in particularly vulnerable position s. At the same time, emotion\nrecognition in specific use contexts, such as for safety and medical care (e.g., health\ntreatment and diagnosis) has benefits.151\n7.2. Main concepts and components of the prohibition\nArticle 5(1) (f) AI Act provides:\nThe following AI practices shall be prohibited:\nf) the placing on the market, the putting into service for this specific purpose, or the use\nof AI systems to infer emotions of a natural person in the areas of workplace and\neducation institutions, except where the use of the AI system is intended to be put in\nplace or into the market for medical or safety reasons."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[Domain: Healthcare] [Article: Article 5]\n\nThis plays an important role in asymmetric relationships especially in the\ncontext of the workplace and education and training institutions, where both workers\nand students are in particularly vulnerable position s. At the same time, emotion\nrecognition in specific use contexts, such as for safety and medical care (e.g., health\ntreatment and diagnosis) has benefits.151\n7.2. Main concepts and components of the prohibition\nArticle 5(1) (f) AI Act provides:\nThe following AI practices shall be prohibited:\nf) the placing on the market, the putting into service for this specific purpose, or the use\nof AI systems to infer emotions of a natural person in the areas of workplace and\neducation institutions, except where the use of the AI system is intended to be put in\nplace or into the market for medical or safety reasons."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nThis plays an important role in asymmetric relationships especially in the\ncontext of the workplace and education and training institutions, where both workers\nand students are in particularly vulnerable position s. At the same time, emotion\nrecognition in specific use contexts, such as for safety and medical care (e.g., health\ntreatment and diagnosis) has benefits.151\n7.2. Main concepts and components of the prohibition\nArticle 5(1) (f) AI Act provides:\nThe following AI practices shall be prohibited:\nf) the placing on the market, the putting into service for this specific purpose, or the use\nof AI systems to infer emotions of a natural person in the areas of workplace and\neducation institutions, except where the use of the AI system is intended to be put in\nplace or into the market for medical or safety reasons."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\n78\n(i) The practice must constitute the ‘placing on the market’, ‘the putting into service\nfor this specific purpose’ or the ‘use’ of an AI system;\n(ii) AI system to infer emotions;152\n(iii) in the area of the workplace or education and training institutions; and\n(iv) excluded from the prohibition are AI systems intended for medical or safety\nreasons. (243) For the prohibition to apply all four conditions must be simultaneously fulfilled. The\nfirst element, i.e. the placing on the market, putting into service or use of the AI system,\nhas already been analysed in sectio n 2.3.."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 3]\n\nThe prohibition, therefore, applies to both\nproviders and deployers of AI systems, each within their respective responsibilities, not\nto place on the market, put into service or use such AI systems. The other conditions\nrelated to the prohibition are further described and analysed below. 7.2.1. AI systems to infer emotions\na) AI systems to infer emotions versus emotion recognition systems\n(244) Article 3(39) AI Act defines ‘emotion recognition systems’ as AI systems ‘for the\npurpose of identifying and inferring emotions or intentions of natural persons on the\nbasis of their biometric data."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nThe prohibition in Article 5(1)(f) AI Act does not refer to\n‘emotion recognition systems’, but only to ‘AI systems to infer emotions of a natural\nperson’. Recital 44 further clarifies that that prohibition covers AI systems ‘to identify\nor infer emotions.’\n(245) Inferring generally encompasses identifying as a prerequisite , so that t he prohibition\nshould be understood as including both AI systems identifying or inferring emotions or\nintentions.153 For consistency reasons, it is also important to construe the prohibition in\nArticle 5(1)(f) AI Act as having a similar scope as the rules applicable to other emotion\nrecognition systems (Annex III, point 1 (c), and Article 50 AI Act) and to limit it to\ninferences based on a person’s biometric data."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 3]\n\nThe definition in Article 3(39) AI Act of\nemotion recognition systems should therefore be considered relevant in relatio n to\nArticle 5(1)(f) AI Act. b) Identification and inference of emotions or intentions\n(246) ‘Identification’ occurs where the processing of the biometric data (for example, of the\nvoice or a facial expression) of a natural person allows to directly compare and identify\nan emotion with one that has been previously programmed in the emotion recognition\nsystem."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Biometrics", "guideline_source": "EU Commission"}, "text": "[Domain: Biometrics] [Article: Article 3]\n\n80\n(250) According to the definition in Article 3(39) AI Act, only AI systems identifying or\ninferring emotions or intentions based on biometric data constitute emotion recognition\nsystems.155\n(251) Personal characteristics from which biometric data can be extracted are physical or\nbehavioural attributes. Physiological biometrics employ physical, structural, and\nrelatively static attributes of a person, such as their fingerprints, the pattern of their iris,\ncontours of their face, or the geometry of veins in their hands."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 3]\n\n80\n(250) According to the definition in Article 3(39) AI Act, only AI systems identifying or\ninferring emotions or intentions based on biometric data constitute emotion recognition\nsystems.155\n(251) Personal characteristics from which biometric data can be extracted are physical or\nbehavioural attributes. Physiological biometrics employ physical, structural, and\nrelatively static attributes of a person, such as their fingerprints, the pattern of their iris,\ncontours of their face, or the geometry of veins in their hands."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Biometrics", "guideline_source": "EU Commission"}, "text": "[Domain: Biometrics]\n\nSome modalities are\nmicroscopic in nature , but still exhibit biological and chemical structures that can be\nacquired and identified e.g., DNA and odour.156 Behavioural biometrics monitor the\ndistinctive characteristics of movements, gestures, and motor -skills of individuals as\nthey perform a task or series of tasks. This means that human movements , such as\nwalking (gait analysis) or finger contact with a keyboard (keystrokes), are captured and\nanalysed."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\nSome modalities are\nmicroscopic in nature , but still exhibit biological and chemical structures that can be\nacquired and identified e.g., DNA and odour.156 Behavioural biometrics monitor the\ndistinctive characteristics of movements, gestures, and motor -skills of individuals as\nthey perform a task or series of tasks. This means that human movements , such as\nwalking (gait analysis) or finger contact with a keyboard (keystrokes), are captured and\nanalysed."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Biometrics", "guideline_source": "EU Commission"}, "text": "[Domain: Biometrics]\n\nBehavioural biometrics encompass a variety of modalities that exhibit both\nvoluntary and involuntary repeated motions and associated rhythmic timings/pressures\nof body features ranging from signat ures, gait, voice, and keystrokes through to eye\ntracking and heartbeats ,157 electroencephalography (EEG) ,158 or electrocardiograms\n(ECG).159 The biometric input can relate to one modality ( e.g., facial images) or\nmultiple modalities ( e.g., facial information combined with electroencephalogram\n(EEG))."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\nBehavioural biometrics encompass a variety of modalities that exhibit both\nvoluntary and involuntary repeated motions and associated rhythmic timings/pressures\nof body features ranging from signat ures, gait, voice, and keystrokes through to eye\ntracking and heartbeats ,157 electroencephalography (EEG) ,158 or electrocardiograms\n(ECG).159 The biometric input can relate to one modality ( e.g., facial images) or\nmultiple modalities ( e.g., facial information combined with electroencephalogram\n(EEG))."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Biometrics", "guideline_source": "EU Commission"}, "text": "[Domain: Biometrics]\n\nRecital 18 gives as examples facial expressions, gestures such as movement of\nhands or characteristics of a person’s voice. For example,\n- An AI system inferring emotions from written text (content/sentiment analyses) to\ndefine the style or the tone of a certain article is not based on biometric data and\ntherefore does not fall within the scope of the prohibition. - An AI system inferring emotions from key stroke (way of typing), facial\nexpressions, body postures or movements is based on biometric data and falls within\nthe scope of the prohibition."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\nRecital 18 gives as examples facial expressions, gestures such as movement of\nhands or characteristics of a person’s voice. For example,\n- An AI system inferring emotions from written text (content/sentiment analyses) to\ndefine the style or the tone of a certain article is not based on biometric data and\ntherefore does not fall within the scope of the prohibition. - An AI system inferring emotions from key stroke (way of typing), facial\nexpressions, body postures or movements is based on biometric data and falls within\nthe scope of the prohibition."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Biometrics", "guideline_source": "EU Commission"}, "text": "[Domain: Biometrics] [Article: Article 3]\n\n155 Article 3(34) AI Act: defines ‘biometric data’ as ‘personal data resulting from specific technical processing relating to the physical,\nphysiological or behavioural characteristics of a natural person, such as facial images or dactyloscopic data’’ See also Recital 18 AI\nAct. About emotion inferences from voice and speech. 156 Physiological and Behavioural Biometrics - Biometrics Institute\n157 Physiological and Behavioural Biometrics - Biometrics Institute\n158 See EDPS, TechDispatch 1/2024 – Neurodata, 3.6.2024, in which the use of brain data and related technology is discussed, as well\nas the legal implication, including the proposition of new ‘neurorights’, including mental privacy and integrity."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[Domain: Healthcare] [Article: Article 3]\n\n155 Article 3(34) AI Act: defines ‘biometric data’ as ‘personal data resulting from specific technical processing relating to the physical,\nphysiological or behavioural characteristics of a natural person, such as facial images or dactyloscopic data’’ See also Recital 18 AI\nAct. About emotion inferences from voice and speech. 156 Physiological and Behavioural Biometrics - Biometrics Institute\n157 Physiological and Behavioural Biometrics - Biometrics Institute\n158 See EDPS, TechDispatch 1/2024 – Neurodata, 3.6.2024, in which the use of brain data and related technology is discussed, as well\nas the legal implication, including the proposition of new ‘neurorights’, including mental privacy and integrity."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Biometrics", "guideline_source": "EU Commission"}, "text": "[Domain: Biometrics]\n\nIn S. O’Sullivan, H. Chneiweiss, A. Pierucci and K. Rommelfanger, Neurotechnologies and Human Rights Framework: Do we need new Human Rights?,\nReport, OECD and CoE, 9.11.2021, p.33 , a state of the art and legal aspects of neurotech is discussed. 159 See Hasnul et al., 2021, Electrocardiogram-Based Emotion Recognition Systems and Their Applications in Healthcare. 160 In the AI Act, the definition of biometric data does not include the wording ‘which allow or confirm the unique identificatio n’ (the\nfunctional use of biometric data), contrary to the definition of biometric data in the GDPR that includes this requirement."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Healthcare", "guideline_source": "EU Commission"}, "text": "[Domain: Healthcare]\n\nIn S. O’Sullivan, H. Chneiweiss, A. Pierucci and K. Rommelfanger, Neurotechnologies and Human Rights Framework: Do we need new Human Rights?,\nReport, OECD and CoE, 9.11.2021, p.33 , a state of the art and legal aspects of neurotech is discussed. 159 See Hasnul et al., 2021, Electrocardiogram-Based Emotion Recognition Systems and Their Applications in Healthcare. 160 In the AI Act, the definition of biometric data does not include the wording ‘which allow or confirm the unique identificatio n’ (the\nfunctional use of biometric data), contrary to the definition of biometric data in the GDPR that includes this requirement."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\n81\n(253) The prohibition in Article 5(1)(f) AI Act is limited to emotion recognition systems in\nthe ‘areas of workplace and educational institutions’. As clarified in recital 44 AI Act,\nthis limitation is meant to address the imbalance of power in the context of work or\neducation. a) ‘Workplace’\n(254) The notion of ’w orkplace’ should be interpreted broadly . That notion relates to any\nspecific physical or virtual space where natural persons engage in tasks and\nresponsibilities assigned by their employer or by the organisation they are affiliated to,\nfor example in case of self -employment."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\n81\n(253) The prohibition in Article 5(1)(f) AI Act is limited to emotion recognition systems in\nthe ‘areas of workplace and educational institutions’. As clarified in recital 44 AI Act,\nthis limitation is meant to address the imbalance of power in the context of work or\neducation. a) ‘Workplace’\n(254) The notion of ’w orkplace’ should be interpreted broadly . That notion relates to any\nspecific physical or virtual space where natural persons engage in tasks and\nresponsibilities assigned by their employer or by the organisation they are affiliated to,\nfor example in case of self -employment."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nThis includes any setting where the work is\nperformed and can vary widely based on the nature of the job, spanning from indoor\noffice spaces, factories and warehouses to publicly accessible spaces like shops,\nstadiums or museums, to open -air sites or cars , as well as temporary or mobile work\nsites. This is independent from the status as an employee, contractor, trainee, volunteer,\netc.161 The notion of ‘workplace’ in Article 5(1)(f) AI Act should also be understood to\napply to candidates during the selection and hiring process , consistently with other\nprovisions of the AI Act addressing the placing on the market, putting into service or\nuse of AI systems in the area of employment, workers management and access to self-\nemployment, since there is an imbalance of powers and the intrusive nature of emotion\nrecognition may already apply at the recruitment stage."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\nThis includes any setting where the work is\nperformed and can vary widely based on the nature of the job, spanning from indoor\noffice spaces, factories and warehouses to publicly accessible spaces like shops,\nstadiums or museums, to open -air sites or cars , as well as temporary or mobile work\nsites. This is independent from the status as an employee, contractor, trainee, volunteer,\netc.161 The notion of ‘workplace’ in Article 5(1)(f) AI Act should also be understood to\napply to candidates during the selection and hiring process , consistently with other\nprovisions of the AI Act addressing the placing on the market, putting into service or\nuse of AI systems in the area of employment, workers management and access to self-\nemployment, since there is an imbalance of powers and the intrusive nature of emotion\nrecognition may already apply at the recruitment stage."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\nFor example,\n- Using webcams and voice recognition systems by a call centre to track their\nemployee’s emotions, such as anger, is prohibited. 162 If only deployed for personal\ntraining purposes, emotion recognition systems are allowed if the results are not shared\nwith HR responsible persons and cannot impact the assessment, promotion etc. of the\nperson trained, provided that the prohibition is not circumvented and the use of the\nemotion recognition system does not have any impact on the work relationship."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\nFor example,\n- Using webcams and voice recognition systems by a call centre to track their\nemployee’s emotions, such as anger, is prohibited. 162 If only deployed for personal\ntraining purposes, emotion recognition systems are allowed if the results are not shared\nwith HR responsible persons and cannot impact the assessment, promotion etc. of the\nperson trained, provided that the prohibition is not circumvented and the use of the\nemotion recognition system does not have any impact on the work relationship."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\n- Using voice recognition systems by a call centre to track their customers emotions,\nsuch as anger or impatience, is not prohibited by Article 5(1)(f) AI Act (for example\nto help the employees cope with certain angry customers). - AI systems monitoring the emotional tone in hybrid work teams by identifying and\ninferring emotions from voice and imagery of hybrid video calls , which would\ntypically serve the purpose of fostering social awareness, emotional dynamics\nmanagement, and conflict prevention, are prohibited."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment] [Article: Article 5]\n\n- Using voice recognition systems by a call centre to track their customers emotions,\nsuch as anger or impatience, is not prohibited by Article 5(1)(f) AI Act (for example\nto help the employees cope with certain angry customers). - AI systems monitoring the emotional tone in hybrid work teams by identifying and\ninferring emotions from voice and imagery of hybrid video calls , which would\ntypically serve the purpose of fostering social awareness, emotional dynamics\nmanagement, and conflict prevention, are prohibited."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n161 See also the recitals in relation to the high-risk AI systems in the workplace, such as Recital 56, deploying a broad interpretation. See\nalso the list of high-risk AI systems in Annex III, referring to self-employment at 4. Self-employment is also broadly covered by EU\nanti-discrimination law. 162 Example from Boyd et al., 2023, Automated Emotion Recognition in the Workplace: How Proposed Technologies Reveal Potential\nFutures of Work."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\n82\n- Using cameras by a supermarket to track its employees’ emotions, such as happiness,\nis prohibited. - Using cameras by a supermarket or a bank to detect suspicious customers, for example\nto conclude that somebody is about to commit a robbery , is not prohibited under\nArticle 5(1)(f) AI Act, when it is ensured that no employees are being tracked and\nthere are sufficient safeguards. b) ‘Education institutions’\n(255) The reference to education institutions is broad and should be understood to include\nboth public and private institutions."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nThere is no limitation as regards the types or ages\nof pupils or students or of a specific environment (online, in person, in a blended\nmode,163 etc.). For example, education and training institutions at all levels fall under\nthe scope of the prohibition in Article 5(1)(f) AI Act, including vocational schools, i.e. schools where students learn skills involving the use of their hands 164 and continuous\ntraining165. Education institutions are normally accredited or sanctioned by the relevant\nnational education authorities or equivalent authorities."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nA key feature is that education\ninstitutions may provide a certificate (respectively participation is a precondition for\nobtaining a certificate). The prohibition should be understood to also apply to\ncandidates during the admissibility process. For example,\n- An AI-based application using emotion recognition for learning a language online\noutside an education institution is not prohibited under Article 5(1)(f) AI Act. By\ncontrast, if students are required to use the application by an education institution,\nthe use of such emotion recognition system is prohibited."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\n- An education institution using AI-based eye tracking software when examining\nstudents online to track the fixation point and movement of the eyes (gaze point, e.g.,\nto detect if unauthorized material is used) is not prohibited, because the system does\nnot identify or infer emotions . B y contrast, if the system is also used to detect\nemotions, such as emotional arousal and anxiousness, this would fall within the scope\nof the prohibition. - Using an emotion recognition AI system by a n education institution to infer the\ninterest and attention of students is prohibited."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\n163 Blended learning is to be understood as taking more than one approach in the education and training process, including blendi ng\ndigital (including online learning) and non-digital learning tools. 164 See e.g. the impact assessment accompanying the proposal of the Commission, where specific AI uses by vocational training\ninstitutions were mentioned as posing intense interference with a broad range of fundamental rights, e.g. when assessing: EU\nCommission, Commission Staff Working Document. Impact Assessment. Annexes, SWD(2021)84 final, Part2/2, p. 43. See also I."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\n83\n- Using an emotion recognition AI system by an education institut ion during\nadmissibility tests for new students is prohibited. - Using an AI system that allows to capture students talking to each other via their\nphones or other channels during online lectures by an education institution is not\nprohibited, since it does not infer emotions. By contrast, if the system is also used to\ndetect emotions, such as emotional arousal, anxiousness and interest, this would fall\nwithin the scope of the prohibition."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\n- An education institution employing an emotion recognition AI system on both\nteachers (workplace) and students (education) is prohibited. 7.2.3. Exceptions for medical and safety reasons\n(256) The prohibition in Article 5(1)(f) AI Act contains an explicit exception for emotion\nrecognition systems used in the area of the workplace and education institutions for\nmedical or safety reasons, such as systems for therapeutical use .166 In light of the AI\nAct’s objective to ensure a high -level of fundamental rights protection, this exception\nshould be narrowly interpreted."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\n(257) In particular, therapeutic uses should be understood to mean uses of CE-marked\nmedical devices. Moreover, t his exception does not comprise the use of emotion\nrecognition systems to detect general aspects of wellbeing. The general monitoring of\nstress levels at the workplace is not permitted under health or safety aspects. For\nexample, an AI system intended to detect burnout or depression at the workplace or in\neducation institutions would not be covered by the ex ception and would remain\nprohibited."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\n(258) The notion of safety reasons within this exception should be understood to apply only\nin relation to the protection of life and health and not to protect other interests, for\nexample property against theft or fraud. (259) It follows from this narrow interpretation of the exception that any use for medical and\nsafety reasons sh ould always remain limited to what is strictly necessary and\nproportionate, including limits in time, personal application and scale, and should be\naccompanied by sufficient safeguards."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education]\n\nSuch safeguards could include, for example,\nprior written and motivated expert opinion relating to t he specific use case. The\nnecessity should be assessed on an objective basis in relation to the medical and safety\npurpose, and not refer to the employer’s or educational institution’s ‘needs’. This\nassessment should inquire whether less intrusive alternative means exist which would\nachieve the same purpose."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Employment", "guideline_source": "EU Commission"}, "text": "[Domain: Employment]\n\n166 Recital 44 AI Act. 167 In conformity with EU employment law, if such new technologies are introduced, employers shall also consult with workers or their\nrepresentatives, conform national procedures. Without respecting these procedural requirements, such systems cannot be introduced\nby reference to the AI Act as such. They will require also consent from the point of view of data protection legislation, which remains\napplicable."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\n85\n(265) As mentioned before, out of scope are:\n- AI systems inferring emotions and sentiments not on the basis of biometric data,\n- AI systems inferring physical states such as pain and fatigue. (266) Emotion recognition systems used in all other domains other than in the areas of the\nworkplace and education institutions do not fall under the prohibition in Article 5(1)(f)\nAI Act. Such systems are, however, considered high-risk AI systems .171 At the same\ntime, such systems may be prohibited in certain cases by virtue of Article 5(1)(a) and\n(b) AI Act (harmful manipulation and exploitation) , or by virtue of other Union\nlegislation."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nAll other applicable legislation , such as U nion data protection law,\nconsumer protection etc. continue to apply to such systems. For example,\nEmotion recognition systems used in a commercial context for addressing customers\ndo not fall under the prohibition of Article 5(1)(f) AI Act, whether based on biometric\ndata or not. Hence, examples such as AI systems that enable emotion recognition based\non keystroke or based on voice messages of customers ( e.g., chat messages, use of\nvirtual voice assistants) , used in online marketing for applications for displaying\npersonalized messages and for advertisement purposes including in smart environments\n(‘intelligent billboards’) are not covered by the prohibition."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nNevertheless, such practices may be covered by the prohibitions of harmful\nmanipulation and exploitation in Article 5(1)(a) and (b) AI Act,172 if all conditions for\nthe application of those prohibitions are met. a) Other systems out of scope\n(267) ‘Crowd control’ generally refers to the control and monitoring of the behaviour of\ngroups to maintain (public) order and event safety. It is often associated with large\ncrowd events (e.g., soccer or football games, concerts, etc) or specific places, such as\nairports or trains."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nCrowd control systems can operate without inferring emotions of\nindividual persons, when for example analysing the general noise and mood level at a\ngiven place. In that case, the system would not fall within the scope of Article 5(1)(f)\nAI Act, because it does not infer emotions of (a concrete) natural person. (268) However, there may be instances where such crowd control systems infer emotions of\nindividuals, for example whether there are many angry faces."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Education", "guideline_source": "EU Commission"}, "text": "[Domain: Education] [Article: Article 5]\n\nNormally, such AI\nsystems would not fall under the prohibition of Article 5(1) (f) AI Act, since they are\ntypically not used in the workplace or in education institutions. (269) Also out of scope are systems that are used in the medical field for example care robots,\nor medical practitioners using emotion recognition systems during an examination at\ntheir workplace, and voice monitors that analyse emergency calls."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n86\n(270) Such systems will often screen persons that are there in a work context, for example the\nsecurity staff at a football stadium or at a central station (where such systems are used\nto recognize aggressive behaviour) , or employees in the medical field. In such cases,\ndeployers must employ safeguards to avoid the screening of employees. However, it\ncannot be completely avoided that such systems also infer the emotions of those\nemployees."}
{"meta": {"section_type": "guideline", "article_ref": "ARTICLE 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: ARTICLE 5]\n\nSince the primary objective of the system is not targeted at assessing\nemployees’ emotions, these systems should be considered to be outside the scope of the\nprohibition. Deployers of such systems remain responsible to ensure that employees are\nnot adversely affected by their use. 8. ARTICLE 5(1)(G) AI ACT: BIOMETRIC CATEGORISATION FOR CERTAIN\n‘SENSITIVE’ CHARACTERISTICS\n(271) Article 5(1) (g) AI Act prohibits biometric categorisation systems that categorise\nindividually natural persons based on their biometric data to deduce or infer their race,\npolitical opinions, trade union membership, religious or philosophical beliefs, sex -life\nor sexual orientation ."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nThis prohibition does not cover the labelling, filtering , or\ncategorisation of biometric data sets acquired in line with Union or national law, which\nmay be used, for example, for law enforcement purposes.173\n8.1. Rationale and objectives\n(272) A wide variety of information, including ‘sensitive’ information , may be extracted,\ndeduced or inferred from biometric information, even without the knowledge of the\npersons concerned, to categorise those persons."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nThis may lead to unfair and\ndiscriminatory treatment, for example when a service is denied because somebody is\nconsidered to be of a certain race. AI-based biometric categorisation systems for the\npurpose of assigning natural persons to specific groups or categories, relating to aspects\nsuch as sexual or political orientation or race, violate human dignity and pose significant\nrisks to other fundamental rights , such as privacy and non-discrimination. They are\ntherefore prohibited by Article 5(1)(g) AI Act. 8.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n87\n(273) Several cumulative conditions must be fulfilled for the prohibition in Article 5(1)(g) AI\nAct to apply:\n(i) The practice must constitute the ‘placing on the market’, ‘the putting into service\nfor this specific purpose’ or ‘the use’ of an AI system;\n(ii) The system must be a biometric categorisation system;\n(iii) individual persons must be categorised;\n(iv) based on their biometric data;\n(v) to deduce or infer their race, political opinions, trade union membership, religious\nor philosophical beliefs, sex-life, or sexual orientation."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(274) For the prohibition to apply, all five conditions must be simultaneously fulfilled. The\nfirst condition, i.e. the placing on the market, the putting into service or the use of the\nAI system , is analysed in sectio n 2.3. The prohibition, therefore, applies to both\nproviders and deployers of AI systems, each within their respective responsibilities, not\nto place on the market, put into service or use such AI systems. The other conditions\nfor the application of the prohibition174 are further described and analysed below."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(275) The prohibition does not cover the labelling or filtering of lawfully acquired biometric\ndatasets, including for law enforcement purposes. 8.2.1. Biometric categorisation system\n(276) ‘The categorisation of an individual by a biometric system is typically the process of\nestablishing whether the biometric data of an individual belongs to a group with some\npre-defined characteristic. It is not about identifying an individual or verifying their\nidentity, but about assigning an individual to a certain category."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 3]\n\nFor instance, an\nadvertising display may show different adverts depending on the individual that is\nlooking at it based on the ir age or gender.’175 Persons may also simply be categorised\nfor statistical reasons , without being identified and without the objective to identify\nthem. (277) Article 3(40) AI Act defines a biometric categorisation system as an AI system for the\npurpose of assigning natural persons to specific cat egories on the basis of their\nbiometric data, unless it is ancillary to another commercial service and strictly necessary\nfor objective technical reasons."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 3]\n\nAs explained in section 7.2.1.d), ‘biometric data’ is\ndefined in Article 3(34) AI Act . In particular, biometric data comprises behavioural\ncharacteristics that are based on biometric features. The scope of biometric\ncategorisation excludes categorisation according to clothes or accessories, such as\nscarfs or crosses, as well as social media activity. (278) Biometric categorisation may rely on categories of physical characteristics (e.g., facial\nfeatures and form , skin colour) based on which persons are assigned to specific\ncategories."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n89\n(282) Furthermore, for the prohibition to apply, natural persons must be ‘individually’\ncategorised. If this is not the purpose or outcome of the biometric categorisation, the\nprohibition does not apply, for example if a whole group is categorised without looking\nat the individual. Examples of individual categorisation include:\n- AI systems that conduct ‘Attribute Estimation’ (count demographics), including for\nexample ‘age, gender, ethnicity’, on the basis of for example bodily features, such as\nface, height, or skin, eye and hair colour (or a combination thereof)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n- AI systems capable of categorising individuals and singling them out based on a\nspecific feature (e.g., a scar under the right eye), or because they have a tattoo on their\nright hand. These use -cases are examples for individual biometric categorisation. For these\nexamples to fall within the prohibition of Article 5(1)(g) AI Act all conditions of that\nprovision must be fulfilled. 8.2.3. To deduce or infer their race, political opinions, trade union\nmembership, religious or philosophical beliefs, sex -life or sexual\norientation\n(283) Article 5(1)(g) AI Act prohibits o nly biometric categorisation systems which have as\ntheir objective to deduce or infer a limited number of sensitive characteristics: race,\npolitical opinions, trade union membership, religious or philosophical beliefs, sex -life\nor sexual orientation."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nFor example, systems prohibited under Article 5(1)(g) AI Act include:\n- a biometric categorisation system that claims to be capable of deducing an individual’s\nrace from their voice (This is different from a system that categorises persons\naccording to skin or eye colour, or a system that analyses the DNA of victims of crimes\nin view of their origin. Those systems would not be prohibited). - a biometric categorisation system that claims to be capable of deducing an individual’s\nreligious orientation from their tattoos or faces. 8.3."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nOut of scope\n(284) The prohibition in Article 5(1)(g) AI Act does not cover AI systems engaged in the\nlabelling or filtering of lawfully acquired biometric datasets, such as images, based on\nbiometric data, including in the area of law enforcement. This is further explained in\nrecital 30 AI Act.177\n(285) The labelling or filtering of biometric datasets may be done by biometric categorisation\nsystems precisely to guarantee that the data equally represent all demographic groups,\nand not, for example, over-represent one specific group."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n90\nalgorithm are biased against a specific group (i.e. systematic differences in the data exist\nbetween groups due to the way the data are collected, or data is historically biased), the\nalgorithm may replicate this bias, possibly resulting in unlawful discrimination against\npersons or groups of persons.178 For this reason, labelling on the basis of some protected\nsensitive information may be necessary for high -quality data, precisely to prevent\ndiscrimination."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nThe AI Act may even require labelling operations to conform to the AI\nAct’s requirements for high-risk AI systems.179 Such labelling or filtering of biometric\ndata is therefore explicitly exempted from the prohibition in Article 5(1)(g) AI Act. The\nprohibition only applies where biometric data is categorised to infer race, political\nopinions, trade union membership, religious or philosophical beliefs, sex-life or sexual\norientation."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nExamples of permissible labelling or filtering include:\n- the labelling of biometric data to avoid cases where a member of an ethnic group has\na lower chance of being invited to a job interview because the algorithm was ‘trained’\nbased on data where that particular group performs worse, i.e. has worse outcomes\nthan other groups.180\n- the categorisation of patients using images according to their skin or eye colour may\nbe important for medical diagnosis, for example cancer diagnoses."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(286) Article 5(1)(g) AI Act also provides that the prohibition in that provision does not apply\nto the l abelling or filtering of lawfully acquired datasets in the area of law\nenforcement.181\nFor example, this covers the use by a law enforcement authority of an AI system that\nallows labelling and filtering of a dataset suspected of containing child sexual abuse\nmaterial. In a first step, law enforcement would use the support of AI systems to detect\nand redact sensitive data from images."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nFurthermore, filtering and labelling according to\ngender, age, biometric data such as eye and hair colour, scars and marking could help\nwith identifying the victim s or creating links with other cases. Similarly filtering and\nlabelling abusers’ hands based on specific characteristics such as length of fingers or\nany distinguishing markings or tattoos to help with identifying possible suspects is\npermitted. 8.4."}
{"meta": {"section_type": "guideline", "article_ref": "Article 10", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 10]\n\n178 Ibid. 179 See e.g. Article 10 and 17 AI Act. 180 FRA, # BigData. Discrimination in data supported decision making, Luxemburg, 2018, 14, p. 5. 181 The AI Act concerning the use of biometric categorisation systems for law enforcement is based on Article 16 TFEU. See also Recital\n3 AI Act. 182 Recital 54 and Annex III, point 1 letter b) AI Act. 183 Recital 54 and Annex III, point 1 letter b). AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n91\n(288) Article 5(1) (g) AI Act further restricts the possibilities for a lawful personal data\nprocessing under Union data protection law, such as the GDPR, LED, EUDPR . In\nparticular, Article 5(1)(g) AI Act excludes the possibilities for biometric categorisation\nof natural persons, based on their biometric data, as defined in the AI Act, to infer race,\npolitical opinions, trade union membership, religious or philosophical beliefs, sex -life\nor sexual orientation, subject to the exception for labelling or filtering of lawfully\nacquired biometric data sets, including in the area of law enforcement , as described\nabove."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nMoreover, the prohibition in Article 5(1)(g) AI Act is consistent with\nArticle 11(3) LED, which explicitly prohibits any ‘profiling’ that results in\ndiscrimination on the basis of special categories of personal data, such as race, ethnic\norigin, sexual orientation, political opinion, or religious beliefs. 9. ARTICLE 5(1) (H) AI ACT - REAL-TIME REMOTE BIOMETRIC\nIDENTIFICATION (RBI) SYSTEMS FOR LAW ENFORCEMENT PURPOSES\n(289) Article 5(1)(h) AI Act prohibits the use of real-time RBI systems in publicly accessible\nspaces for law enforcement purposes, subject to limited exceptions exhaustively set out\nin the AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nSpecifically, Article 5(1)(h)(i)-(iii) AI Act envisages three situations in\nwhich the use of such systems may be permitted where authorised by national\nlegislation and where the conditions and safeguards of Article 5(2) to ( 7) AI Act are\nmet. (290) In accordance with Article 5(5) AI Act, Member States are free to decide whether and\nin which of the three situations the use of real-time RBI systems in publicly accessible\nspaces for law enforcement purposes is permitted in their territory ."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nIn the absence of\nnational legislation allowing and regulating such use, law enforcement authorities and\nentities acting on their behalf may not deploy such systems for law enforcement\npurposes. The existence of national legislation that complies with the relevant\nrequirements of the AI Act is therefore a pre-requisite of such use. (291) Article 5(1)(h) AI Act only prohibits the use of real -time RBI systems in publicly\naccessible spaces for law enforcement purposes, so that only deployers of such systems\nare concerned by that provision."}
{"meta": {"section_type": "guideline", "article_ref": "Article 6", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 6]\n\nThe placing on the market and the putting into service\nof such systems, as well as the use of other RBI systems, is not prohibited, but subject\nto the rules for high-risk AI systems in accordance with Article 6(2) and point a) of\nAnnex I II AI Act .184 Where a Member States authorises the use of real-time RBI\nsystems in publicly accessible spaces for law enforcement purposes for any of the three\nobjectives listed in Article 5(1)(h) AI Act, the rules for high-risk AI systems also apply\nto that use."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n92\n(293) Recital 32 AI Act acknowledges the intrusive nature of real-time RBI systems in\npublicly accessible spaces for law enforcement purposes to the rights and freedoms of\npersons concerned, to the extent that it may affect the private life of a large part of the\npopulation, evoke a feeling of constant surveillance , and indirectly dissuade the\nexercise of the freedom of assembly and other fundamental rights. Technical\ninaccuracies of AI systems intended for the remote biometric identification of natural\npersons can lead to biased results and entail discriminatory effects."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nSuch possibly biased\nresults and discriminatory effects are particularly relevant with regard to age, ethnicity,\nrace, sex or disabilities. In addition, the immediacy of the impact and the limited\nopportunities for further checks or corrections in relation to the use of such systems\noperating in real-time carry heightened risks for the rights and freedoms of the persons\nconcerned in the context of, or impacted by, law enforcement activities."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(294) However, where the use of such systems is strictly necessary to achieve a substantial\npublic interest and where the situations in which such use may occur are exhaustively\nlisted and narrowly defined, that use outweighs the risks to fundamental rights (Recital\n33 AI Act). To ensure that such systems are used in a ‘responsible and proportionate\nmanner’, their use is subject to the safeguards and the specific obligations and\nrequirements in Article 5(2)-(7) AI Act. 9.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nMain concepts and components of the prohibition\nArticle 5(1)(h) AI Act\nThe following AI practices shall be prohibited:\nh) the use of ‘real-time’ remote biometric identification systems in publicly accessible\nspaces for the purpose of law enforcement, unless and in so far as such use is strictly\nnecessary for one of the following objectives:\ni) the targeted search for specific victims of abduction, trafficking in human beings\nor sexual exploitation of human beings, as well as the search for missing persons;\nii) the prevention of a specific, substantial and imminent threat to the life or physical\nsafety of natural persons or a genuine and present or genuine and foreseeable threat of\na terrorist attack;\niii) the localisation or identification of a person suspected of having committed a\ncriminal offence, for the purpose of conducting a criminal investigation or prosecution\nor executing a criminal penalty for offences referred to in Annex II and punishable in\nthe Member State concerned by a custodial sentence or a detention order for a maximum\nperiod of at least four years."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n93\n(ii) The activity consists of the ‘use’ of that system,\n(iii)in ‘real-time’,\n(iv) in publicly accessible spaces, and\n(v) for law enforcement purposes. (296) The second condition, i.e. the ‘use’ of the AI system , has been already analysed in\nsection 2.3. of these Guidelines. The other conditions listed above are further described\nand analysed below. 9.2.1. The Notion of Remote Biometric Identification\n(297) Biometric recognition technologies detect, capture, and transform measurable physical\ncharacteristics (such as eye distance and size, nose length, etc.) or behavioural\ncharacteristics (such as gait or voice) into machine-readable biometric data (see section\n7.2.1.d) above)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 3]\n\nThese data are available in different forms: images or templates, which\nare a mathematical representation of the salient features of an individual, used for\nrecognition purposes. Biometric recognition technologies are used for verification and\nidentification purposes.185\n(298) According to Article 3(41) AI Act, a RBI system is\n[a]n AI system for the purpose of identifying natural persons, without their\nactive involvement, typically at a distance through the comparison of a person’s\nbiometric data with the biometric data contained in a reference database."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(299) This definition covers only the identification functionality of biometric recognition\nsystems, which implies the absence of active involvement of the persons concerned (i.e. no active participation) and results in the capture of the characteristics of those persons\ntypically at a distance. For identification performance, the captured biometric data are\ncompared with biometric data already stored in a reference database (such as a\nrepository, e.g., a criminal database containing facial images or templates of suspects)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 3]\n\na) Identification purposes only\n(300) The notion of ‘biometric identification’ is defined in Article 3(35) of the AI Act as\nthe automated recognition of physical, physiological and behavioural or\npsychological human features for the purpose of establishing the identity of\na natural person by comparing biometric data of that individual to biometric\ndata of individuals stored in a database. (301) Recital 15 AI Act further clarifies that such human features may comprise\nthe face, eye movement, body shape, voice, prosody, gait, posture, heart rate,\nblood pressure, odour, keystroke characteristics,\n(302) AI systems used for following natural persons can also be included in the definition of\nbiometric identification, for example to see in which direction a suspect escapes."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n95\n- Systems that are used to give access to the metro station , such as biometric metro\ntickets, where persons are actively involved and consciously approach the biometric\nsensor to obtain access, do not fulfil that condition. (307) Biometric recognition systems that process (contactless) fingerprints, gait, voice, DNA,\nkeystrokes and other (biometric) behavioural signals may also constitute RBI\nsystems.188\nFor example,\n- A voice biometric technology system may be deployed to identify a person\nspeaking. The microphone then collects the biometric sample."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n- A gait recognition system may be used via CCTV and the videos are automatically\nchecked for matches with previously captured templates. - Keystroke biometric technology may be used to identify the person typing a\nfraudulent message. The fact that these systems are given as examples of RBI systems does not imply that\nthey are prohibited under Article 5 AI Act. (308) In the case of body -cams capable of RBI used by individual law enforcement agents,\nthe untargeted filming during, for example , a demonstration with hundreds of\nparticipants will be considered to fulfil the condition of remoteness."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nc) Reference database\n(309) Identification is not possible without a reference database containing biometric data for\ncomparison purposes. Thus, the existence of a reference database is indispensable to\nperform the comparison for identification purposes.189\nFor example, in the case of missing persons, the Schengen Information System190\ndatabase could be used as the reference database for facial recognition purposes (once\noperational). 9.2.2."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nReal-time\n(310) Real-time means that the system capture s and further process es biometric data\n‘instantaneously, near-instantaneously or in any event without any significant delay.’191\nAll the processing steps, i.e. the capture, comparison, and identification of biometric\ndata, occur simultaneously or almost simultaneously, which may include a ‘limited\nshort delay’ to avoid the prohibition being circumvented through the retrospective use\nof RBI systems."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n96\nAct; it will have to be assessed on a case -by-case basis. As the devices used for real -\ntime or post -remote identification are increasingly one and the same with differ ent\nfunctionalities, the distinction is temporal. Generally speaking, a delay is significant at\nleast when the person is likely to have left the place where the biometric data was taken. (311) Real-time systems in general are used at a given place to facilitate a quick reaction and\nnot to retrospectively identify persons."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nThey provide the user of the system with a means\nto track the movements of persons under surveillance and to monitor them. a) An AI system screens all incoming visitors to a concert venue: Real-time RBI\nb) A system films all incoming visitors to a concert. An incident happens at the\nconcert. After the concert, the identification system is operated on the video material\nin order to identify the offender: Post-RBI."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(312) When a law enforcement authority covertly takes a picture of a person via a mobile\ndevice and submits it to a database for immediate search, depending on the\ncircumstances, this may fall under the prohibition of Article 5(1)(h) AI Act. 9.2.3. In publicly accessible spaces\n(313) Article 3(44) AI Act defines publicly accessible spaces as\nany publicly or privately owned physical space accessible to an undetermined\nnumber of natural persons, regardless of whether certain conditions for access\nmay apply, and regardless of the potential capacity restrictions."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(314) Recital 19 AI Act lists several elements that characterise such spaces:\n- Accessibility to an undetermined number of persons, independently of the potential\ncapacity or security restrictions, such as purchasing a ticket or title of transport,\nprior registration or having a certain age. The possibility of getting access to a space\nthrough an unlocked door does not mean that the space is publicly accessible if\nindications or circumstances suggest the contrary (such as a sign restricting access)."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nMoreover, the access to a space can be limited to certain persons, as defined by law,\nlinked to public safety or security, or to the decision of the person having the\nrelevant authority over the space. For example, publicly accessible spaces are in principle:\n- a concert venue for which participants pay an entrance fee. - an event location where a trade fair is organised targeting participants over\nthe age of 50."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nA space closed by a gate, even if the gate is unlocked, such as the gated entrance of\na fenced residential area of several houses, will normally not be considered a publicly\naccessible space. By contrast, a park in a gated residence with public opening hours\nwithout any access restrictions during those hours will generally constitute a publicly\naccessible space during those hours and a closed space outside those hours."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n98\ncontrol (where the customs officials stand and passports or ID checks occur) is excluded\nfrom the scope of the prohibition. (318) As clarified in Recital 19 AI Act, assessing whether a space is accessible to the public\nshould be done based on a case-by-case analysis. 9.2.4. For law enforcement purposes\n(319) The prohibition in Article 5(1)(h) AI Act applies to the use of RBI systems for law\nenforcement purposes, irrespective of the entity, authority, or body carrying out the law\nenforcement activities."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 3]\n\n(320) Law enforcement is defined in Article 3(46) AI Act as the ‘activities carried out by law\nenforcement authorities or on their behalf for the prevention, investigation, detection or\nprosecution of criminal offences or the execution of criminal penalties, including\nsafeguarding against and preventing threats to public security.’ These purposes are the\nsame as those listed in Article 1 LED.196 Thus, any interpretation of those purposes in\nrelation to LED may also be relevant for the purpose of interpreting the notion of ‘law\nenforcement’ used in the AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(321) Law enforcement purposes comprise the investigation, detection, and prosecution of\ncriminal offences. They also comprise activities in relation to the prevention of criminal\noffences, including safeguarding against and the prevention of threats to public security,\nbefore any crime has actually been committed. For instance, the police may take\n‘coercive measures at demonstrations, major sporting events or riots’ in the context of\ncrime prevention.197 Finally, those activities comprise the execution of penalties, such\nas the execution of sentences."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 3]\n\n(322) According to Article 3(46) AI Act, law enforcement activities may be performed by\nlaw enforcement authorities or on their behalf. Law enforcement authorities are further\ndefined in Article 3(45) AI Act in the same manner as national competent authorities\nare defined in the LED. 198 That definition covers law enforcement authorities and\nentrusted bodies or entities (which may be private parties):\n(a) any public authority competent for the prevention, investigation, detection\nor prosecution of criminal offences or the execution of criminal penalties,\nincluding the safeguarding against and the prevention of threats to public\nsecurity; or\nFor example, such public authorities include police authorities and criminal justice\nauthorities (such as prosecutors) when they carry out a law enforcement task."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n99\nof criminal penalties, including the safeguarding against and the prevention\nof threats to public security;\n(323) Under the AI Act other entities, bodies, or persons, may exercise l aw enforcement\nactivities after being entrusted by Member States law entrusting them public authority\nand public powers for the purposes listed above. (324) ‘On behalf of’ means that a law enforcement authority has delegated the performance\nof a law enforcement activity (or part of it) to another entity or person, including private\nparties, or has requested in specific cases another entity or person to act to support law\nenforcement activities."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nIn both cases, the law enforcement authorities must instruct on\nall major aspects and supervise the other entity, as this requirement is inherent to the\nnotion of acting ‘on behalf’ of a person. Delegation of tasks to other bodies may include, for example,\n- Public transport companies requested by law enforcement authorities to\nensure security on the public transport networks under their instructions\nand supervision. - Sports federations requested by law enforcement authorities to act under\ntheir instructions and supervision to provide security at sporting events."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n- Banks that are requested by law enforcement authorities to conduct certain\nactions to ‘counter certain crimes in specific cases’ under the instructions\nand supervision of law enforcement authorities. These activities fall within the definition of “for the purpose of law enforcement”\nsince those entities act ‘on behalf’ of law enforcement authorities. If those entities act\non their ‘own behalf’ when detecting and countering crimes (such as fraud , money\nlaundering), they will not be considered to fall under the prohibition of Article 5(1)(h)\nAI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(325) Only when those other bodies or entities have been entrusted with a specific law\nenforcement task will their activities fall under the definition of ‘law enforcement’. 9.3. Exceptions to the prohibition\n(326) The AI Act provides three exceptions to the general prohibition on the use of real-time\nRBI in publicly accessible spaces for law enforcement purposes. Article 5(1)(h)(i) to\n(iii) AI Act exhaustively list s three objectives for which real -time RBI may be\nauthorised, while Article 5(2) to 5(7) AI Act lays down the conditions and safeguards\nfor such authorisation."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nArticle 5(1)(h)(i)-(iii) AI Act does not in itself constitute a legal\nbasis for the real-time use of RBI systems in publicly accessible spaces. Rather, only a\ndomestic Member State law that fulfils, in particular, the requirements in Article 5(2)-\n(7) AI Act can allow the use of real-time RBI, as provided by Article 5(2) AI Act. Consequently, in the absence of Member State legislation authorising the use of real -\ntime RBI for one or more of those objectives, such use is prohibited as from 2 February\n2025."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n100\n9.3.1. Rationale and objectives\n(327) The objectives set out in Article 5(1)(h)(i)-(iii) AI Act aim to allow the use of certain\nAI and investigative tools for law enforcement purposes. These objectives are:\n(i) the targeted search of victims of three specific serious crimes and missing\npersons [protection];\n(ii) the prevention of imminent threats to life or physical safety or a genuine\nthreat of terrorist attacks [prevention]; and\n(iii) the localisation or identification of suspects and offenders of certain serious\ncrimes as listed in Annex II [prosecution/investigation]."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(328) In those scenarios, the Union legislature has balanced the security needs of society\nagainst the risk that real-time RBI systems pose to the fundamental rights of individuals\nsubject to those systems. According to Recital 33 AI Act, the objectives for which the\nuse of real -time RBI systems for law enforcement purposes in publicly accessible\nspaces is allowed must be strictly, exhaustively, and narrowly defined, and appear when\nthere is a ‘strict necessity’ to achieve ‘a substantial public interest’ which ‘outweighs\nthe risks’ posed to fundamental rights."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article \n5]\n\nAny other use of real-time RBI systems in\npublicly accessible spaces for law enforcement purpose s which is not listed in Article\n5(1)(h)(i)-(iii) AI Act is prohibited. For instance, the use of real-time RBI systems by the police to identify a shoplifter and\ncompare their facial images against criminal databases is prohibited, as it does not fall\nunder any of the objectives listed in Article 5(1)(h)(i)-(iii) AI Act. 9.3.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nTargeted search for the victims of three serious crimes and missing\npersons\n(329) According to Article 5(1)(h)(i) AI Act, the use of real-time RBI in publicly accessible\nspaces for law enforcement purposes is allowed , subject to strict necessity and the\nconditions in Article 5(2) -(7) AI Act, for the targeted search of victims of abduction,\ntrafficking in human beings , or sexual exploitation of human beings, as well as the\nsearch for missing persons."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n101\n(332) The targeted search for specific victims of three serious crimes are covered by the\nscenario listed in Article 5(1)(h)(i) AI Act: the abduction of, trafficking in, and sexual\nexploitation of human beings.199\nIf, for example , a child is kidnapped and there are concrete indications that the\nkidnapper intends to bring the child from one place to another by car, the police may\nuse a real-time RBI system for the targeted search of that child , but it must define a\nperimeter of deployment and duration of use to identify the child."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nb) Searching for missing persons\n(333) The first scenario also covers the search for a missing person.200\n(334) A distinction may be made between missing children and missing adults, since the\nvoluntary disappearance of a missing adult will not always trigger a search . The\napplicable rules regarding missing children vary considerably from one Member State\nto another.201 In any event, Article 5(1)(h)(i) AI Act only allows the use of a real-time\nRBI system to search for missing persons for law enforcement purposes."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(335) The disappearance of an adult does not always lead to a search of that person by police,\nas adults have the right to disappear. A search could be linked to the legal status of the\nperson (‘under curatorship’), their health condition (a mental illness), the existence of\na suicidal note, but also the departure without personal belongings. If the circumstances\nof the disappearance are a cause for concern, the disappearance may be filed with the\npolice so that a search can start."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(336) In some Member States , the search for a missing person may occur under an\nadministrative procedure and not for law enforcement purposes. For example, where a\nvulnerable person is missing, but there is no suspicion of a crime or any other law\nenforcement purpose, the use of real-time RBI systems to search for that person would\nnot be deemed to be for law enforcement purposes and would therefore fall under the\nrules for such use under the GDPR. 9.3.3."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration]\n\n199 Kidnapping, trafficking in human beings, and sexual exploitation are three crimes that can trigger a European Arrest Warrant (EAW)\nto arrest and transfer a criminal suspect or a sentenced person to the country that issued the EAW. The three crimes relate mostly but\nnot exclusively to women and children. According to the European Commission’s DG Migration and Home Affairs, almost 40 percent\nof the victims are EU citizens, and most of them are wom en and children trafficked for sexual exploitation. The number of men\nvictims has nearly doubled in ten years."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Migration", "guideline_source": "EU Commission"}, "text": "[Domain: Migration]\n\nThey are trafficked for forced labour and forced begging, while most of the women an d\nchildren are trafficked for sexual exploitation. https://home-affairs.ec.europa.eu/policies/internal-security/organised-crime-and-\nhuman-trafficking/together-against-trafficking-human-beings_en\n200 A ‘missing person’ is not defined at EU level. But in Council Conclusions of December 2021 on ‘Stepping Up Cross-Border Police\nCooperation in the area of Missing Persons’, the Council takes as reference both the definition of a missing person in the Council of\nEurope’s Recommendation CM/Rec (2009) 12 and in national regulations."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Critical Infrastructure", "guideline_source": "EU Commission"}, "text": "[Domain: Critical Infrastructure] [Article: Article 2]\n\n102\nthe prevention of a specific, substan tial and imminent threat to the life or\nphysical safety of natural persons or a genuine and present or genuine and\nforeseeable threat of a terrorist attack. a) Specific, substantial and imminent threat to life or physical safety of natural\npersons\n(338) In application of Article 2 of the Charter, which guarantees the right to life, the U nion\nand its Member States must safeguard and, thus, protect the lives of individuals."}
{"meta": {"section_type": "guideline", "article_ref": "Article 2", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 2]\n\n102\nthe prevention of a specific, substan tial and imminent threat to the life or\nphysical safety of natural persons or a genuine and present or genuine and\nforeseeable threat of a terrorist attack. a) Specific, substantial and imminent threat to life or physical safety of natural\npersons\n(338) In application of Article 2 of the Charter, which guarantees the right to life, the U nion\nand its Member States must safeguard and, thus, protect the lives of individuals."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Critical Infrastructure", "guideline_source": "EU Commission"}, "text": "[Domain: Critical Infrastructure] [Article: Article 5]\n\nThe\ncriteria in Article 5(1)(h)(ii) AI Act concerning the threat to life to allow for the use of\nreal-time RBI systems in publicly accessible spaces require the existence of (1) a\nspecific, (2) substantial and (3) imminent threat to the life or physical safety of (4)\nnatural persons. The threat does not need to be limited to identified indiv iduals or a\ngroup, as it relates to natural persons in general."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nThe\ncriteria in Article 5(1)(h)(ii) AI Act concerning the threat to life to allow for the use of\nreal-time RBI systems in publicly accessible spaces require the existence of (1) a\nspecific, (2) substantial and (3) imminent threat to the life or physical safety of (4)\nnatural persons. The threat does not need to be limited to identified indiv iduals or a\ngroup, as it relates to natural persons in general."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Critical Infrastructure", "guideline_source": "EU Commission"}, "text": "[Domain: Critical Infrastructure]\n\n(339) Recital 33 AI Act clarifies that an imminent threat to life or the physical safety of natural\npersons may also include an imminent threat to critical infrastructure 202 ‘where the\ndisruption or destruction of such critical infrastructure would result in an imminent\nthreat to the life, or the physical safety of a person, including through serious harm to\nthe provision of basic supplies to the population or to the exercise of the core function\nof the State.’\nFor example,203\nA serious disruption and destruction of critical infrastructure ( e.g., a power plant,\nwater supply, or a hospital) may result in an imminent threat to the life or the physical\nsafety of a person when the re is serious harm of cessation of basic supplies to the\npopulation (deprivation of electricity or drinkable water for a long period, in a\nparticularly warm or cold weather, etc)."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(339) Recital 33 AI Act clarifies that an imminent threat to life or the physical safety of natural\npersons may also include an imminent threat to critical infrastructure 202 ‘where the\ndisruption or destruction of such critical infrastructure would result in an imminent\nthreat to the life, or the physical safety of a person, including through serious harm to\nthe provision of basic supplies to the population or to the exercise of the core function\nof the State.’\nFor example,203\nA serious disruption and destruction of critical infrastructure ( e.g., a power plant,\nwater supply, or a hospital) may result in an imminent threat to the life or the physical\nsafety of a person when the re is serious harm of cessation of basic supplies to the\npopulation (deprivation of electricity or drinkable water for a long period, in a\nparticularly warm or cold weather, etc)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Critical Infrastructure", "guideline_source": "EU Commission"}, "text": "[Domain: Critical Infrastructure] [Article: Article 5]\n\n(340) What constitutes an imminent threat to life or the physical safety of natural persons is\nultimately defined and assessed at the level of the Member State based on its national\nlaws, in accordance with EU law, in particular taking into account the key elements and\nrationale of Article 5 AI Act. This will have to be laid down/referred to in the laws\nMember States must adopt to make use of the exceptions to the prohibition on the use\nof real-time RBI for law enforcement purposes in publicly accessible spaces."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(340) What constitutes an imminent threat to life or the physical safety of natural persons is\nultimately defined and assessed at the level of the Member State based on its national\nlaws, in accordance with EU law, in particular taking into account the key elements and\nrationale of Article 5 AI Act. This will have to be laid down/referred to in the laws\nMember States must adopt to make use of the exceptions to the prohibition on the use\nof real-time RBI for law enforcement purposes in publicly accessible spaces."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n103\nFor example, the police are informed that a former student plans a deadly attack at\nhis former university as he seeks revenge on several former classmates. The police\nreceives information about the imminence of the attack, the targeted school, and the\nweapons he plans to use to execute his plans. (343) A specific threat needs not be intentional. Non-intentional actions could also result in a\nthreat to life or physical safety."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nb) A genuine and present or genuine and foreseeable threat of a terrorist attack\n(344) This part of the second scenario described in Article 5(1)(h)(ii) AI Act is comprised of\nseveral elements: the existence of a threat of a terrorist attack and the characteristics of\nthe threat, which must be genuine and present or genuine and foreseeable. Threat of a terrorist attack\n(345) The assessment concerning the existence and seriousness of the threat is ma de at\nnational level when assessing the actual circumstances of a measure to be taken to\nsafeguard national security, and more specifically, in case of a terrorist attack."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nThe\nterrorist threat level is defined at national level and varies from one Member State\nto another. For example, the Netherlands has established five levels of threats, 205\nBelgium four,206 France three, 207 and Sweden five. 208 However, the concept of ‘a\ngenuine and present or genuine and foreseeable threat’, as used in Article 5(1)(h)(ii), is\nan autonomous notion of U nion law and should therefore be assessed, in principle,\nindependently of national definitions. The threat relates not to terrorism in general, but\nspecifically to a threat of a terrorist attack."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nCharacteristics of the threat: genuine and present or genuine and foreseeable\n(346) The threshold of seriousness that a threat needs to reach to allow for the use of real-\ntime RBI systems in publicly accessible spaces for law enforcement purposes was\ninspired by the CJEU’s case law on data retention and passenger name record measures\naimed at safeguarding national security, in particular, against terrorist attacks."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nAccording to the CJEU, in those contexts, ‘a threat to national security must be genuine\nand present, or at the very least, foreseeable, which presupposes that sufficiently\nconcrete circumstances have arisen.’209\nPrevention\n(347) Contrary to Article 5(1)(h)(i) and Article 5(1)(h)(iii) AI Act, the scenario described in\nArticle 5(1)(h)(ii) does not specify that the use of real-time RBI is permitted to locate\nor identify a concrete person. Its purpose is the prevention of a particular threat."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n105\n- sabotage,\n- participation in a criminal organisation involved in one or more of the offences listed\nabove. a) Localisation and identification\n(349) A Member State may authorise the use of real-time RBI in publicly accessible spaces\nfor law enforcement purposes to locate and identify a suspect of a criminal offence to\nconduct a criminal investigation, prosecute that person for the crime committed, or\nexecute an existing sentence. b) Suspects and Perpetrators\n(350) Article 5(1)(h)(iii) AI Act covers two categories of individuals: suspects and\nperpetrators."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nA suspect is a person with regard to whom there are serious grounds for\nbelieving that they have committed a criminal offence , and sufficient evidence of that\nperson’s involvement in the offence has already been gathered. A perpetrator is a person\nwho is accused or convicted of having committed a criminal offence . The same\nconditions (crime listed in Annex II and maximum punishment of at least four years)\napply to locate or identify the accomplice of the crimes listed in Annex II AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 83", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 83]\n\nc) List of serious crimes\n(351) Only serious crimes justify the use of real -time RBI systems in publicly accessible\nspaces for law enforcement purposes. (352) The first five offences listed in Annex II AI Act are the same as the ‘euro crimes’; listed\nin Article 83 TFEU, while the other offences constitute priorities for law enforcement\ncooperation.210 Some of them ( e.g., kidnapping, illicit trafficking in nuclear or\nradioactive materials) may be linked to terrorism.211\n(353) Although all the criminal offences listed in Annex II may trigger the issuance of a\nEuropean Arrest Warrant (‘EAW’) against a suspect or perpetrator, the use of real-time\nRBI to locate and identify a suspect for one of these serious criminal offences does not\nrequire that an EAW has been issued."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(354) Moreover, to use real-time RBI for this purpose, the respective criminal offence must\nbe punishable in the Member State concerned by a custodial sentence or a detention\norder for a maximum period of at least four years. During a busy festival in a city, police authorities deploy live facial recognition\ntechnologies to monitor the area around the festival and identify wanted individuals\nwith outstanding arrest warrants for illegal drug trafficking and sexual offences."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n106\nFirst, concerning the offence types, RBI can be used in case of illegal drug\ntrafficking. However sexual offences are not on the list of offences, unless they relate\nto the sexual exploitation of children, child sexual abuse material, or rape. The police\nare not allowed to deploy real-time facial recognition technologies in a broad,\nuntargeted manner, i.e. in the hope of finding wanted criminal s and taking them off\nthe streets."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nThe case is different if the police have received a physical description with a\nphotograph of a wanted individual that is subject to a European Arrest Warrant for\ndrug trafficking and they have reasons to believe that he will be present at the festival. In those circumstances , deploying real -time facial recognition technologies to\nidentify a targeted individual may be covered by Article 5(1)(h)(iii) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nAfter a serious terror attack at a Christmas market with 12 deaths, the police uses\nreal-time facial recognition technologies to identify the offender and to see to where\nhe is escaping. In that context they als o use the real -time facial recognition\ntechnologies of the nearby train station and at destination stations of the trains leaving\nfrom there shortly after the attack. In the case of a terror attack, such use can be\npermitted under Article 5(1)(h)(iii) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(355) A link between Article 5(1)(h)(i) and Article 5(1)(h)(iii) AI Act may be made for the\ncrimes covered by the scenario described in Article 5(1)(h)(i) AI Act. While real-time\nRBI systems may be deployed to find a victim or a missing person, those systems may\nalso be used to locate and identify the perpetrator or suspect of trafficking in human\nbeings, sexual exploitation as far as it concerns children (as listed in Annex II), and\nkidnapping (as far as the abduction mentioned in Article 5(1)(h)(i) AI Act qualifies as\nkidnapping as listed in Annex II AI Act)."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article \n5]\n\nA link may also be made between Article\n5(1)(h)(ii) and (iii) AI Act: real-time RBI systems may be used to prevent a threat falling\nwithin the scope of Article 5(1)(h)(ii) and, if that threat materialises, those systems may\nbe used to identify/locate the perpetrator ‘on the move’. 10. SAFEGUARDS AND CONDITIONS FOR THE EXCEPTIONS (ARTICLE 5(2)-\n(7) AI ACT)\n10.1."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n107\n(a) the nature of the situation giving rise to the possible use, in particular the\nseriousness, probability and scale of the harm that would be caused if the\nsystem were not used;\n(b) the consequences of the use of the system for the rights and freedoms of\nall persons concerned, in particular the seriousness, probability and scale\nof those consequences. In addition, the use of ‘real-time’ remote biometric identification systems in publicly\naccessible spaces for the purposes of law enforcement for any of the objectives\nreferred to in paragraph 1, first subparagraph, point (h), of this Article shall comply\nwith necessary and proportionate safeguards and conditions in relation to the use in\naccordance with the national law authorising the use thereof, in particular as regards\nthe temporal, geographic and personal limitations."}
{"meta": {"section_type": "guideline", "article_ref": "Article  27", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article  27]\n\nThe use of the ‘real -time’ remote\nbiometric identification system in publicly accessible spaces shall be authorised only\nif the law enforcement authority has completed a fundamental rights impact\nassessment as provided for in Article 27 and has registered the system in the EU\ndatabase according to Article 49. However, in duly justified cases of urgency, the use\nof such systems may be commenced without the registration in the EU database,\nprovided that such registration is completed without undue delay.’\n(356) The use of real-time RBI systems for one of the objectives listed in Article 5(1)(h)(i) to\n(iii) AI Act is subject to certain safeguards and conditions, which are detailed in Article\n5(2) to Article 5(7) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(357) First, the use of real-time RBI systems in publicly accessible spaces for law\nenforcement purposes is only allowed to ‘confirm the identity of the specifically\ntargeted individual.’ This first condition aims to balance the seriousness of the situation\nand the harm resulting from not using the system with the impact of the technology on\nindividuals’ rights and freedoms. It aims to avoid mass surveillance by targeting an\nindividual for the deployment of real-time RBI."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nAs a consequence, the deployment of\na real-time RBI system in publicly accessible spaces for law enforcement purposes\nshould only be authorized for targeted individuals. (358) The use of the expression ‘confirming the identity’ , as opposed to ‘identification’, is\nmeant as an additional safeguard for the fundamental rights limiting the risk of\nindiscriminate surveillance and implies that the identification of an individual within\nthe meaning of Article 5(1)(h) AI Act must be targ eted."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nThat expression should be\nunderstood as meaning that the use of real-time RBI may only be initiated to search for\nspecific individuals for which the law enforcement authorities have reasons to believe\nor are informed that they are victims of the crimes listed in Article 5(1)(h)(i) AI Act or\nare involved in one of the scenarios described in Article 5(1)(h)(ii) or Article 5(1)(h)(iii)\nAI Act. This means, in practice, a comparison of the data collected real -time with the\ndata contained in the reference database."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n108\nnot necessarily need to know the identity of the individuals they are searching for before\nusing the system . I f they have factual indications and information about a planned\nterrorist attack by a terrorist group (without knowing who will execute the plan) at a\nspecific time and place, the RBI system may be used to identify the offender from the\nterrorist group, provided the law enforcement authorities have constituted a reference\ndatabase containing the biometric data of the individuals forming part of the terrorist\ngroup."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nIn all three scenarios described in Article 5(1)(h)(i) to (iii) AI Act, ‘confirming\nthe identity’ may also include the localisation of the person in question. (359) Second, before using the system, the nature of the situation giving rise to the possible\nuse, in particular, the seriousness, probability and scale of the harm for natural persons,\nsociety and law enforcement purposes that would be caused if the system were not used,\nshould be assessed against the consequences of the use of the system on the rights and\nfreedoms of the persons concerned, in particular, the seriousness, probability and scale\nof those consequences."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nThis should include evaluating whether less intrusive alternative\nsolutions are available to the law enforcement authorities or entit ies acting on their\nbehalf. For example, law enforcement authorities are prohibited from using real-time facial\nrecognition systems in the street based on general security, crime prevention and over-\ncrowding concerns, since that would involve the constant monitoring and surveillance\nof all persons, it is not limited in time , and it would therefore not meet the criteria for\nthe exception from the prohibition laid down in Article 5(1)(h) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(360) The ‘ seriousness’ criterion , applied here in connection to the possible harm and\nconsequences, implies a variation in degrees of interference with the fundamental rights\nat stake, which is linked to the principle of proportionality. 212 Concerning the\ninterferences with fundamental rights, some interfe rences are viewed as more serious\nthan others. (361) The ‘ scale’ criterion refers , in particular, to the number and categories of persons\naffected by the interference (including children and vulnerable or marginalised\npersons)."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(362) Finally, the ‘probability’ is the likelihood that an event will occur. (363) The assessment of the seriousness, scale and probability of the harm and consequences\nshould all be part of the Fundamental Rights Impact Assessment that the law\nenforcement authority is obliged to complete (see below) . That assessment will be\nconcluded on a case-by-case basis. (364) Third, the real-time use of RBI should be clearly limited in terms of geographic scope,\nduration, and the targeted person. This is to ensure that the RBI system is only used\nwhen strictly necessary."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n109\n(365) Concerning the geographic restriction, it may cover one or several geographical areas\nbased on ‘objective and non -discriminatory factors’ . In the case of biometric\nidentification this implies that the geographic restriction applies to a clearly delineated\nboundary for which there are indications that the event will take place. Such a\ndelineation should -under normal circumstances- not comprise an entire city or country,\nbut should be more targeted. (366) Another safeguard relates to the personal scope of the measure, i.e. defining the\ncategories of persons concerned ."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nThis would exclude the untargeted, indiscriminat e\nidentification of persons, without further indications of an incident. (367) Finally, the time limit is a period limited to what is strictly necessary, but which may\nbe extended in case of need in accordance with the applicable rules. The use of real -\ntime RBI systems therefore cannot be for an indefinite or vague period of time. The\nperiod needs to be determined in light of the concrete indications that lead to the use of\nRBI systems."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(368) Fourth, before deployment, the law enforcement authority deploying the real-time RBI\nsystem must have conducted a Fundamental Right Impact Assessment (FRIA) and\nregistered the system in the EU database (except in a duly justified case). 10.1.1. Fundamental Rights Impact Assessment\n(369) FRIAs carried out in application of Article 5(2) AI Act must comply with the conditions\nlaid down in Article 27 AI Act. Th at provision sets out the requirements concerning\nFRIAs applicable to high-risk AI systems."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(370) In the period between when the prohibitions in Article 5 AI Act become applicable\n(after 2 February 2025) but the provisions on high-risk AI systems are not yet applicable\n(before 2 August 2026), the requirements for FRIA set out in Article 27 AI Act should\nbe implemented by the deployers of real-time RBI systems meeting the conditions to\nbenefit from one or more of the exceptions in Article 5(1) (h) AI Act ."}
{"meta": {"section_type": "guideline", "article_ref": "Article 27", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 27]\n\nThe following\nprovisional guidance relates only to the use of real -time RBI in publicly accessible\nspaces for law enforcement purposes for the period before the obligations for high-risk\nAI systems become applicable and the Commission adopts the template for FRIA and\nprovides further guidance on the obligation under Article 27 AI Act. (371) A FRIA is a new type of impact assessment that aims to identify the impact that certain\nhigh-risk AI systems, including RBI systems, may produce on fundamental rights. A\nFRIA is an accountability tool."}
{"meta": {"section_type": "guideline", "article_ref": "Article 27", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 27]\n\nThe FRIA does not replace the existing Data Protection\nImpact Assessment (DPIA) that data controllers (i.e. those responsible for the\nprocessing of personal data) must perform under Article 27 LED, Article 35 GDPR or\nArticle 39 EUDPR. For example, a DPIA must be conducted when biometric data are processed through\nnew technologies likely to result in a high risk to the rights and freedoms of natural\npersons (such as CCTV, AI facial recognition, and body -worn cameras) in publicly\naccessible spaces."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n110\n(372) Whereas a DPIA focuses on the risks to the rights and freedoms of individuals resulting\nfrom the processing of their personal data , a FRIA covers the possible impact of AI\nsystems on individuals’ fundamental rights more generally . The scope of a FRIA is\ntherefore broader in terms of activities covered and fundamental rights assessed. Where\npersonal data are processed by the AI system (which is the case of RBI systems), the\nFRIA should complement the DPIA performed by the deployer as data controller,213\nwithout covering aspects already addressed in the DPIA and avoiding overlap s."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nThe\nanalysis of the FRIA in these Guidelines is limited to the authorized use of RBI in real-\ntime and is aimed to serve as a preliminary guidance for deployers in this interim period\nbefore the AI Office provides a template.214\n(373) The obligation to carry out the FRIA under Article 5(2) AI Act is imposed on the\ndeployers of the RBI system , and not entities or bodies or anyone else acting on their\nbehalf."}
{"meta": {"section_type": "guideline", "article_ref": "Article 27", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 27]\n\n(375) According to Article 27 AI Act, a FRIA should include the following information:\n• A description of the RBI use and the deployer’s processes for the use, together with\nthe intended purpose of use\nThe description should include:\n- the name of the deployer;\n- the law enforcement purpose(s) for which the real-time RBI system will be used;\n- the description of the reference database against which the biometric identification\nwill be compared, including the sources of the biometric data (facial images, voice\nsamples, etc.) that will be used;\n- the description of the technology underlying the system to explain its functioning (by\nreferencing the available documentation provided by the provider and its name);215\n- the legal basis on which the real-time RBI will be deployed."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n111\n• The categories of persons and groups affected by the system\nFor the purpose of the exception in Article 5(1)(h) AI Act, the FRIA should distinguish\nbetween:\n- The targeted individual, who may be a victim of a crime, a perpetrator, or a suspect,\n- The individuals whose biometric data are included in the reference database, and\n- Categories of persons who are present in the surrounding areas where the RBI system\nwill be deployed. The use of real-time RBI systems will not only affect the fundamental rights of the\ntargeted individual."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n• The specific risks of harm to the affected persons\n(376) The fundamental rights that may be affected by the use of real-time RBI in publicly\naccessible spaces for law enforcement purposes include, in particular:\n- the right to private and family life , including people’s reasonable expectation of\nanonymity in public spaces;\n- the right to data protection, as RBI systems rely on the processing of biometric data\nand other personal data (e.g., names, ID numbers, as well as sensitive data such as\nethnicity) to identify specific individuals;\n- freedom of thought, conscience and religion, freedom of expression and freedom of\nassembly and association in the public spaces being searched, on which the use of\nRBI systems could have a chilling effect, preventing individuals to fully exercise\ntheir rights and freedoms, since if individuals know that they are monitored, th ey\nmight change their behaviour , or even prevent themselves to behave in a certain\nmanner;\n- the right to an effective remedy and a fair trial;\n- the right to non-discrimination if the system embeds biases (such as gender, ethnic\nor racial biases) and leads to the misidentification of a suspect or perpetrator;\n- the right to human dignity by the feeling of being reduced to an object of the system;\n- the presumption of innocence and right t o defence; since no decision adversely\naffecting an individual may be solely taken on the output of the real -time RBI\nsystem;\n- the rights of the child in case the victim, missing person or suspect is a minor;\n- the rights of the elderly in the case of a missing person."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n113\nOther considerations for human oversight and monitoring under Articles 14 and 26 AI\nAct are also relevant and should be described. • Risk mitigation measures\nBeyond implementing human oversight measures (including to avoid discriminatory\nmeasures), the deployer should explain redress measures in case a risk materialises ,\nincluding the governance procedures and the complaint mechanisms (such as in the\ncase of a misidentification). 10.1.2."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nRegistration of the authorized RBI systems\n(377) Article 5(2) AI Act also obliges the deployer of a real-time RBI system used in publicly\naccessible spaces for law enforcement purposes to register the system in the EU\ndatabase provided for in Article 49 AI Act . However , in case s of a duly justified\nemergency (such as an imminent threat), deployment may start even prior to\nregistration, provided the law enforcement authority registers the system without undue\ndelay."}
{"meta": {"section_type": "guideline", "article_ref": "Article 49", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 49]\n\nUndue delay should be understood as meaning ‘as soon as possible’ considering\nthe circumstances of the emergency that prevented the registration of the system prior\nto its use. Whether registration meets that criterion requires an appreciation on a case-\nby-case basis. It cannot be defined a priori with a precise time limit. The delay should\nnot be caused by a deliberate action. According to Article 49(4 ) AI Act, RBI systems\nused for the purpose of law enforcement will be registered in a se cure non -public\nsection of the database, with limited information and limited access to that information."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nFor example, requesting law enforcement authorities to register the RBI system\nwithin 24 hours of the use might be considered a reasonable delay where the system\nwas deployed in a situation of an imminent threat to life, such as in the scenario of a\nlive shooter. 10.2. Need for prior authorisation\n(378) Article 5(3) AI Act requires prior authorisation of each individual use of a real -time\nRBI system and prohibits automated decision-making based solely on the output of\nsuch a system which produces an adverse legal effect."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nArticle 5(3) AI Act provides:\nFor the purposes of paragraph 1, first subparagraph, point (h) and paragraph 2, each\nuse for the purposes of law enforcement of a ‘real -time’ remote biometric\nidentification system in publicly accessi ble spaces shall be subject to a prior\nauthorisation granted by a judicial authority or an independent administrative\nauthority whose decision is binding of the Member State in which the use is to take\nplace, issued upon a reasoned request and in accordanc e with the detailed rules of\nnational law referred to in paragraph 5."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n114\n24 hours. If such authorisation is rejected, the use shall be stopped with immediate\neffect and all the data, as well as the results and outputs of that use shall be\nimmediately discarded and deleted. The competent judicial authority or a n independent administrative authority whose\ndecision is binding shall grant the authorisation only where it is satisfied, on the basis\nof objective evidence or clear indications presented to it, that the use of the ‘real -\ntime’ remote biometric identificat ion system concerned is necessary for, and\nproportionate to, achieving one of the objectives specified in paragraph 1, first\nsubparagraph, point (h), as identified in the request and, in particular, remains limited\nto what is strictly necessary concerning the period of time as well as the geographic\nand personal scope."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article \n5]\n\nIn deciding on the request, that authority shall take into account\nthe elements referred to in paragraph 2. No decision that produces an adverse legal\neffect on a person may be taken based so lely on the output of the ‘real-time’ remote\nbiometric identification system. 10.2.1. Objective\n(379) The objective for requiring prior authorisation (‘authorisation ex ante’) for any use of a\n‘real-time’ RBI system in publicly accessible spaces for law enforcement purposes is\nthe need for an assessment and a decision as to whether any envisaged use of such a\nsystem for such purposes is:\n- necessary and proportionate to achieve any one of the objectives listed in Article\n5(1)(h)(i) to (iii) , i.e."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nfor the targeted search of specific victims, the prevention of\nspecific threats, or the localisation or identification of offenders; and\n- limited to what is strictly necessary concerning the time period and the geographic\nand personal scope. (380) The consequence of th ese requirements is that a double necessity and proportionality\nassessment should occur prior to the deployment of any real -time RBI system in\npublicly accessible spaces for law enforcement purposes . First, an assessment should\nbe made by the user when performing a FRIA , as required by Article 5 (2) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nSecond, in accordance with Article 5(3) AI Act, a judicial or independent administrative\nauthority must also assess the necessity and proportionality of using such a system\nwithin the limits of the national law providing the legal basis for any such use, taking\nthe Charter and other Union law into consideration. As a consequence, any such system\nmay only be used 1) after a FRIA and 2) when the competent national authority has\nauthorised such use."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(381) Article 5(3) AI Act must be read and understood in conjunction with Article 5(5) AI\nAct: for the use of a real -time RBI system to be authorized, a national law adopted in\nthe Member States concerned must exist authorising such use.217 Certain Member States\nalready have a system of prior authorisation in place for the use of biometric systems\nunder other Union or national law, such as data protection law."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article \n5]\n\n115\n10.2.2. The main principle: Prior authorisation by a judicial authority or an\nindependent administrative authority\n(382) The use of real-time RBI systems which pursue one of the objectives listed in Article\n5(1)(h)(i) to (iii ) AI Act and which has been provided for in the national law of the\nMember States concerned must be authorized by a judicial authority or an independent\nadministrative authority prior to its use. This is the main principle. (383) However, there is an exception in the case of urgency."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nThis shall be duly justified.218\nUrgency is described as ‘situations where th e need to use the systems concerned is\nsuch as to make it effectively and objectively impossible to obtain an authorisation\nbefore commencing the use of the AI system’ .219 In such case of urgency, ‘the use of\nthe AI system should be restricted to the absolute minimum necessary and should be\nsubject to appropriate safeguards and conditions , as determined in national law and\nspecified in the context of each individual urgent use case by the law enforc ement\nauthority itself.’\n10.2.2.1."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 3]\n\nPrior and reasoned request in accordance with national procedural rules\na) Request by whom? (384) Whilst not specified, it may be assumed that the request will normally be initiated by\nthe deployer, i.e. by the competent (law enforcement) authority . According to the\ndefinition of law enforcement authority under Article 3(45) b) AI Act, any ‘other body\nor entity entrusted by Member State law to exercise public authority and public powers\nfor the purposes of the prevention, investigation, detection or prosecution of criminal\noffences or the execution of criminal penalties, including the safeguarding against and\nthe prevention of threats to public security’ is considered a law enforcement authority\nand could also be the responsible as the ‘competent authority’ for submitting the request\nfor prior authorisation."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(385) The use of a real-time RBI system for activities falling outside the scope of the AI Act\ndoes not need to be authorised under Article 5(3) AI Act. If subsequently such a system\nis being used for law enforcement purposes, the use would fall within the scope of the\nAI Act and an authorisation would be required where the requirements of Article 5(1)(h)\nAI Act are met. b) Request for which use?"}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article \n5]\n\n116\n- An organisation entrusted with resources for searching for missing children decides\nto use a real-time RBI system . It has no mandate to exercise public authority and\npublic powers or for preventing criminal offences or tasks for the prevention of threats\nto public security. Such use does not fall under the prohibition laid down in Article\n5(1)(h) AI Act, since it is not for law enforce ment purposes."}
{"meta": {"section_type": "guideline", "article_ref": "Article 36", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 36]\n\nThat system will,\nhowever, be categorised as ‘high-risk’ (point 1 (a) of Annex III) and a requirement of\nprior consultation of the su pervisory data protection authority may be necessary\npursuant to Article 36 GDPR. Depending on the applicable national law and whether\none of the exceptions to Article 9(1) GDPR applies, a prior authorisation may also be\nrequired for such processing. By contrast, if the same organisation were requested by\nlaw enforcement authorities to act on their behalf for the search of missing children in\na law enforcement context and under the supervision and instructions of the competent\nlaw enforcement authorities, prior authorisation would be needed pursuant to Article\n5(3) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n- A private organisation entrusted with providing resources to aid persons who risk\nbecoming victims of a natural disaster 220 decides to use a real -time RBI system for\nthat purpose. Such use does not fall under the prohibition laid down in Article 5(1)(h)\nAI Act, since it is not for law enforcement purposes. That system will, however, be\ncategorised as ‘high-risk’ (point 1 (a) of Annex III) and a requirement of prior\nconsultation of the supervisory data protection authority may be necessary pursuant to\nArticle 36 GDPR."}
{"meta": {"section_type": "guideline", "article_ref": "Article 9", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 9]\n\nDepending on the applicable national law and whether one of the\nexceptions to Article 9(1) GDPR applies , a prior authorisation may also be required\nfor such processing. c) When? ‘Each use’\n(387) In accordance with Article 5(3) AI Act, prior authorisation is needed for ‘each use’. This implies that the decisive moment for obtaining such authorisation is not the\nmoment prior to installing real-time RBI systems, but each concrete use thereof."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nFor example,\n- the police install biometric ready CCTV cameras at the main train station of a city (no\nauthorisation under the AI Act is needed, but the biometric system must comply with\nthe requirements on high-risk systems, a FRIA must be prepared prior to the first use\nand an individual authorisation by a judicial or independent administrative authority\nis needed before each individual use of the system). The police has concrete indications that a terrorist will arrive by train in the town\n(prior authorisation is needed for real-time identification)."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n118\nc) Authorisation only if ‘necessary and proportionate’ to achieving one of the\nobjectives set forth in the exceptions\n(395) Any authorisation to use real -time RBI in publicly accessible spac es for law\nenforcement purposes must assess whether the requirements of Article 5(3) AI Act have\nbeen met. Highly intrusive\n(396) In the data protection context, the use of biometric data, in particular facial recognition\ntechnology, has been considered by the European Data Protection Board (EDPB) in its\nGuidelines 5/2022 and the European Data Protection Supervisor (EDPS) as affecting\nseveral fundamental rights and freedoms."}
{"meta": {"section_type": "guideline", "article_ref": "Article 52", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 52]\n\nThat view is shared by The EU Agency for\nFundamental Rights and by the Council of Europe .226 Both the CJEU 227 and the\nECtHR228 have confirmed the sensitive nature of processing biometric data. (397) Any interference in fundamental rights and freedoms must always respect the\nessence of the rights and freedoms. This follows from Article 52(1) Charter. (398) The notion of ‘essence’ of fundamental rights and freedoms has been developed in the\nCJEU’s case-law and is an independent value in the Union’s legal order."}
{"meta": {"section_type": "guideline", "article_ref": "Article 52", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 52]\n\nIf the essence\nof a fundamental right or freedom is not respect ed, it means that a right or freedom is\nunduly touched by a measure so that no interference shall be allowed upfront. Only ‘if necessary and proportionate’\n(399) Any interference with fundamental rights and freedoms requires ‘a law’ that should in\nprinciple respect the necessity and proportionality pursuant to Article 52 Charter."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(See\nbelow under Article 5(5) AI Act.) Article 5(3) AI Act requires that the national law\npermitting the use of real -time RBI in publicly accessible spaces for law enforcement\npurposes must provide that authorisation for such use is pe rmitted ‘only where it [the\nauthority] is satisfied [..] that the use is necessary for, and proportionate to, achieving\none of the objectives specified ’ in Article 5(1)(h) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nNational authorities need to\nverify whether the biometric identification is strictly necessary .229 This assessment\nshould be based on the FRIA that should have already include an assessment of the\nnecessity and the proportionality in a general manner prior to requesting the\nauthorisation for each use with the specific circumstances. 10.2.2.3. The exception to the requirement of prior authorisation: request\nwithin 24 hours and consequences if rejected\n(400) In cases of urgency, a user may submit a request for authorisation within 24 hours as\nfrom the moment that the real-time RBI system is used."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n119\ndeployed and the first biometric comparison is made with the system. The logging of\nthe processing activities should be made available to the authority to substantiate the\ntimeliness of the request.230\n(401) In such a case, the request should motivate why no prior request was submitted prior to\nstarting using the system. 10.2.2.4. Immediate cessation in case the request for authorisation is rejected\nand deletion of the data\n(402) Article 5(3 ) AI Act further provides that if an authorisation request in the case of\nurgency is rejected, the use of the real -time RBI system should be ceased with\nimmediate effect."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nIn such cases, all the data, including the results and outputs of that\nuse, must be immediately discarded and deleted.231 Article 5(3) AI Act is explicit in this\nregard, without exception. The deployer will have:\na) a reference database, containing the biometric information ( e.g., facial images,\nvoice snippets, ….) and related identifying information, if applicable, against which\nb) captured biometric information from individuals present in the publicly accessible\nspace is compared to identify and single out those individuals. c) This comparison will lead to the comparison result."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(403) The requirement to discard and delete the data collected and processed also means that\nthe reference database(s) used for the unauthorized biometric identification must be\nremoved and deleted if it was built specifically for the contested search. Only where the\nlaw enforcement authorities had built and intended to maintain the database used for\nidentification in a lawful manner for legitimate aims other than for the unauthorized\nuse of real-time RBI may the database be maintained."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n(404) Besides the deletion of any (unlawful) database with biometric information, all the\ncollected images and other personal data, including the meta data, technical processing\ndata, including the templates and other personal data, and other comparison - and output\ndata obtained during the unlawful use of the real-time RBI system must also be deleted. (405) Where the law enforcement authority challenges the rejection, the data may be kept by\na trustee until a final decision has been taken on the request."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nDuring that period, those\ndata should normally not be placed at the disposal of the law enforcement authority.310\n10.2.2.5. No decision-making solely on the output of the real-time RBI system\n(406) In accordance with Article 5(3) AI Act , even where the deployer of a real -time RBI\nsystem obtains an authorisation, no decision that produces an adverse legal effect on a\nperson may be taken based solely on the output of the ‘real-time’ RBI system."}
{"meta": {"section_type": "guideline", "article_ref": "Article 14", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 14]\n\n120\n- A person is arrested and imprisoned for a serious crime solely based on identification\nby a facial recognition system, without any further checks. This comes on top of the\nrequirement under Article 14 AI Act for human oversight. Checks could relate for\nexample to the question whether a given person has been at a different place or also\nwhether there are other reasons for that the person cannot be the person searched."}
{"meta": {"section_type": "guideline", "article_ref": "Article 14", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 14]\n\nRequirements of Article 14 AI Act for Human Oversight\n(407) The use of real-time RBI that is permitted because it pursues one of the objectives listed\nin Article 5(1)(h) and complies with Articles 5(2) -(6) AI Act still falls under the rules\nfor high-risk systems. In accordance with Article 14 AI Act, high-risk AI systems ‘shall\nbe designed and developed in such a way, including with appropriate human -machine\ninterface tools, that they can be effectively overseen by natural persons during the\nperiod in which they are in use.’ Pursuant to Article 14(5) AI Act, no action or decision\nmay be taken by the deployer on the basis of the identification resulting from the\nsystem, ‘unless that identification has been separately verified and confirmed by at least\ntwo natural persons with the necessary competence, training and a uthority’ or unless\n‘Union or national law considers the application of this requirement to be\ndisproportionate’."}
{"meta": {"section_type": "guideline", "article_ref": "Article 4", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 4]\n\nArticle 4 AI Act prescribes AI literacy measures for providers and\nusers of AI systems to ensure ‘a sufficient level of AI literac y of their staff and other\npersons dealing with the operation and use of AI systems’ and considering the persons\non whom the systems are to be used. (408) As stated by the EDPB in the data protection context , for human oversight to be\neffective it is crucial ‘to enable the person to understand the (in that case facial\nrecognition) system and its limits as well as to interpret its results properly."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nIt is also\nnecessary to establish a workplace and organisation that counteracts the effects of\nautomation bias, and av oids fostering the uncritical acceptance of the results e.g., by\ntime pressure, burdensome procedures, potential detrimental career effects etc .’232\nSimilar considerations may apply in the context of the AI Act. 10.3. Notification to the authorities of each use of ‘real -time’ remote biometric\nidentification systems in publicly accessible spaces for law enforcement\nArticle 5(4) AI Act provides:\nWithout prejudice to paragraph 3, each use of a ‘real -time’ remote biometric\nidentification system in publicly accessible spaces for law enforcement purposes shall\nbe notified to the relevant market surveillance authority and the national data protection\nauthority in accordance with the national rules referred to in paragraph 5."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nThe\nnotification shall, as a minimum, contain the information specified under paragraph 6\nand shall not include sensitive operational data. (409) Each use of an RBI system pursuing one of the objectives listed in Article 5(1)(h)(i)-\n(iii) AI Act must be notified to the relevant market surveillance authority and the\nnational data protection authority."}
{"meta": {"section_type": "guideline", "article_ref": "Article 3", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 3]\n\n121\nto be able to report about the number of authorisations and their result. The notification\ndoes not need to include sensitive operational data. According to Article 3 (38) AI Act,\n‘sensitive operational data’ means operational data related to law enforcement activities\n(prevention, detection, investigation or prosecution of criminal offences), the disclosure\nof which could jeopardise the integrity of criminal proceedings. (410) For the details on the reporting requirement, see section 10.6 below. 10.4. Need for national laws within the limits of the AI Act exceptions\n10.4.1."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nPrinciple: national law required to provide the legal basis for the\nauthorisation for all or some of the exceptions\n(411) National laws are required for operationali sing the use of ‘real -time’ RBI systems in\npublicly accessible sp aces for the purposes of law enforcement. At the same time,\nArticle 5(5) AI Act provides that Member States remain free to decide whether to adopt\nsuch national laws."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nIf a national law authorising the use of real-time RBI is adopted, the\nAI Act specifies the substantive elements which the national laws must contain to\ncomply with the requirements laid down in the AI Act. Article 5(5) AI Act\nA Member State may decide to provide for the possibility to fully or partially authorise\nthe use of ‘real -time’ remote biometric identification systems in publicly accessible\nspaces for the purposes of law enforcement within the limits and under the conditions\nlisted in paragraph 1, first subpara graph, point (h), and paragraphs 2 and 3."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nMember\nStates concerned shall lay down in their national law the necessary detailed rules for\nthe request, issuance and exercise of, as well as supervision and reporting relating to,\nthe authorisations referred to in paragraph 3. Those rules shall also specify in respect of\nwhich of the objectives listed in paragraph 1, first subparagraph, point (h), including\nwhich of the criminal offences referred to in point (h)(iii) thereof, the competent\nauthorities may be authorised to use those systems for the purposes of law enforcement."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nMember States shall notify those rules to the Commission at the latest 30 days following\nthe adoption thereof. Member States may introduce, in accordance with Union law,\nmore restrictive laws on the use of remote biometric identification systems. 10.4.2. National law shall respect the limits and conditions of Article 5(1)(h) AI\nAct\n(412) Since the use of ‘real -time’ RBI systems in publicly accessible spaces for law\nenforcement purposes is considered an interference with fundamental rights, Article\n5(5) AI Act provides that such use shall be established by national law in the Member\nStates."}
{"meta": {"section_type": "guideline", "article_ref": "Article \n5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article \n5]\n\n122\naccessible spaces for law enforcement purposes beyond those listed in Article\n5(1)(h)(i)-(iii) AI Act.233\n(414) Member States shall notify their national laws to the Commission at the latest 30 days\nfollowing the adoption thereof . Such notification does not provide a presumption of\nconformity of the Member States’ law with the AI Act . The AI Office, after receiving\nthe notification, will send a confirmation of receipt. Prior to its adoption, Member States\nare also encouraged to send a preliminary version of the national (or regional) law\nproposed to the AI Office."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nIn any case, non-notification to the AI Office within the legal\ndeadline of 30 days following the adoption set out in Article 5(5) may imply that the\nnational law is unenforceable in legal proceedings , as has been held in different\ncontexts.234 The Commission will publish the Member States’ laws on a public website. (415) Member States may introduce, in accordance with Union law, more restrictive laws, i.e. laws with stricter requirements than those laid down in Article 5(1)(h) and (2) to (7) AI\nAct. 10.4.3."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nDetailed national law on the authorisation request, the issuance and the\nexercise\n(416) As regards the detailed rules that apply for the request, the issuance and the exercise of\nthe authorisation, these are to be determined by national law. Each Member State that\nwishes to allow the use of the systems at issue must specify in its national laws such\nrules, which are aimed at providing relevant and complete information as to the use of\nreal-time RBI systems to the authorising authority to enable it to decide as to the strict\nnecessity and proportionality of such use."}
{"meta": {"section_type": "guideline", "article_ref": "Article 70", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 70]\n\n124\n(417) Article 70 AI Act obliges the Member States to ‘establish at least one notifying and at\nleast one market surveillance authority.’ Article 74(8) AI Act provides that ‘Member\nStates shall designate as market surveillance authorities for the purposes of this\nRegulation either the compete nt data protection supervisory authorities under\nRegulation (EU) 2016/679 or Directive (EU) 2016/680, or any other authority\ndesignated pursuant to the same conditions laid down in Articles 41 to 44 of Directive\n(EU) 2016/680.’\n(418) This comes on top of the designation of the authorising authority, which the Member\nState will have to establish before it can authorise the use of real-time RBI systems for\nany of the objectives listed in Article 5(1)(h)(i) to (iii) AI Act."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n10.5. Annual reports by the national market surveillance authorities and the\nnational data protection authorities of Member States\nArticle 5(6) AI Act provides\nNational market surveillance authorities and the national data protection authorities of\nMember States that have been notified of the use of ‘real -time’ remote biometric\nidentification systems in publicly accessible spac es for law enforcement purposes\npursuant to paragraph 4 shall submit to the Commission annual reports on such use."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nFor\nthat purpose, the Commission shall provide Member States and national market\nsurveillance and data protection authorities with a template , including information on\nthe number of the decisions taken by competent judicial authorities or an independent\nadministrative authority whose decision is binding upon requests for authorisations in\naccordance with paragraph 3 and their result. (419) The national market surveillance authorities and the national data protection authorities\nof Member States that have been informed by deployers of the use of real -time RBI\nsystems in publicly accessible spaces for law enforcement purposes (see Article 5 (4))\nmust submit annual reports on such use to the Commission."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nThese reports shall be made\non the basis of a template provided by the Commission. This template will be\nestablished in due time. (420) Where the deployer is an EU Institution, body, or agency, the EDPS is obliged to inform\nthe Commission accordingly on an annual basis of the real -time RBI systems used in\npublicly accessible spaces for law enforcement purposes. (421) Only the report of the national data protection authority will cover the period between\n2 February 2025 and 2 August 2025, since the AI Act does not require Member States\nto appoint a national market surveillance authority before the latter date."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n125\nThe Commission shall publish annual reports on the use of real -time remote biometric\nidentification systems in publicly accessible spaces for law enforcement purposes,\nbased on aggregated data in Member States on the basis of the annual reports referred\nto in paragraph 6. Those annual reports shall not include sensitive operational data of\nthe related law enforcement activities. (423) The AI Act requires the Commission to publish annual reports of the uses of real-time\nRBI systems in publicly accessible spaces for law enforcement purposes in the Member\nstates and by Union institutions, agencies and bodies, based on aggregated data."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nThese\nreports will be based on the information notified by the national authorities pursuant to\nArticle 5(6) AI Act. (424) The Commission’s annual report shall not contain sensitive operational data. Sensitive\noperational data means ‘operational data related to activities of prevention, detection,\ninvestigation or prosecution of criminal offences, the disclosure of which could\njeopardise the integrity of criminal proceedings ’.236 This could mean that specific\ndetails that reveal ongoing or past investigations, such as e.g., locations, camera’s used,\nshall not be published. 10.7."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nOut-of-Scope\n(425) All other uses of RBI systems that are not covered by the prohibition of Article 5(1)(h)\nAI Act fall within the category of high-risk AI systems as defined by Article 6 and listed\nin point 1(a) of Annex III AI Act provided they fall within the scope of the AI Act. (426) RBI systems that fall outside the scope of the prohibition in Article 5(1)(h) AI Act\ninclude biometric verification/authentication systems and the retrospective use of (post-\n) RBI systems in publicly accessible spaces for law enforcement purposes."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nFor instance,\npolice authorities might be authoris ed by national law to perform retrospective facial\nrecognition to compare images of criminal suspects with recorded facial images in a\ncriminal database.237 Another use that falls outside the scope of the prohibition is the\nuse of real-time RBI systems for law enforcement purposes in either a private (such as\nat somebody’s place) or an online space (such as the use of a chat room or online game\nto identify a suspect of disseminating child sexual abuse material)."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nFinally, the use of\nRBI systems by private actors, both in real -time and retrospective (such as the use of\nlive facial recognition technology by a supermarket to identify known shoplifters, the\nuse of live facial recognition technology by a sports arena to identify individuals banned\nfrom entering the arena, or the use of live facial recognition technology in schools for\nsecurity purposes and school attendance) fall outside the scope of the prohibition."}
{"meta": {"section_type": "guideline", "article_ref": "Article 9", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 9]\n\n126\n(428) Uses for purposes other than for law enforcement must in any event comply with\ndata protection rules . The cases below illustrate the interpretation of Article 9(2)\nGDPR in cases of such use and the exceptions to process biometric data. For example,\n- A French administrative Court found that the trial of live facial recognition\ntechnology in two public schools for access control and security purposes was\nneither necessary nor proportionate (under data protection rules) ."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nAlternative\nsolutions that were less intrusive for the students were avail able, e.g., the use of\nbadges. In addition, the conditions for explicit consent were not met. Therefore,\nconsent could not be used as a valid legal basis to trial facial recognition technology\nin high schools.239\n- A supermarket was not allowed to use live facial recognition technology to prevent\nshoplifting in the Netherlands."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nWithout explicit consent from the customer or any\nlegal basis allowing the processing for a substantial public interest (such as security\npurposes), the supermarket could not process biometric data and thus deploy facial\nrecognition technology.240\n- The use of live facial recognition technology at the entrance of a football club to\nidentify supporters was prohibited in France241 and to ensure the safety of spectators\nwas prohibited in Spain.242\n10.8."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nExamples of uses\nThe police instals mobile CCTV cameras equipped with AI-based facial recognition\ntechnologies on a police van around the main entrance of a football stadium during a\nEuropean Championship match to secure the area and identify individuals whose\nfaces are re corded in an ad hoc watchlist database of wanted individuals. This\nwatchlist includes persons suspected of having committed a crime (ranging from\nserious crimes to frauds and burglaries), persons of possible interests for intelligence\npurposes, and vulnerable persons with mental issues."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\n128\nrecognition by comparing the extracted images with photographs posted on social\nmedia. The retrospective use of facial recognition technology is not prohibited by the AI\nAct. That use is considered high-risk and should comply with the requirements in the\nAI Act for such systems.243\nFurther examples of NOT prohibited practices:\n- Hotels using real -time RBI to recognise VIP guests. This is not law\nenforcement. - Shopping malls using real-time RBI to find shoplifters. This is not law\nenforcement."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nProhibited:\nEntrusted by the police, a shopping mall is using real -time RBI to find\nshoplifters. The system is deployed for law enforcement purposes, in a\npublicly accessible space. The use is prohibited because the search for\nshoplifters does not fall under any of the exceptions of Article 5(1)(h) AI Act. 11. ENTRY INTO APPLICATION\n(429) According to Article 113 AI Act, Article 5 AI Act applies as from 2 February 2025."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\nThe prohibitions in that provision will apply in principle to all AI system regardless of\nwhether they were placed on the market or put into service before or after that date.244\n(430) At the same time, the chapters on governance and penalties will become applicable on\n2 August 2025. Consequently, the provisions on penalties for non-compliance with the\nprohibitions in Article 5 AI Act will not apply before 2 August 2025 . In this interim\nperiod, there will also be no market surveillance authorities to monitor whether the\nprohibitions are being properly complied with."}
{"meta": {"section_type": "guideline", "article_ref": "Article 5", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 5]\n\n(431) Nevertheless, even in this interim period , the prohibitions are fully applicable and\nmandatory for providers and deployers of AI systems. Those operators should therefore\ntake necessary measures to ensure that they do not place on the market, put into service\nor use AI systems that could constitute prohibited practices under Article 5 AI Act. Even if the provisions on monitoring and fines do not apply until a later date, t he\nprohibitions themselves have direct effect and thus enable affected parties to enforce\nthem in national courts and request interim injunctions against the prohibited practices."}
{"meta": {"section_type": "guideline", "article_ref": "Article 10", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement] [Article: Article 10]\n\n243 The processing of biometric data for a law enforcement purpose remains subject to Article 10 of the LED, which needs to be\nimplemented at national level. Their processing to perform the retrospective use of FRT should only be allowed if it is stric tly\nnecessary and should be subject to appropriate safeguards. Whether the retrospective use of FRT is strictly necessary to identify the\ndemonstrator is questionable."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "Law Enforcement", "guideline_source": "EU Commission"}, "text": "[Domain: Law Enforcement]\n\nIn the Glukhin v Russia judgment that serves as a basis for this scenario, the ECtHR ruled that while\ncrime detection can be a legitimate aim, the use of FRT, both retrospective and live, was disproportionate as there were no r isks to\npublic order or transport safety. The Court emphasized the ‘highly intrusive’ nature of FRTs. In that case, the Court concluded that\nusing FRTs did not answer a pressing social need, nor was it necessary in a democratic society."}
{"meta": {"section_type": "guideline", "article_ref": "Article 50", "guideline_source": "BSA"}, "text": "[Article-specific] [Article: Article 50]\n\ns (Art. 50 of the EU AI Act)\nFebruary 2026\nThe Business Software Alliance (BSA) wanted to raise concerns about the draft Code of Practice (CoP) on\nthe transparency requirements deriving from Article 50 of the EU AI Act.\nBSA is the global trade association representing the enterprise (B2B) software industry. Our members 1\noperate at the forefront of AI, cybersecurity, cloud computing, and data -driven innovation. In particular,\nBSA members are on the leading edge of providing AI -enabled products and services. As such, they have\nunique insights into the technology’s potential to spur digital transformation and practices that can best\nsupport the responsible development and use of AI.\nBSA and its members are determined for the Code to be successful with many companies wanting to sign\non to it. This is directly linked to the EU’s ambition to be a leading AI continent with innovative AI\ncompanies. However, the current draft does not meet this competitiveness requirement and, as it stands,\nwould further create complexity, burden and cost for businesses developing and deploying generative AI\nsystems in Europe."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "finance", "guideline_source": "FSC"}, "text": "[Domain: Finance]\n\nAI Guidelines: Background to the Discussion\n(Background) Changes in technological aspects and regulatory environment → Necessity of revision of AI guidelines in the financial sector\n- Rapid technological development such as generative AI and AI Agent and expansion of the scope of AI use within financial companies\n- Enactment of the “Basic Act on the Development of Artificial Intelligence and Creation of Trust Base, etc.” in January 2025, announcement of draft subordinate laws in September, and announcement of enforcement ordinance in November.\n→ Unification of the existing three guidelines (①Operation, ②Development, ③Security)* into the ‘Financial AI Guidelines’\n* ①「Financial AI Operation Guideline (July 2021)」, ②「Financial AI Development and Utilization Guide」(August 2022), ③「Financial AI Security Guideline (April 2023)」\n4\n(Application) Financial companies and non-financial companies that use AI when providing financial services and products\n- (Work) AI-based credit evaluation, loan screening, chatbot, fraud detection system (FDS), etc.\n- (Applicable to) Financial companies (banks, financial investment companies, etc.) + Non-financial companies (AI utilization results impact financial transactions)\n- This principle will be regulated in the form of best practice and self-regulation by industry, and we plan to continuously collect opinions and improve and supplement on a regular basis.\nChanges in technology and regulatory environment\nGuidelines revised to reflect\nIncreased need (July ‘21)\n「AI in the financial sector\n“Operating Guidelines”\n(April ‘23)\n「AI in the financial sector\n“Security Guidelines”\n(August ‘22)\n「AI in the financial sector\n“Development and Utilization Guide”\npast present future\nGenerative AI and\nEmergence of AI Agents\n&\nBasic Act on Artificial Intelligence\nEnacted (’25."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "FSC"}, "text": "[Domain: Credit]\n\nAI Guidelines: Background to the Discussion\n(Background) Changes in technological aspects and regulatory environment → Necessity of revision of AI guidelines in the financial sector\n- Rapid technological development such as generative AI and AI Agent and expansion of the scope of AI use within financial companies\n- Enactment of the “Basic Act on the Development of Artificial Intelligence and Creation of Trust Base, etc.” in January 2025, announcement of draft subordinate laws in September, and announcement of enforcement ordinance in November.\n→ Unification of the existing three guidelines (①Operation, ②Development, ③Security)* into the ‘Financial AI Guidelines’\n* ①「Financial AI Operation Guideline (July 2021)」, ②「Financial AI Development and Utilization Guide」(August 2022), ③「Financial AI Security Guideline (April 2023)」\n4\n(Application) Financial companies and non-financial companies that use AI when providing financial services and products\n- (Work) AI-based credit evaluation, loan screening, chatbot, fraud detection system (FDS), etc.\n- (Applicable to) Financial companies (banks, financial investment companies, etc.) + Non-financial companies (AI utilization results impact financial transactions)\n- This principle will be regulated in the form of best practice and self-regulation by industry, and we plan to continuously collect opinions and improve and supplement on a regular basis.\nChanges in technology and regulatory environment\nGuidelines revised to reflect\nIncreased need (July ‘21)\n「AI in the financial sector\n“Operating Guidelines”\n(April ‘23)\n「AI in the financial sector\n“Security Guidelines”\n(August ‘22)\n「AI in the financial sector\n“Development and Utilization Guide”\npast present future\nGenerative AI and\nEmergence of AI Agents\n&\nBasic Act on Artificial Intelligence\nEnacted (’25."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "education", "guideline_source": "FSC"}, "text": "[Domain: Education]\n\n1. Governance principles (3/3)\nEstablish and implement all procedures for risk control, such as performing control and management differentiated by risk level and monitoring, documentation, and education.\n9\nBackground of discussion/ Seven principles/ Conclusion\n(Example) Differentiated control and management implementation plan according to AI service risk level\n- Apply basic control measures to all AI services, but apply relaxed control measures to low-risk AI services to promote operational efficiency.\n- High-risk AI services are subject to additional controls, such as AI Ethics Committee pre-approval and post-verification, evaluation and verification by a third party, and strengthened monitoring of the operation stage.\n- In accordance with the Framework Act on AI, ‘high-impact AI’ is classified as ‘high risk’ regardless of grading according to risk score to strengthen control.\n(Reference) Financial Supervisory Service’s “AI Risk Management Framework (AI RMF) in the Financial Sector”\n10\nBackground of discussion/ Seven principles/ Conclusion\nTo help financial companies manage AI risks flexibly and systematically throughout the entire introduction and utilization cycle of AI systems,\nPresents core processes related to ① governance, ② risk assessment, and ③ risk control.\n※ Specific details such as governance, risk assessment, and risk control are scheduled to be disclosed by the Financial Supervisory Service in the future.\nYou can refer to the 「AI Risk Management Framework in Financial Sector」\n2."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "finance", "guideline_source": "FSC"}, "text": "[Domain: Finance]\n\n1. Governance principles (3/3)\nEstablish and implement all procedures for risk control, such as performing control and management differentiated by risk level and monitoring, documentation, and education.\n9\nBackground of discussion/ Seven principles/ Conclusion\n(Example) Differentiated control and management implementation plan according to AI service risk level\n- Apply basic control measures to all AI services, but apply relaxed control measures to low-risk AI services to promote operational efficiency.\n- High-risk AI services are subject to additional controls, such as AI Ethics Committee pre-approval and post-verification, evaluation and verification by a third party, and strengthened monitoring of the operation stage.\n- In accordance with the Framework Act on AI, ‘high-impact AI’ is classified as ‘high risk’ regardless of grading according to risk score to strengthen control.\n(Reference) Financial Supervisory Service’s “AI Risk Management Framework (AI RMF) in the Financial Sector”\n10\nBackground of discussion/ Seven principles/ Conclusion\nTo help financial companies manage AI risks flexibly and systematically throughout the entire introduction and utilization cycle of AI systems,\nPresents core processes related to ① governance, ② risk assessment, and ③ risk control.\n※ Specific details such as governance, risk assessment, and risk control are scheduled to be disclosed by the Financial Supervisory Service in the future.\nYou can refer to the 「AI Risk Management Framework in Financial Sector」\n2."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "FSC"}, "text": "[Domain: Credit]\n\n1. Governance principles (3/3)\nEstablish and implement all procedures for risk control, such as performing control and management differentiated by risk level and monitoring, documentation, and education.\n9\nBackground of discussion/ Seven principles/ Conclusion\n(Example) Differentiated control and management implementation plan according to AI service risk level\n- Apply basic control measures to all AI services, but apply relaxed control measures to low-risk AI services to promote operational efficiency.\n- High-risk AI services are subject to additional controls, such as AI Ethics Committee pre-approval and post-verification, evaluation and verification by a third party, and strengthened monitoring of the operation stage.\n- In accordance with the Framework Act on AI, ‘high-impact AI’ is classified as ‘high risk’ regardless of grading according to risk score to strengthen control.\n(Reference) Financial Supervisory Service’s “AI Risk Management Framework (AI RMF) in the Financial Sector”\n10\nBackground of discussion/ Seven principles/ Conclusion\nTo help financial companies manage AI risks flexibly and systematically throughout the entire introduction and utilization cycle of AI systems,\nPresents core processes related to ① governance, ② risk assessment, and ③ risk control.\n※ Specific details such as governance, risk assessment, and risk control are scheduled to be disclosed by the Financial Supervisory Service in the future.\nYou can refer to the 「AI Risk Management Framework in Financial Sector」\n2."}
{"meta": {"section_type": "guideline", "article_ref": "제10조", "domain": "education", "guideline_source": "FSC"}, "text": "[Domain: Education] [Article: Article 10]\n\nlegality principle\nUnderstand applicable laws and regulations in advance when developing and using AI, reflect them in internal policies and work procedures, and periodically inspect and improve\n- Review commonly applied laws (Basic Act on Artificial Intelligence) and subordinate laws (hereinafter referred to as “Basic Act on Artificial Intelligence”) and related laws and regulations applicable to each industry.\n- Legal requirements → Reflection in internal policies and work procedures, periodic inspection → Effectiveness evaluation and continuous improvement\n11\nBackground of discussion/ Seven principles/ Conclusion\nClassification Related laws\ncommon\n❶ Compliance with business conduct regulations for financial consumer protection Articles 10 and 17 to 22 of the Financial Consumer Protection Act\n❷ Ensuring the safety of information processing systems and establishing and implementing protective measures Articles 14 and 21 of the Electronic Financial Supervision Regulations\n❸ Establishment of risk management standards and procedures Article 27 (1) of the Financial Corporation Governance Structure Act\n❹ Guaranteeing data subject rights regarding automated decisions\n(When processing personal information)\nArticle 37-2 of the Personal Information Protection Act\nCredit rating, insurance,\ngoddess etc\n❶ Implement explanations on automated evaluation, etc.\n(When processing personal credit information)\nArticle 36-2 Paragraph 1 of the Credit Information Act\n❷ Compliance with the principles of personal credit evaluation Article 22-3 of the Credit Information Act\nFinancial investment, etc.\n❶ Requirements for the use of electronic investment advice devices Article 2 of the Enforcement Decree of the Capital Markets Act and Article 1-2-2 of the Financial Investment Business Regulations\n❷ Preparation, issuance, storage, etc. of discretionary investment reports of electronic investment advisory devices, Article 99 of the Capital Markets Act, Article 100 of the Enforcement Decree of the same Act, Articles 4-78 and 4-13 of the Financial Investment Business Regulations.\nFinancial sector laws and regulations\n3."}
{"meta": {"section_type": "guideline", "article_ref": "제10조", "domain": "finance", "guideline_source": "FSC"}, "text": "[Domain: Finance] [Article: Article 10]\n\nlegality principle\nUnderstand applicable laws and regulations in advance when developing and using AI, reflect them in internal policies and work procedures, and periodically inspect and improve\n- Review commonly applied laws (Basic Act on Artificial Intelligence) and subordinate laws (hereinafter referred to as “Basic Act on Artificial Intelligence”) and related laws and regulations applicable to each industry.\n- Legal requirements → Reflection in internal policies and work procedures, periodic inspection → Effectiveness evaluation and continuous improvement\n11\nBackground of discussion/ Seven principles/ Conclusion\nClassification Related laws\ncommon\n❶ Compliance with business conduct regulations for financial consumer protection Articles 10 and 17 to 22 of the Financial Consumer Protection Act\n❷ Ensuring the safety of information processing systems and establishing and implementing protective measures Articles 14 and 21 of the Electronic Financial Supervision Regulations\n❸ Establishment of risk management standards and procedures Article 27 (1) of the Financial Corporation Governance Structure Act\n❹ Guaranteeing data subject rights regarding automated decisions\n(When processing personal information)\nArticle 37-2 of the Personal Information Protection Act\nCredit rating, insurance,\ngoddess etc\n❶ Implement explanations on automated evaluation, etc.\n(When processing personal credit information)\nArticle 36-2 Paragraph 1 of the Credit Information Act\n❷ Compliance with the principles of personal credit evaluation Article 22-3 of the Credit Information Act\nFinancial investment, etc.\n❶ Requirements for the use of electronic investment advice devices Article 2 of the Enforcement Decree of the Capital Markets Act and Article 1-2-2 of the Financial Investment Business Regulations\n❷ Preparation, issuance, storage, etc. of discretionary investment reports of electronic investment advisory devices, Article 99 of the Capital Markets Act, Article 100 of the Enforcement Decree of the same Act, Articles 4-78 and 4-13 of the Financial Investment Business Regulations.\nFinancial sector laws and regulations\n3."}
{"meta": {"section_type": "guideline", "article_ref": "제10조", "domain": "credit", "guideline_source": "FSC"}, "text": "[Domain: Credit] [Article: Article 10]\n\nlegality principle\nUnderstand applicable laws and regulations in advance when developing and using AI, reflect them in internal policies and work procedures, and periodically inspect and improve\n- Review commonly applied laws (Basic Act on Artificial Intelligence) and subordinate laws (hereinafter referred to as “Basic Act on Artificial Intelligence”) and related laws and regulations applicable to each industry.\n- Legal requirements → Reflection in internal policies and work procedures, periodic inspection → Effectiveness evaluation and continuous improvement\n11\nBackground of discussion/ Seven principles/ Conclusion\nClassification Related laws\ncommon\n❶ Compliance with business conduct regulations for financial consumer protection Articles 10 and 17 to 22 of the Financial Consumer Protection Act\n❷ Ensuring the safety of information processing systems and establishing and implementing protective measures Articles 14 and 21 of the Electronic Financial Supervision Regulations\n❸ Establishment of risk management standards and procedures Article 27 (1) of the Financial Corporation Governance Structure Act\n❹ Guaranteeing data subject rights regarding automated decisions\n(When processing personal information)\nArticle 37-2 of the Personal Information Protection Act\nCredit rating, insurance,\ngoddess etc\n❶ Implement explanations on automated evaluation, etc.\n(When processing personal credit information)\nArticle 36-2 Paragraph 1 of the Credit Information Act\n❷ Compliance with the principles of personal credit evaluation Article 22-3 of the Credit Information Act\nFinancial investment, etc.\n❶ Requirements for the use of electronic investment advice devices Article 2 of the Enforcement Decree of the Capital Markets Act and Article 1-2-2 of the Financial Investment Business Regulations\n❷ Preparation, issuance, storage, etc. of discretionary investment reports of electronic investment advisory devices, Article 99 of the Capital Markets Act, Article 100 of the Enforcement Decree of the same Act, Articles 4-78 and 4-13 of the Financial Investment Business Regulations.\nFinancial sector laws and regulations\n3."}
{"meta": {"section_type": "guideline", "article_ref": "제34조", "domain": "education", "guideline_source": "FSC"}, "text": "[Domain: Education] [Article: Article 34]\n\nAuxiliary instrumentality principle\nExecutives and employees are ultimately responsible for AI output, and employee intervention is differentiated throughout all stages of operation.\n- Artificial intelligence should be used as a work aid, and final decisions and responsibilities should be made by executives and employees. - Determine roles and responsibilities for each decision-making stage corresponding to task importance and risk level (e.g. RACI chart). - However, high-impact AI is only used to assist in human decision-making (Human-in-the-loop)\n12\nBackground of discussion/ Seven principles/ Conclusion\nCategory Credit Review Manager Credit Review Team Leader Risk\nManagement department\ncompliance\nWatchman IT Operations\n① Application submission/confirmation of basic requirements R I I I I\n② Model score calculation and recommendation display C I C I R\n③ Judgment of intervention criteria and supplementation of data R A C C C\n④ First review of review opinion/statement of reasons R I C I I\n⑤ Review by superiors/confirmation by multiple people R A C C I\n⑥ Final decision/notification/record I A/R C C C\nNote: A RACI chart refers to a responsibility division structure chart that clarifies who will be responsible for, approve, refer to, and be notified of what during the work process (Responsible: Responsible performer,\nAccountable: Final Responsible Person, Consulted: Opinion Presenter, Informed: Notified Person)\n(Example) Constructing a responsibility performance system using the RACI chart\n(Reference) Human intervention principles and use of auxiliary means\n13\nBackground of discussion/ Seven principles/ Conclusion\nArticle 34 of the Act (Obligations of business operators related to high-impact artificial intelligence) ① Artificial intelligence business operators must use high-impact artificial intelligence or\nTo ensure the safety and reliability of high-impact artificial intelligence when providing products and services using it\nMeasures including the following items must be implemented as prescribed by Presidential Decree."}
{"meta": {"section_type": "guideline", "article_ref": "제34조", "domain": "finance", "guideline_source": "FSC"}, "text": "[Domain: Finance] [Article: Article 34]\n\nAuxiliary instrumentality principle\nExecutives and employees are ultimately responsible for AI output, and employee intervention is differentiated throughout all stages of operation.\n- Artificial intelligence should be used as a work aid, and final decisions and responsibilities should be made by executives and employees. - Determine roles and responsibilities for each decision-making stage corresponding to task importance and risk level (e.g. RACI chart). - However, high-impact AI is only used to assist in human decision-making (Human-in-the-loop)\n12\nBackground of discussion/ Seven principles/ Conclusion\nCategory Credit Review Manager Credit Review Team Leader Risk\nManagement department\ncompliance\nWatchman IT Operations\n① Application submission/confirmation of basic requirements R I I I I\n② Model score calculation and recommendation display C I C I R\n③ Judgment of intervention criteria and supplementation of data R A C C C\n④ First review of review opinion/statement of reasons R I C I I\n⑤ Review by superiors/confirmation by multiple people R A C C I\n⑥ Final decision/notification/record I A/R C C C\nNote: A RACI chart refers to a responsibility division structure chart that clarifies who will be responsible for, approve, refer to, and be notified of what during the work process (Responsible: Responsible performer,\nAccountable: Final Responsible Person, Consulted: Opinion Presenter, Informed: Notified Person)\n(Example) Constructing a responsibility performance system using the RACI chart\n(Reference) Human intervention principles and use of auxiliary means\n13\nBackground of discussion/ Seven principles/ Conclusion\nArticle 34 of the Act (Obligations of business operators related to high-impact artificial intelligence) ① Artificial intelligence business operators must use high-impact artificial intelligence or\nTo ensure the safety and reliability of high-impact artificial intelligence when providing products and services using it\nMeasures including the following items must be implemented as prescribed by Presidential Decree."}
{"meta": {"section_type": "guideline", "article_ref": "제34조", "domain": "credit", "guideline_source": "FSC"}, "text": "[Domain: Credit] [Article: Article 34]\n\nAuxiliary instrumentality principle\nExecutives and employees are ultimately responsible for AI output, and employee intervention is differentiated throughout all stages of operation.\n- Artificial intelligence should be used as a work aid, and final decisions and responsibilities should be made by executives and employees. - Determine roles and responsibilities for each decision-making stage corresponding to task importance and risk level (e.g. RACI chart). - However, high-impact AI is only used to assist in human decision-making (Human-in-the-loop)\n12\nBackground of discussion/ Seven principles/ Conclusion\nCategory Credit Review Manager Credit Review Team Leader Risk\nManagement department\ncompliance\nWatchman IT Operations\n① Application submission/confirmation of basic requirements R I I I I\n② Model score calculation and recommendation display C I C I R\n③ Judgment of intervention criteria and supplementation of data R A C C C\n④ First review of review opinion/statement of reasons R I C I I\n⑤ Review by superiors/confirmation by multiple people R A C C I\n⑥ Final decision/notification/record I A/R C C C\nNote: A RACI chart refers to a responsibility division structure chart that clarifies who will be responsible for, approve, refer to, and be notified of what during the work process (Responsible: Responsible performer,\nAccountable: Final Responsible Person, Consulted: Opinion Presenter, Informed: Notified Person)\n(Example) Constructing a responsibility performance system using the RACI chart\n(Reference) Human intervention principles and use of auxiliary means\n13\nBackground of discussion/ Seven principles/ Conclusion\nArticle 34 of the Act (Obligations of business operators related to high-impact artificial intelligence) ① Artificial intelligence business operators must use high-impact artificial intelligence or\nTo ensure the safety and reliability of high-impact artificial intelligence when providing products and services using it\nMeasures including the following items must be implemented as prescribed by Presidential Decree."}
{"meta": {"section_type": "guideline", "article_ref": "제27조", "domain": "education", "guideline_source": "FSC"}, "text": "[Domain: Education] [Article: Article 27]\n\n4. Human management and supervision of high-impact artificial intelligence\nEnforcement Decree (Draft) Article 27 (Obligations of business operators related to high-impact artificial intelligence) ① Artificial intelligence business operators\nAmong the measures stipulated in each subparagraph of Article 34 (1) of the Act, the following contents shall be posted on one's website, etc.\nMust be posted. (omitted below)\n4. Name and contact information of the person who manages and supervises the relevant high-impact artificial intelligence\nBusiness Responsibilities Notice (Draft) Article 7 (Management and Supervision of Personnel) ① Business operators shall\nFor human management and supervision, the following measures must be implemented. 1. Establishment of standards for human intervention in artificial intelligence operations\n2. ‘Emergency stop’ function, which allows a person to immediately stop the artificial intelligence system or change its operation\nDeveloping intervention methods\n② Business operators must implement the following measures for human management and supervision during high-impact artificial intelligence operation.\nDo it. 1. Establish regular inspection plans and measures for performance degradation and error occurrence\n2."}
{"meta": {"section_type": "guideline", "article_ref": "제27조", "domain": "finance", "guideline_source": "FSC"}, "text": "[Domain: Finance] [Article: Article 27]\n\n4. Human management and supervision of high-impact artificial intelligence\nEnforcement Decree (Draft) Article 27 (Obligations of business operators related to high-impact artificial intelligence) ① Artificial intelligence business operators\nAmong the measures stipulated in each subparagraph of Article 34 (1) of the Act, the following contents shall be posted on one's website, etc.\nMust be posted. (omitted below)\n4. Name and contact information of the person who manages and supervises the relevant high-impact artificial intelligence\nBusiness Responsibilities Notice (Draft) Article 7 (Management and Supervision of Personnel) ① Business operators shall\nFor human management and supervision, the following measures must be implemented. 1. Establishment of standards for human intervention in artificial intelligence operations\n2. ‘Emergency stop’ function, which allows a person to immediately stop the artificial intelligence system or change its operation\nDeveloping intervention methods\n② Business operators must implement the following measures for human management and supervision during high-impact artificial intelligence operation.\nDo it. 1. Establish regular inspection plans and measures for performance degradation and error occurrence\n2."}
{"meta": {"section_type": "guideline", "article_ref": "제27조", "domain": "credit", "guideline_source": "FSC"}, "text": "[Domain: Credit] [Article: Article 27]\n\n4. Human management and supervision of high-impact artificial intelligence\nEnforcement Decree (Draft) Article 27 (Obligations of business operators related to high-impact artificial intelligence) ① Artificial intelligence business operators\nAmong the measures stipulated in each subparagraph of Article 34 (1) of the Act, the following contents shall be posted on one's website, etc.\nMust be posted. (omitted below)\n4. Name and contact information of the person who manages and supervises the relevant high-impact artificial intelligence\nBusiness Responsibilities Notice (Draft) Article 7 (Management and Supervision of Personnel) ① Business operators shall\nFor human management and supervision, the following measures must be implemented. 1. Establishment of standards for human intervention in artificial intelligence operations\n2. ‘Emergency stop’ function, which allows a person to immediately stop the artificial intelligence system or change its operation\nDeveloping intervention methods\n② Business operators must implement the following measures for human management and supervision during high-impact artificial intelligence operation.\nDo it. 1. Establish regular inspection plans and measures for performance degradation and error occurrence\n2."}
{"meta": {"section_type": "guideline", "article_ref": "제34조", "domain": "education", "guideline_source": "FSC"}, "text": "[Domain: Education] [Article: Article 34]\n\nProvide education and training to improve understanding of the scope and performance of artificial intelligence\n【Related laws】 Article 34 of the Basic Act on Artificial Intelligence / Article 26 of the Enforcement Decree of the same Act (draft) / Article 7 of the Business Responsibility Notice (draft)\nCategory Main contents and design method High risk ·\nHigh Impact AI\nPre-approval process\n◦ (Content) Execution after human review or approval\n◦ (Method) Electronic approval, double approval, committee deliberation, etc.\n◦ (Other) Mandatory recording of reasons/basis\nApply\nAI Recommendations\nIgnore/Modify/Oppose\nOverride\n◦ (Content) The supervisor makes a model recommendation for reasonable reasons.\nGrant authority to ignore, modify or make opposing decisions\n◦ (Method) Provide override UI screen after establishing internal system\n◦ (Other) Standardize and record reasons, evidence, etc.\nRequired application\nEmergency stop function and\nquarantine\n◦ (Content) Immediate stoppage and problem factors when an error occurs\nIsolation, rollback/restart after switching to safe mode\n◦ (Method) Emergency stop. For safe mode conversion, etc.\nProvides intuitive UI\n* Requires securing supervisor access rights\nRequired application\nreal time\nmonitoring\ndashboard\n◦ (Content) Performance, fairness, reliability, regulatory/sanction signals\nReal-time visualization, etc.\n◦ (Method) Automatic notification/reporting when threshold is exceeded\nrecommendation\n[Reference] NIST AI RMF Playbook\nHuman intervention methods for artificial intelligence decision making\n4."}
{"meta": {"section_type": "guideline", "article_ref": "제34조", "domain": "finance", "guideline_source": "FSC"}, "text": "[Domain: Finance] [Article: Article 34]\n\nProvide education and training to improve understanding of the scope and performance of artificial intelligence\n【Related laws】 Article 34 of the Basic Act on Artificial Intelligence / Article 26 of the Enforcement Decree of the same Act (draft) / Article 7 of the Business Responsibility Notice (draft)\nCategory Main contents and design method High risk ·\nHigh Impact AI\nPre-approval process\n◦ (Content) Execution after human review or approval\n◦ (Method) Electronic approval, double approval, committee deliberation, etc.\n◦ (Other) Mandatory recording of reasons/basis\nApply\nAI Recommendations\nIgnore/Modify/Oppose\nOverride\n◦ (Content) The supervisor makes a model recommendation for reasonable reasons.\nGrant authority to ignore, modify or make opposing decisions\n◦ (Method) Provide override UI screen after establishing internal system\n◦ (Other) Standardize and record reasons, evidence, etc.\nRequired application\nEmergency stop function and\nquarantine\n◦ (Content) Immediate stoppage and problem factors when an error occurs\nIsolation, rollback/restart after switching to safe mode\n◦ (Method) Emergency stop. For safe mode conversion, etc.\nProvides intuitive UI\n* Requires securing supervisor access rights\nRequired application\nreal time\nmonitoring\ndashboard\n◦ (Content) Performance, fairness, reliability, regulatory/sanction signals\nReal-time visualization, etc.\n◦ (Method) Automatic notification/reporting when threshold is exceeded\nrecommendation\n[Reference] NIST AI RMF Playbook\nHuman intervention methods for artificial intelligence decision making\n4."}
{"meta": {"section_type": "guideline", "article_ref": "제34조", "domain": "credit", "guideline_source": "FSC"}, "text": "[Domain: Credit] [Article: Article 34]\n\nProvide education and training to improve understanding of the scope and performance of artificial intelligence\n【Related laws】 Article 34 of the Basic Act on Artificial Intelligence / Article 26 of the Enforcement Decree of the same Act (draft) / Article 7 of the Business Responsibility Notice (draft)\nCategory Main contents and design method High risk ·\nHigh Impact AI\nPre-approval process\n◦ (Content) Execution after human review or approval\n◦ (Method) Electronic approval, double approval, committee deliberation, etc.\n◦ (Other) Mandatory recording of reasons/basis\nApply\nAI Recommendations\nIgnore/Modify/Oppose\nOverride\n◦ (Content) The supervisor makes a model recommendation for reasonable reasons.\nGrant authority to ignore, modify or make opposing decisions\n◦ (Method) Provide override UI screen after establishing internal system\n◦ (Other) Standardize and record reasons, evidence, etc.\nRequired application\nEmergency stop function and\nquarantine\n◦ (Content) Immediate stoppage and problem factors when an error occurs\nIsolation, rollback/restart after switching to safe mode\n◦ (Method) Emergency stop. For safe mode conversion, etc.\nProvides intuitive UI\n* Requires securing supervisor access rights\nRequired application\nreal time\nmonitoring\ndashboard\n◦ (Content) Performance, fairness, reliability, regulatory/sanction signals\nReal-time visualization, etc.\n◦ (Method) Automatic notification/reporting when threshold is exceeded\nrecommendation\n[Reference] NIST AI RMF Playbook\nHuman intervention methods for artificial intelligence decision making\n4."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "education", "guideline_source": "FSC"}, "text": "[Domain: Education]\n\nReliability Principle\nEstablish model performance management, data quality assurance, decision-making process explanation, systematic verification and error response system, etc.\n- (Model performance management) Setting clear indicators to measure AI model performance, regular inspection and improvement\n- (Data quality) Verification and confirmation of the quality of data used for AI learning and reference and data input to the AI system\n- (Explainability) AI decision-making process and results ← Provided in an explainable form to enable reasonable understanding by stakeholders (Reliability ↑)\n14\nBackground of discussion/ Seven principles/ Conclusion\n- Result: Rejected\n- Basic information used (main original information of the relevant customer)\n- Main reasons for judgment: number of loans from primary financial institutions (negative), revolving balance\nratio (negative), transaction delinquency ratio (positive), repayment ratio (positive)\nExample of AI explanation provision\nCustomers have a non-overdue transaction rate of 94% and\nThe percentage of credit transactions that were normally repaid in the past was 96%.\nAlthough it was a positive factor,\nNumber of loans from first-tier financial institutions: 18 and credit limit\nThe revolving balance ratio of 63% has a negative effect.\nThe loan was declined."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "finance", "guideline_source": "FSC"}, "text": "[Domain: Finance]\n\nReliability Principle\nEstablish model performance management, data quality assurance, decision-making process explanation, systematic verification and error response system, etc.\n- (Model performance management) Setting clear indicators to measure AI model performance, regular inspection and improvement\n- (Data quality) Verification and confirmation of the quality of data used for AI learning and reference and data input to the AI system\n- (Explainability) AI decision-making process and results ← Provided in an explainable form to enable reasonable understanding by stakeholders (Reliability ↑)\n14\nBackground of discussion/ Seven principles/ Conclusion\n- Result: Rejected\n- Basic information used (main original information of the relevant customer)\n- Main reasons for judgment: number of loans from primary financial institutions (negative), revolving balance\nratio (negative), transaction delinquency ratio (positive), repayment ratio (positive)\nExample of AI explanation provision\nCustomers have a non-overdue transaction rate of 94% and\nThe percentage of credit transactions that were normally repaid in the past was 96%.\nAlthough it was a positive factor,\nNumber of loans from first-tier financial institutions: 18 and credit limit\nThe revolving balance ratio of 63% has a negative effect.\nThe loan was declined."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "FSC"}, "text": "[Domain: Credit]\n\nReliability Principle\nEstablish model performance management, data quality assurance, decision-making process explanation, systematic verification and error response system, etc.\n- (Model performance management) Setting clear indicators to measure AI model performance, regular inspection and improvement\n- (Data quality) Verification and confirmation of the quality of data used for AI learning and reference and data input to the AI system\n- (Explainability) AI decision-making process and results ← Provided in an explainable form to enable reasonable understanding by stakeholders (Reliability ↑)\n14\nBackground of discussion/ Seven principles/ Conclusion\n- Result: Rejected\n- Basic information used (main original information of the relevant customer)\n- Main reasons for judgment: number of loans from primary financial institutions (negative), revolving balance\nratio (negative), transaction delinquency ratio (positive), repayment ratio (positive)\nExample of AI explanation provision\nCustomers have a non-overdue transaction rate of 94% and\nThe percentage of credit transactions that were normally repaid in the past was 96%.\nAlthough it was a positive factor,\nNumber of loans from first-tier financial institutions: 18 and credit limit\nThe revolving balance ratio of 63% has a negative effect.\nThe loan was declined."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "education", "guideline_source": "FSC"}, "text": "[Domain: Education]\n\n(Reference) Data quality management/bias example\n15\nBackground of discussion/ Seven principles/ Conclusion\n□ Skewed Sample: When initial bias occurs by chance, time\nBias amplifies over time\n※ Example) There is a tendency to dispatch more police officers to places with high initial crime rates, and\nCrime rates are more likely to be recorded in these areas\n□ Tainted Example: Human bias present in accumulated data\nThe same bias is replicated when maintained without special correction in the algorithm.\n※ Example) In the Google News article, the relationship between a man and a programmer is similar to the relationship between a woman and a housewife.\nfound to be very similar (Bolukbasi et al., 2016)\n□ Limited Feature: Specific attributes of the data are restricted to a small group of people.\nCollect only limited or low-reliability information\n□ Sample Size Disparity: Learning provided by minority groups\nMinority group when the data is much less than the training data provided by the majority group\nunlikely to accurately model\n□ Presence of proxy variables (Proxy): Sensitive data properties in terms of fairness during learning (proxy)\nEven if species, gender, etc.) are not used, other attributes (neighborhood, etc.) that represent them always exist.\nHowever, if these attributes are included, bias will still occur.\nSource: Research trends related to machine learning fairness (Software Policy Research Institute, February 2020)\n(Example) Causes of AI bias\nItem Description\nnoise\n(Noise) ∙ Error in measured values that occurs randomly during the measurement process\noutlier\n(Outlier)\n∙ Values that exhibit characteristics that are significantly different from the rest of the data\n∙ It may occur due to data input/measurement error/experiment error, but there are some exceptions.\nCan be a value with characteristics\nmissing value\n(Missing Value) ∙ Measurement value missing due to computer errors or non-input.\nmismatch value\n(Mismatch Value) ∙ When measurement data appears different in the same object\nduplicate\n(Duplicate) ∙ When all properties and values are the same\nbias\n(Bias) ∙ Difference between the value measured by the measuring equipment and the actual value\nartifact\n(Artifact)\n∙ Distortion or errors that occur repeatedly due to external factors\n※ (Example) When acquiring image data using a camera, there may be a problem due to stains on the lens.\ncausing continuous distortion, etc.\npollution\n(Poisoning) ∙ Data altered for malicious purposes\n(Example) Elements to be processed during data preprocessing\n5."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "finance", "guideline_source": "FSC"}, "text": "[Domain: Finance]\n\n(Reference) Data quality management/bias example\n15\nBackground of discussion/ Seven principles/ Conclusion\n□ Skewed Sample: When initial bias occurs by chance, time\nBias amplifies over time\n※ Example) There is a tendency to dispatch more police officers to places with high initial crime rates, and\nCrime rates are more likely to be recorded in these areas\n□ Tainted Example: Human bias present in accumulated data\nThe same bias is replicated when maintained without special correction in the algorithm.\n※ Example) In the Google News article, the relationship between a man and a programmer is similar to the relationship between a woman and a housewife.\nfound to be very similar (Bolukbasi et al., 2016)\n□ Limited Feature: Specific attributes of the data are restricted to a small group of people.\nCollect only limited or low-reliability information\n□ Sample Size Disparity: Learning provided by minority groups\nMinority group when the data is much less than the training data provided by the majority group\nunlikely to accurately model\n□ Presence of proxy variables (Proxy): Sensitive data properties in terms of fairness during learning (proxy)\nEven if species, gender, etc.) are not used, other attributes (neighborhood, etc.) that represent them always exist.\nHowever, if these attributes are included, bias will still occur.\nSource: Research trends related to machine learning fairness (Software Policy Research Institute, February 2020)\n(Example) Causes of AI bias\nItem Description\nnoise\n(Noise) ∙ Error in measured values that occurs randomly during the measurement process\noutlier\n(Outlier)\n∙ Values that exhibit characteristics that are significantly different from the rest of the data\n∙ It may occur due to data input/measurement error/experiment error, but there are some exceptions.\nCan be a value with characteristics\nmissing value\n(Missing Value) ∙ Measurement value missing due to computer errors or non-input.\nmismatch value\n(Mismatch Value) ∙ When measurement data appears different in the same object\nduplicate\n(Duplicate) ∙ When all properties and values are the same\nbias\n(Bias) ∙ Difference between the value measured by the measuring equipment and the actual value\nartifact\n(Artifact)\n∙ Distortion or errors that occur repeatedly due to external factors\n※ (Example) When acquiring image data using a camera, there may be a problem due to stains on the lens.\ncausing continuous distortion, etc.\npollution\n(Poisoning) ∙ Data altered for malicious purposes\n(Example) Elements to be processed during data preprocessing\n5."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "FSC"}, "text": "[Domain: Credit]\n\n(Reference) Data quality management/bias example\n15\nBackground of discussion/ Seven principles/ Conclusion\n□ Skewed Sample: When initial bias occurs by chance, time\nBias amplifies over time\n※ Example) There is a tendency to dispatch more police officers to places with high initial crime rates, and\nCrime rates are more likely to be recorded in these areas\n□ Tainted Example: Human bias present in accumulated data\nThe same bias is replicated when maintained without special correction in the algorithm.\n※ Example) In the Google News article, the relationship between a man and a programmer is similar to the relationship between a woman and a housewife.\nfound to be very similar (Bolukbasi et al., 2016)\n□ Limited Feature: Specific attributes of the data are restricted to a small group of people.\nCollect only limited or low-reliability information\n□ Sample Size Disparity: Learning provided by minority groups\nMinority group when the data is much less than the training data provided by the majority group\nunlikely to accurately model\n□ Presence of proxy variables (Proxy): Sensitive data properties in terms of fairness during learning (proxy)\nEven if species, gender, etc.) are not used, other attributes (neighborhood, etc.) that represent them always exist.\nHowever, if these attributes are included, bias will still occur.\nSource: Research trends related to machine learning fairness (Software Policy Research Institute, February 2020)\n(Example) Causes of AI bias\nItem Description\nnoise\n(Noise) ∙ Error in measured values that occurs randomly during the measurement process\noutlier\n(Outlier)\n∙ Values that exhibit characteristics that are significantly different from the rest of the data\n∙ It may occur due to data input/measurement error/experiment error, but there are some exceptions.\nCan be a value with characteristics\nmissing value\n(Missing Value) ∙ Measurement value missing due to computer errors or non-input.\nmismatch value\n(Mismatch Value) ∙ When measurement data appears different in the same object\nduplicate\n(Duplicate) ∙ When all properties and values are the same\nbias\n(Bias) ∙ Difference between the value measured by the measuring equipment and the actual value\nartifact\n(Artifact)\n∙ Distortion or errors that occur repeatedly due to external factors\n※ (Example) When acquiring image data using a camera, there may be a problem due to stains on the lens.\ncausing continuous distortion, etc.\npollution\n(Poisoning) ∙ Data altered for malicious purposes\n(Example) Elements to be processed during data preprocessing\n5."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "education", "guideline_source": "FSC"}, "text": "[Domain: Education]\n\nFinancial Stability Principles\nMinimize financial stability risks throughout the entire process, including AI design and learning.\n- (Financial stability risk assessment and management) Establishment of measures to evaluate and manage the impact of AI systems on the overall financial market and financial stability\n- (Preparation of safety devices) Preparation of safety devices such as use of backup model in case of AI system malfunction, emergency stop device for post-intervention, design of recovery plan, etc.\n- (Third-party IT risk management) Outsource development of AI model or use of open source-based AI → Establish a separate third-party IT risk management plan\n- (Sharing and reporting information to supervisory authorities) Report to the supervisory authorities when an AI incident that may cause system risk occurs.\nInformation sharing is necessary so that supervisory authorities can understand the AI utilization structure in advance.\n16\nBackground of discussion/ Seven principles/ Conclusion\n(Example) Items included when concluding an AI-related third-party contract\n① Clear explanation of functions, risks, etc. for services provided by third parties, etc.\n② Contents regarding cyber security, data and privacy protection\n③ Location of the region or country where the service is provided\n④ In case of bankruptcy of a third party, suspension of business operation, or termination of contract, the relevant services, data, etc.\nInformation regarding access, recovery, and guarantee of return by financial companies, etc.\n⑤ Content related to information sharing, accident handling, and compensation liability in the event of an accident\n⑥ Responsibility in case of AI-related damage\n⑦ Obligation to comply with AI-related laws, regulations, orders and policies, etc.\n⑧ Development and use of safe and reliable AI and compliance with codes of conduct and policies of related financial companies, etc.\n6-7."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "finance", "guideline_source": "FSC"}, "text": "[Domain: Finance]\n\nFinancial Stability Principles\nMinimize financial stability risks throughout the entire process, including AI design and learning.\n- (Financial stability risk assessment and management) Establishment of measures to evaluate and manage the impact of AI systems on the overall financial market and financial stability\n- (Preparation of safety devices) Preparation of safety devices such as use of backup model in case of AI system malfunction, emergency stop device for post-intervention, design of recovery plan, etc.\n- (Third-party IT risk management) Outsource development of AI model or use of open source-based AI → Establish a separate third-party IT risk management plan\n- (Sharing and reporting information to supervisory authorities) Report to the supervisory authorities when an AI incident that may cause system risk occurs.\nInformation sharing is necessary so that supervisory authorities can understand the AI utilization structure in advance.\n16\nBackground of discussion/ Seven principles/ Conclusion\n(Example) Items included when concluding an AI-related third-party contract\n① Clear explanation of functions, risks, etc. for services provided by third parties, etc.\n② Contents regarding cyber security, data and privacy protection\n③ Location of the region or country where the service is provided\n④ In case of bankruptcy of a third party, suspension of business operation, or termination of contract, the relevant services, data, etc.\nInformation regarding access, recovery, and guarantee of return by financial companies, etc.\n⑤ Content related to information sharing, accident handling, and compensation liability in the event of an accident\n⑥ Responsibility in case of AI-related damage\n⑦ Obligation to comply with AI-related laws, regulations, orders and policies, etc.\n⑧ Development and use of safe and reliable AI and compliance with codes of conduct and policies of related financial companies, etc.\n6-7."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "FSC"}, "text": "[Domain: Credit]\n\nFinancial Stability Principles\nMinimize financial stability risks throughout the entire process, including AI design and learning.\n- (Financial stability risk assessment and management) Establishment of measures to evaluate and manage the impact of AI systems on the overall financial market and financial stability\n- (Preparation of safety devices) Preparation of safety devices such as use of backup model in case of AI system malfunction, emergency stop device for post-intervention, design of recovery plan, etc.\n- (Third-party IT risk management) Outsource development of AI model or use of open source-based AI → Establish a separate third-party IT risk management plan\n- (Sharing and reporting information to supervisory authorities) Report to the supervisory authorities when an AI incident that may cause system risk occurs.\nInformation sharing is necessary so that supervisory authorities can understand the AI utilization structure in advance.\n16\nBackground of discussion/ Seven principles/ Conclusion\n(Example) Items included when concluding an AI-related third-party contract\n① Clear explanation of functions, risks, etc. for services provided by third parties, etc.\n② Contents regarding cyber security, data and privacy protection\n③ Location of the region or country where the service is provided\n④ In case of bankruptcy of a third party, suspension of business operation, or termination of contract, the relevant services, data, etc.\nInformation regarding access, recovery, and guarantee of return by financial companies, etc.\n⑤ Content related to information sharing, accident handling, and compensation liability in the event of an accident\n⑥ Responsibility in case of AI-related damage\n⑦ Obligation to comply with AI-related laws, regulations, orders and policies, etc.\n⑧ Development and use of safe and reliable AI and compliance with codes of conduct and policies of related financial companies, etc.\n6-7."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "finance", "guideline_source": "FSC"}, "text": "[Domain: Finance]\n\nBackground of discussion/7 principles/Conclusion\nconclusion\n19\nThese guidelines serve as best practices in the financial industry and encourage autonomous efforts by the industry.\n- There is a need to appropriately reflect the contents of the guidelines depending on the circumstances of each individual financial company.\n- In order to manage AI-related risks, the principle is to share experiences among financial companies and conduct self-regulation by reflecting the characteristics of each industry.\nAs AI-related technology and policy discussions develop, this guideline will be continuously revised.\n- Not long after the rapid introduction of AI technology, policy methods related to the financial sector are currently being actively discussed.\n- In the future, there is a need to proactively identify risk cases in the financial sector and prepare devices to prevent accidents through knowledge sharing with authorities in each country.\n- Details such as changes in security standards according to technological changes will be continuously revised.\nThe Basic Artificial Intelligence Act is scheduled to be revised to continuously reflect the details of the subordinate laws and the opinions of the financial industry.\n- There was a notice of enactment of the Enforcement Decree of the Framework Act on Artificial Intelligence in November 25, but specific details such as other notices and guidelines have not yet been confirmed.\n- Once the future policy direction becomes concrete, the AI guidelines in the financial sector will be revised to reflect this.\n- In addition, the main direction is to continuously collect opinions from the industry and revise these guidelines. thank you\nBaek Yeon-joo, Research Fellow, Korea Institute of Finance\n2025.12."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\n- 2 -\nThis guide synthesizes these policy experiences, systematizes personal information processing and protection issues to be considered at each stage of the generative AI life cycle, and presents the corresponding legal standards and safety measures. Through this, the personal information protection perspective is reflected in a balanced manner in the development and use of generative AI. We aim to contribute to the creation of an AI utilization environment based on trust and responsibility by increasing on-site predictability and autonomous compliance capabilities. In addition, this guide was written with consideration of connectivity and consistency with global discussions, and is provided as a reference for related policy materials from major countries such as the United States, the United Kingdom, and the European Union."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\n- 3 -\n□ Domestic AI personal information processing and protection guide Guide main content Standards for pseudonymization of unstructured data (February 2024)‣ Presentation of standards for pseudonymization of unstructured data such as images, videos, and voices - Detailed information on the entire process of pseudonymization information utilization through 7 scenarios for each field such as medical (CT, MRI), transportation, chatbot, etc. Guide to the processing of public personal information for AI development and service (July 2024)‣ Presentation of legal standards for using information disclosed on the Internet, etc. for AI learning - Presentation of application requirements for ‘legitimate interest’ provisions (§15➀6) of the Personal Information Protection Act and standards for technical and managerial safety measures, etc. Personal video information protection and use guide (’24.10)‣ Presentation of standards for utilizing images captured by mobile image information processing devices such as self-driving cars, robots, drones, etc. for AI development, etc. - Presentation of compliance requirements for each stage of processing of personal image information, such as shooting, use, provision, storage, and destruction. Guide to creating and using synthetic data ('24.12)‣ Detailed procedure information such as due process for each stage of synthetic data creation and utilization, original data pre-processing method, safety and usefulness verification method and indicators, etc. ※Empirical cases of synthetic data creation and verification, etc. are provided in the “Synthetic Data Generation Reference Model” 」(’24.5.) / Synthetic dataset can be downloaded from ‘Pseudonym information support platform (dataprivacy.go.kr) AI privacy risk management model (’24.12.)‣ Guide to reducing AI privacy risks according to AI type, usage, and context - Support for companies to evaluate and reduce privacy risks by AI life cycle □ Reference materials related to overseas AI data processing Reference materials Main contents\nUS NIST Privacy Framework ver1.1 (‘25.4.)‣ A personal information protection and risk management system published by the National Institute of Standards and Technology (NIST), which supports the organization’s autonomous privacy risk identification, evaluation, and management. ※The revised plan (ver. 1.1) that newly created the AI privacy risk management section will be released (April 2025) and the final plan will be released after collecting opinions (~December 2025)\nUK UK AI Playbook (‘25.2.)‣ Practical guidelines to help public institutions introduce and utilize AI safely and responsibly, providing step-by-step guidelines for the entire process of AI introduction and operation, including procurement, design, data management, and privacy protection.\nEUAI Privacy Risks & Mitigations (‘25.4."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "traffic", "guideline_source": "PIPC"}, "text": "[Domain: Transportation]\n\n- 3 -\n□ Domestic AI personal information processing and protection guide Guide main content Standards for pseudonymization of unstructured data (February 2024)‣ Presentation of standards for pseudonymization of unstructured data such as images, videos, and voices - Detailed information on the entire process of pseudonymization information utilization through 7 scenarios for each field such as medical (CT, MRI), transportation, chatbot, etc. Guide to the processing of public personal information for AI development and service (July 2024)‣ Presentation of legal standards for using information disclosed on the Internet, etc. for AI learning - Presentation of application requirements for ‘legitimate interest’ provisions (§15➀6) of the Personal Information Protection Act and standards for technical and managerial safety measures, etc. Personal video information protection and use guide (’24.10)‣ Presentation of standards for utilizing images captured by mobile image information processing devices such as self-driving cars, robots, drones, etc. for AI development, etc. - Presentation of compliance requirements for each stage of processing of personal image information, such as shooting, use, provision, storage, and destruction. Guide to creating and using synthetic data ('24.12)‣ Detailed procedure information such as due process for each stage of synthetic data creation and utilization, original data pre-processing method, safety and usefulness verification method and indicators, etc. ※Empirical cases of synthetic data creation and verification, etc. are provided in the “Synthetic Data Generation Reference Model” 」(’24.5.) / Synthetic dataset can be downloaded from ‘Pseudonym information support platform (dataprivacy.go.kr) AI privacy risk management model (’24.12.)‣ Guide to reducing AI privacy risks according to AI type, usage, and context - Support for companies to evaluate and reduce privacy risks by AI life cycle □ Reference materials related to overseas AI data processing Reference materials Main contents\nUS NIST Privacy Framework ver1.1 (‘25.4.)‣ A personal information protection and risk management system published by the National Institute of Standards and Technology (NIST), which supports the organization’s autonomous privacy risk identification, evaluation, and management. ※The revised plan (ver. 1.1) that newly created the AI privacy risk management section will be released (April 2025) and the final plan will be released after collecting opinions (~December 2025)\nUK UK AI Playbook (‘25.2.)‣ Practical guidelines to help public institutions introduce and utilize AI safely and responsibly, providing step-by-step guidelines for the entire process of AI introduction and operation, including procurement, design, data management, and privacy protection.\nEUAI Privacy Risks & Mitigations (‘25.4."}
{"meta": {"section_type": "guideline", "article_ref": "제15조", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical] [Article: Article 15]\n\n- 10 -\nof disclosed personal information When developing basic models such as collection and use LLM, it has become a practical practice to directly collect large amounts of data through scraping or use large corpora9). These basic models secure extensive knowledge and adaptability to actual reality. Although the need to process disclosed personal information may be recognized for this purpose, there are concerns about rights infringement through indiscriminate scraping, so special attention must be paid to ensuring legality and safety. If personal information is collected in the Internet space without a specific service, companies, institutions and Since a direct relationship is not formed between the information subjects, it may be difficult to apply legal grounds such as consent or contract. In this case, the legal grounds that can be considered in practice include the legitimate interest clause (Article 15, Paragraph 1, Item 6), and in order to satisfy this, ▲ Three criteria must be reviewed: legitimacy of purpose, necessity of processing disclosed personal information, and judgment of interest. In particular, the key is to prepare technical and managerial safety measures and measures to guarantee the rights of information subjects to minimize the possibility of violation of data subject rights at the profit judgment stage. 9) Generally refers to a large-scale natural language text dataset consisting of more than billions of words, collected from various sources such as web documents, news, and papers."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\nRepresentative English-based corpora include Common Crawl, Wikipedia, and OpenWeb Text, and Korean-specific corpora include AI Hub Corpus and Everyone’s Corpus.\nReference ｢Public Personal Information Processing Guide｣(’24."}
{"meta": {"section_type": "guideline", "article_ref": "제28조", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical] [Article: Article 28]\n\n- 13 -\n□3 Developing a new service that is separate from the original collection purpose When using user personal information to develop a new AI service that is separate from the original collection purpose, it is necessary to ▲ use it under a pseudonym or anonymity (Article 28-2 and 58-2 of the Act)10) or ▲ establish a new legal basis (Article 18 (2) of the Act). 10) Refer to “Pseudonymous Information Processing Guidelines” (February 24, 2024, Personal Information Protection Committee)\nArticle 28-2 of the Act (Processing of pseudonymous information, etc.) ① Personal information processors may process pseudonymous information without the consent of the information subject for the purpose of statistical compilation, scientific research, public record preservation, etc. Article 58-2 (Exclusion of application) This Act does not apply to information that can no longer identify an individual even if other information is used when reasonable consideration of time, cost, technology, etc. Article 18 (Other than the purpose of personal information) (Restrictions on use and provision) ② Notwithstanding paragraph 1, the personal information processor may use personal information for purposes other than the intended purpose or provide it to a third party in any of the following cases, except when there is a risk of unfairly infringing on the interests of the information subject or a third party. However, cases pursuant to subparagraphs 5 through 9 are limited to public institutions. 1. When separate consent is obtained from the information subject 2."}
{"meta": {"section_type": "guideline", "article_ref": "제28조", "domain": "finance", "guideline_source": "PIPC"}, "text": "[Domain: Finance] [Article: Article 28]\n\n- 13 -\n□3 Developing a new service that is separate from the original collection purpose When using user personal information to develop a new AI service that is separate from the original collection purpose, it is necessary to ▲ use it under a pseudonym or anonymity (Article 28-2 and 58-2 of the Act)10) or ▲ establish a new legal basis (Article 18 (2) of the Act). 10) Refer to “Pseudonymous Information Processing Guidelines” (February 24, 2024, Personal Information Protection Committee)\nArticle 28-2 of the Act (Processing of pseudonymous information, etc.) ① Personal information processors may process pseudonymous information without the consent of the information subject for the purpose of statistical compilation, scientific research, public record preservation, etc. Article 58-2 (Exclusion of application) This Act does not apply to information that can no longer identify an individual even if other information is used when reasonable consideration of time, cost, technology, etc. Article 18 (Other than the purpose of personal information) (Restrictions on use and provision) ② Notwithstanding paragraph 1, the personal information processor may use personal information for purposes other than the intended purpose or provide it to a third party in any of the following cases, except when there is a risk of unfairly infringing on the interests of the information subject or a third party. However, cases pursuant to subparagraphs 5 through 9 are limited to public institutions. 1. When separate consent is obtained from the information subject 2."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\nIf there are special provisions in other laws3. 4. When it is clearly deemed necessary for the urgent benefit of life, body, or property of the information subject or a third party. Delete <2020. 2. 4.>5."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "finance", "guideline_source": "PIPC"}, "text": "[Domain: Finance]\n\nIf there are special provisions in other laws3. 4. When it is clearly deemed necessary for the urgent benefit of life, body, or property of the information subject or a third party. Delete <2020. 2. 4.>5."}
{"meta": {"section_type": "guideline", "article_ref": "제23조", "domain": "traffic", "guideline_source": "PIPC"}, "text": "[Domain: Transportation] [Article: Article 23]\n\n- 14 -\nOn the other hand, if personal information is used beyond the scope of initial collection without pseudonymization or anonymization or without establishing a new legal basis, it may be judged as use for purposes other than the lawful purpose. However, if the use is innovative or public interest, the basis for processing personal information can be secured subject to certain requirements such as strengthened safety measures by utilizing the regulatory sandbox system. Case 3 Service Regarding the use of user conversation data collected for the purpose of quality improvement to develop a new service (artificial intelligence chatbot) without reasonable relevance without safety measures such as encryption, there is no basis for legal processing, so it was judged to be a use of the user's personal information for purposes other than the intended purpose. ※ Personal Information Protection Commission decision No. 2021-007-072 Reference case 4 When video information collected by a self-driving company is pseudonymized and used for AI learning, there is difficulty in improving the performance of self-driving AI → Compliance with strengthened safety measures using regulatory verification special cases Allows use of original video without consent and pseudonym processing □4 Special personal information If the development of AI services involves the processing of sensitive information, unique identification information, etc., processing can be done only with separate consent or legal grounds. In addition, when personal video information is used for AI learning, it must be processed within the scope of the purpose of installation, operation, and filming."}
{"meta": {"section_type": "guideline", "article_ref": "제23조", "domain": "finance", "guideline_source": "PIPC"}, "text": "[Domain: Finance] [Article: Article 23]\n\n- 14 -\nOn the other hand, if personal information is used beyond the scope of initial collection without pseudonymization or anonymization or without establishing a new legal basis, it may be judged as use for purposes other than the lawful purpose. However, if the use is innovative or public interest, the basis for processing personal information can be secured subject to certain requirements such as strengthened safety measures by utilizing the regulatory sandbox system. Case 3 Service Regarding the use of user conversation data collected for the purpose of quality improvement to develop a new service (artificial intelligence chatbot) without reasonable relevance without safety measures such as encryption, there is no basis for legal processing, so it was judged to be a use of the user's personal information for purposes other than the intended purpose. ※ Personal Information Protection Commission decision No. 2021-007-072 Reference case 4 When video information collected by a self-driving company is pseudonymized and used for AI learning, there is difficulty in improving the performance of self-driving AI → Compliance with strengthened safety measures using regulatory verification special cases Allows use of original video without consent and pseudonym processing □4 Special personal information If the development of AI services involves the processing of sensitive information, unique identification information, etc., processing can be done only with separate consent or legal grounds. In addition, when personal video information is used for AI learning, it must be processed within the scope of the purpose of installation, operation, and filming."}
{"meta": {"section_type": "guideline", "article_ref": "제28조", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical] [Article: Article 28]\n\n- 20 -\nIn addition, companies and institutions must prepare the Enterprise Terms of Use and Data Processing Addendum (DPA) applicable to service-type LLM. It is recommended to clearly stipulate matters such as ▲ownership of input data ▲restrictions on reuse ▲safety measures ▲data destruction, etc., and, if necessary, to secure data protection requirements contractually by adding special provisions considering service characteristics.< Comparison of corporate terms of use and key contents of DPA18) >Old division Company A Company B Company C Data ownershipCompany/institution (customer) Owned AI learning Utilization Reuse/learning prohibitedSafety measuresTLS encryption, access control, log management Equal trust relationship specifiedCustomer = Processor / Company A·B·C=Trustee Specified Re-consignment restrictions Customers can be notified in advance + objections can be raised in case of re-consignment Control clauses related to re-consignment included in DPA Data destruction Within a reasonable period upon customer's request for deletion Destruction management/supervision Customers can perform third-party or internal audits (request 10 days in advance) Audit logs and access records for administrators can be checked Audits can be performed once a year and employee access records and security incident notifications In addition, if the LLM server is located overseas, it is necessary to review whether personal information is transferred overseas and check the related regulations. Article 28-8 of the Act (Overseas Transfer of Personal Information) ① Personal information processors transfer personal information overseas. Provision (including cases of inquiry), entrustment of processing, and storage (hereinafter referred to as “transfer” in this section) shall not be permitted."}
{"meta": {"section_type": "guideline", "article_ref": "제28조", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit] [Article: Article 28]\n\n- 20 -\nIn addition, companies and institutions must prepare the Enterprise Terms of Use and Data Processing Addendum (DPA) applicable to service-type LLM. It is recommended to clearly stipulate matters such as ▲ownership of input data ▲restrictions on reuse ▲safety measures ▲data destruction, etc., and, if necessary, to secure data protection requirements contractually by adding special provisions considering service characteristics.< Comparison of corporate terms of use and key contents of DPA18) >Old division Company A Company B Company C Data ownershipCompany/institution (customer) Owned AI learning Utilization Reuse/learning prohibitedSafety measuresTLS encryption, access control, log management Equal trust relationship specifiedCustomer = Processor / Company A·B·C=Trustee Specified Re-consignment restrictions Customers can be notified in advance + objections can be raised in case of re-consignment Control clauses related to re-consignment included in DPA Data destruction Within a reasonable period upon customer's request for deletion Destruction management/supervision Customers can perform third-party or internal audits (request 10 days in advance) Audit logs and access records for administrators can be checked Audits can be performed once a year and employee access records and security incident notifications In addition, if the LLM server is located overseas, it is necessary to review whether personal information is transferred overseas and check the related regulations. Article 28-8 of the Act (Overseas Transfer of Personal Information) ① Personal information processors transfer personal information overseas. Provision (including cases of inquiry), entrustment of processing, and storage (hereinafter referred to as “transfer” in this section) shall not be permitted."}
{"meta": {"section_type": "guideline", "article_ref": "제30조", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical] [Article: Article 30]\n\nHowever, in any of the following cases, personal information may be transferred overseas. 1. When separate consent for overseas transfer is received from the information subject... 3. When processing and storage of personal information is necessary for the conclusion and performance of a contract with the information subject and any of the following applies a. When the matters in each subparagraph of Paragraph 2 are disclosed in the personal information processing policy pursuant to Article 30 B. When the information subject is notified of the matters specified in each subparagraph of Paragraph 2 in accordance with a method prescribed by Presidential Decree, such as e-mail.\nExample: Case of overseas transfer of call content and personal information when using LLM as a service ▪ In the process of using LLM as a service to provide call recording and summary services to users, personal information, including call content, was transferred to an overseas server ▪ However, some notices regarding overseas transfer were omitted from the personal information processing policy, and it did not have a legal basis pursuant to Article 28-8, Paragraph 1, Item 3 of the Act ※ Personal Information Protection Committee Decision No. 2024-010-184 (private) (results of preliminary inspection of major AI services)18) Reorganization based on enterprise licenses, terms of use, DPA, etc. of major service-type LLMs (‘25."}
{"meta": {"section_type": "guideline", "article_ref": "제30조", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit] [Article: Article 30]\n\nHowever, in any of the following cases, personal information may be transferred overseas. 1. When separate consent for overseas transfer is received from the information subject... 3. When processing and storage of personal information is necessary for the conclusion and performance of a contract with the information subject and any of the following applies a. When the matters in each subparagraph of Paragraph 2 are disclosed in the personal information processing policy pursuant to Article 30 B. When the information subject is notified of the matters specified in each subparagraph of Paragraph 2 in accordance with a method prescribed by Presidential Decree, such as e-mail.\nExample: Case of overseas transfer of call content and personal information when using LLM as a service ▪ In the process of using LLM as a service to provide call recording and summary services to users, personal information, including call content, was transferred to an overseas server ▪ However, some notices regarding overseas transfer were omitted from the personal information processing policy, and it did not have a legal basis pursuant to Article 28-8, Paragraph 1, Item 3 of the Act ※ Personal Information Protection Committee Decision No. 2024-010-184 (private) (results of preliminary inspection of major AI services)18) Reorganization based on enterprise licenses, terms of use, DPA, etc. of major service-type LLMs (‘25."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\n(as of August)\n- 21 -\nEstablished LLM Companies and organizations developing AI systems using pre-learning models (‘established LLM’) corresponding to open-weight models can download the open models from model repositories such as HuggingFace and upload them to their own chain infrastructure or third-party cloud environment. At this time, companies and institutions must make efforts to verify the source of the data set used in the initial learning of the existing LLM. Learning data of unknown origin may contain personal information that is illegal or has been disclosed regardless of the will of the information subject, so special caution is required19). Therefore, the source and history of the learning data It is desirable to first utilize a model that can be verified, and if there are limitations in verifying the source, it is recommended to reduce residual risks by applying various technical and managerial safety measures in the subsequent stages. In addition, check what safety measures are built into the existing LLM itself through model cards, technical documents, licensing policies, etc. It is recommended to apply additional complementary measures through red team testing20), etc. In addition, if the developer of an existing LLM clinic announces a risk discovered after model deployment, it is important to quickly reflect this to supplement the risk management system and secure the safety and reliability of the system by periodically applying the latest version and patch of the model."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\n(as of August)\n- 21 -\nEstablished LLM Companies and organizations developing AI systems using pre-learning models (‘established LLM’) corresponding to open-weight models can download the open models from model repositories such as HuggingFace and upload them to their own chain infrastructure or third-party cloud environment. At this time, companies and institutions must make efforts to verify the source of the data set used in the initial learning of the existing LLM. Learning data of unknown origin may contain personal information that is illegal or has been disclosed regardless of the will of the information subject, so special caution is required19). Therefore, the source and history of the learning data It is desirable to first utilize a model that can be verified, and if there are limitations in verifying the source, it is recommended to reduce residual risks by applying various technical and managerial safety measures in the subsequent stages. In addition, check what safety measures are built into the existing LLM itself through model cards, technical documents, licensing policies, etc. It is recommended to apply additional complementary measures through red team testing20), etc. In addition, if the developer of an existing LLM clinic announces a risk discovered after model deployment, it is important to quickly reflect this to supplement the risk management system and secure the safety and reliability of the system by periodically applying the latest version and patch of the model."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\n19) At least 1,000 images of child sexual exploitation were discovered in the LAION image dataset, which is frequently used in AI learning (December 2023). 20) Refer to “AI Privacy Governance System” in Chapter 3, Section 5 of this guide.\nReference Example of role division related to ready-made LLM ※ Refer to page 50 of 「AI Privacy Risk Management Model」(December 2024)‣ (Model developer) If risks that were not recognized during the learning stage are recognized after the model is released, the risk is notified through the model distribution platform (hugging face, etc.), and the model is updated within a technically and economically reasonable period of time to take complementary measures such as redistribution and deactivation of the previous model - License terms and conditions specifying usage methods and conditions that take privacy into account Establishment and distribution‣ (Model users) Efforts are made to utilize universal models with guaranteed safety, such as reviewing risk reduction measures applied by model developers through model cards, etc.\n- 22 -\nCompanies and institutions that self-develop their own LLM system are fully responsible for all processes in the AI life cycle. This method requires identifying personal information risk factors at all stages, from pre-training based on large-scale learning data, fine tuning, distribution and operation, and follow-up management, and taking measures to reduce them. Reference Guide available for reference when self-developing LLM ▪ Processing of disclosed personal information Guide (‘24."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\n19) At least 1,000 images of child sexual exploitation were discovered in the LAION image dataset, which is frequently used in AI learning (December 2023). 20) Refer to “AI Privacy Governance System” in Chapter 3, Section 5 of this guide.\nReference Example of role division related to ready-made LLM ※ Refer to page 50 of 「AI Privacy Risk Management Model」(December 2024)‣ (Model developer) If risks that were not recognized during the learning stage are recognized after the model is released, the risk is notified through the model distribution platform (hugging face, etc.), and the model is updated within a technically and economically reasonable period of time to take complementary measures such as redistribution and deactivation of the previous model - License terms and conditions specifying usage methods and conditions that take privacy into account Establishment and distribution‣ (Model users) Efforts are made to utilize universal models with guaranteed safety, such as reviewing risk reduction measures applied by model developers through model cards, etc.\n- 22 -\nCompanies and institutions that self-develop their own LLM system are fully responsible for all processes in the AI life cycle. This method requires identifying personal information risk factors at all stages, from pre-training based on large-scale learning data, fine tuning, distribution and operation, and follow-up management, and taking measures to reduce them. Reference Guide available for reference when self-developing LLM ▪ Processing of disclosed personal information Guide (‘24."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\n23) An AI system that analyzes problems on its own to achieve its goals, autonomously decides on necessary tasks, and can perform them directly. 24) Windland, V. et al. ‘What is LLM orchestration’ (2024) https://www.ibm.com/think/topics/llm-orchestration25) Fine-tuning and alignment are techniques for improving the performance of generative AI, but fine-tuning focuses on training the model to suit a specific purpose or domain, and alignment is a technique for refining the output behavior so that the model meets human values and social standards. Fine-tuning is used to align the model. It can precede or run in parallel26) See GDS, ｢AI Playbook for the UK Government｣, “Principle 3: You know how to use AI securely” and ”Principle 5: You understand how to manage the AI life cycle”\nReference International discussion on memorization and storage of personal information in LLM ▪ (German Hamburg Personal Information Supervisory Authority (HmbBfDI) Opinion (July 2024)) Because personal information is not stored in LLM, simply storing LLM does not constitute personal information processing under GDPR."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\n23) An AI system that analyzes problems on its own to achieve its goals, autonomously decides on necessary tasks, and can perform them directly. 24) Windland, V. et al. ‘What is LLM orchestration’ (2024) https://www.ibm.com/think/topics/llm-orchestration25) Fine-tuning and alignment are techniques for improving the performance of generative AI, but fine-tuning focuses on training the model to suit a specific purpose or domain, and alignment is a technique for refining the output behavior so that the model meets human values and social standards. Fine-tuning is used to align the model. It can precede or run in parallel26) See GDS, ｢AI Playbook for the UK Government｣, “Principle 3: You know how to use AI securely” and ”Principle 5: You understand how to manage the AI life cycle”\nReference International discussion on memorization and storage of personal information in LLM ▪ (German Hamburg Personal Information Supervisory Authority (HmbBfDI) Opinion (July 2024)) Because personal information is not stored in LLM, simply storing LLM does not constitute personal information processing under GDPR."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\nHowever, if an AI system consisting of an LLM, etc. processes personal information through queries or output, the processing must comply with the requirements of the GDPR ▪ (European Data Protection Board (EDPB) Opinion 28/2024 (December 2024)) Whether an AI model that has learned personal information is anonymous requires a case-by-case judgment, and there is a possibility of obtaining personal information extracted directly or indirectly by all reasonably possible means. Opinion that it is difficult to say that anonymity exists unless it is at a negligible level ※ Therefore, when developing AI using open source models, etc., unless the user applies separate anonymization, an evaluation is required to check the legitimacy of the model to comply with GDPR obligations, and all actors included in the AI ecosystem are responsible. ▪ (US California Personal Information Protection Act (CCPA) Amendment (AB-1008) (effective January 1, 2025)) LLM is required to conduct an evaluation to check the legitimacy of the model to comply with GDPR obligations. If the model itself can be considered personal information if it is created or the results of its creation can lead to personal identification, it is stipulated that\n- 24 -\n❏What should be considered from a privacy perspective?1."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\nHowever, if an AI system consisting of an LLM, etc. processes personal information through queries or output, the processing must comply with the requirements of the GDPR ▪ (European Data Protection Board (EDPB) Opinion 28/2024 (December 2024)) Whether an AI model that has learned personal information is anonymous requires a case-by-case judgment, and there is a possibility of obtaining personal information extracted directly or indirectly by all reasonably possible means. Opinion that it is difficult to say that anonymity exists unless it is at a negligible level ※ Therefore, when developing AI using open source models, etc., unless the user applies separate anonymization, an evaluation is required to check the legitimacy of the model to comply with GDPR obligations, and all actors included in the AI ecosystem are responsible. ▪ (US California Personal Information Protection Act (CCPA) Amendment (AB-1008) (effective January 1, 2025)) LLM is required to conduct an evaluation to check the legitimacy of the model to comply with GDPR obligations. If the model itself can be considered personal information if it is created or the results of its creation can lead to personal identification, it is stipulated that\n- 24 -\n❏What should be considered from a privacy perspective?1."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\n7.), “Ⅲ. Standards for safety assurance”29) CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart): A testing technique used to determine whether a client accessing a website is a human or a computer bot. There are various methods, such as entering displayed text or numbers or selecting an image that matches the description.30) CNIL, “La base légale de l’intérêt légitime: See “Fiche focus sur les mesures à prendre en cas de collecte des données par moissonnage (web scraping)”, ('25. 6.), “Les mesures obligatoires” and “Respecter les attentes raisonnables”."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\n7.), “Ⅲ. Standards for safety assurance”29) CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart): A testing technique used to determine whether a client accessing a website is a human or a computer bot. There are various methods, such as entering displayed text or numbers or selecting an image that matches the description.30) CNIL, “La base légale de l’intérêt légitime: See “Fiche focus sur les mesures à prendre en cas de collecte des données par moissonnage (web scraping)”, ('25. 6.), “Les mesures obligatoires” and “Respecter les attentes raisonnables”."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\nIn particular, the CNIL holds the position that it is difficult to recognize the reasonable foreseeability of the information subject if a company or institution learns content that clearly expresses refusal to data scraping through robots.txt or CAPTCHA, which may affect the judgment of ‘benefit’, which is one of the requirements for recognition of ‘legitimate interest’. 31) In the CNIL document above, “Comment les éditeurs de site peuvent-ils protéger leur contenu du moissonnage?” Note\n▪ (Research trends) Research showing that LLM can function as a high-performance lossless compressor that compresses and restores data, including personal information, without loss ※ Delétang, Grégoire, et al. Language Modeling Is Compression. International Conference on Learning Representations (ICLR), 2024. arXiv:2309.10668."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\nIn particular, the CNIL holds the position that it is difficult to recognize the reasonable foreseeability of the information subject if a company or institution learns content that clearly expresses refusal to data scraping through robots.txt or CAPTCHA, which may affect the judgment of ‘benefit’, which is one of the requirements for recognition of ‘legitimate interest’. 31) In the CNIL document above, “Comment les éditeurs de site peuvent-ils protéger leur contenu du moissonnage?” Note\n▪ (Research trends) Research showing that LLM can function as a high-performance lossless compressor that compresses and restores data, including personal information, without loss ※ Delétang, Grégoire, et al. Language Modeling Is Compression. International Conference on Learning Representations (ICLR), 2024. arXiv:2309.10668."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\nhttps://arxiv.org/abs/2309.10668\n- 25 -\nPre-processing of learning data is the starting point for preventing risks such as disclosure and exposure of personal information. To this end, considering the intended use and performance of generative AI, if pseudonymization and anonymization of learning data are sufficient to achieve the purpose, it is recommended to pseudonymize and anonymize learning data immediately after collection. Reference Unstructured data pseudonymization technology and examples ※ ｢Pseudonymous Information Processing Guidelines｣ (February 2024)  Personal information detection and masking through rules, regular expressions, etc. may have limitations in terms of accuracy, and to complement this, personal information of undefined patterns can be detected and masked through the LLM model.  Various types of AI-based personal information detection techniques exist depending on the learning method (HMM, MEM, CRFs, etc.)\nAI-based text information pseudonymization Example of video information pseudonymization (applying image filtering technology) In particular, the risk of personal identification is high and if abused for crimes, it can cause great harm to the public, so resident registration numbers, other unique identification information, account numbers, credit card numbers, etc., which are specially protected by the Personal Information Protection Act, must be deleted or pseudonymized or anonymized before AI learning."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\nhttps://arxiv.org/abs/2309.10668\n- 25 -\nPre-processing of learning data is the starting point for preventing risks such as disclosure and exposure of personal information. To this end, considering the intended use and performance of generative AI, if pseudonymization and anonymization of learning data are sufficient to achieve the purpose, it is recommended to pseudonymize and anonymize learning data immediately after collection. Reference Unstructured data pseudonymization technology and examples ※ ｢Pseudonymous Information Processing Guidelines｣ (February 2024)  Personal information detection and masking through rules, regular expressions, etc. may have limitations in terms of accuracy, and to complement this, personal information of undefined patterns can be detected and masked through the LLM model.  Various types of AI-based personal information detection techniques exist depending on the learning method (HMM, MEM, CRFs, etc.)\nAI-based text information pseudonymization Example of video information pseudonymization (applying image filtering technology) In particular, the risk of personal identification is high and if abused for crimes, it can cause great harm to the public, so resident registration numbers, other unique identification information, account numbers, credit card numbers, etc., which are specially protected by the Personal Information Protection Act, must be deleted or pseudonymized or anonymized before AI learning."}
{"meta": {"section_type": "guideline", "article_ref": "제34조", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical] [Article: Article 34]\n\nResult of preliminary inspection of major AI services for reference (March 2024) ▪ Recommended to remove information items (unique identification information such as resident registration number, account information, credit card information, mobile phone number, etc.) that have a high risk of identifying Korean information subjects in the pre-training stage and are highly likely to cause secondary damage when leaked or exposed ▪ Personal information exposure pages (URLs) of Korean information subjects detected by the Personal Information Commissioner-Korea Internet & Security Agency (KISA) are provided to AI services Provided to provider (based on application)\nArticle 34-2 of the Act (Delete/Blocking of Exposed Personal Information) ① Personal information processors must ensure that personal information, such as unique identification information, account information, and credit card information, is not exposed to the public through information and communications networks. ② If there is a request from the Protection Commission or a specialized agency designated by Presidential Decree regarding personal information exposed to the public, the personal information processor shall take necessary measures, such as deleting or blocking the relevant information."}
{"meta": {"section_type": "guideline", "article_ref": "제34조", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit] [Article: Article 34]\n\nResult of preliminary inspection of major AI services for reference (March 2024) ▪ Recommended to remove information items (unique identification information such as resident registration number, account information, credit card information, mobile phone number, etc.) that have a high risk of identifying Korean information subjects in the pre-training stage and are highly likely to cause secondary damage when leaked or exposed ▪ Personal information exposure pages (URLs) of Korean information subjects detected by the Personal Information Commissioner-Korea Internet & Security Agency (KISA) are provided to AI services Provided to provider (based on application)\nArticle 34-2 of the Act (Delete/Blocking of Exposed Personal Information) ① Personal information processors must ensure that personal information, such as unique identification information, account information, and credit card information, is not exposed to the public through information and communications networks. ② If there is a request from the Protection Commission or a specialized agency designated by Presidential Decree regarding personal information exposed to the public, the personal information processor shall take necessary measures, such as deleting or blocking the relevant information."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\n- 26 -\nIn addition, appropriately generated synthetic data32) is attracting attention as a practical alternative that can maintain model performance without the legal restrictions required for personal information. When generating synthetic data, it is important to find a balance by maintaining the structural information of real data as much as possible to ensure usability while ensuring that individuals included in the original data are not identified. Reference Synthetic data Considerations when creating and utilizing ※｢Synthetic Data Creation and Utilization Guide｣(December 2024)▪ (Safety standard setting) Considering the trade-off between the usefulness and safety of synthetic data, set safety standards first according to the purpose of use, etc.▪ (Original data preprocessing) Based on analysis of the original data, it is recommended to delete areas unnecessary for synthesis and perform data purification.▪ (Safety verification) In preparation for situations where the generated synthetic data contains personally identifiable information. Conduct quantitative and qualitative safety verification ▪ (Safe management) Prepare and implement a management plan in preparation for the possibility of residual risks such as re-identification when synthetic data is disclosed to the public\n(Example 1: When focusing on usability) Utilization in a safe internal closed environment, Utilization for token-level AI learning of LLM in an environment with low risk of infringement (Example 2: When focusing on safety) Utilization during consignment development of IT systems, external disclosure of synthetic data\nLastly, as part of privacy-enhancing technology (PET), various learning techniques that preserve privacy, such as differential privacy and federated learning, are being researched and developed, and are being directly applied to the development of generative AI to increase the safety of AI. Example PET technology (federated learning) application case ▪ Performing federated learning for safe use of highly sensitive medical data Medical data held by each hospital is It is safely stored in a cloud environment within the hospital, and after model learning is performed locally at each hospital, only the results of the learned model are transmitted and utilized to the central server. In the medical field where securing data precision and accuracy is important, applying federated learning techniques may be useful ※ However, it is advisable to evaluate the risk of leakage and exposure for federated learned LLMs and review the need for combining additional PET techniques in addition to federated learning.\n32) Simulated or artificial data created by learning the format, structure, and statistical distribution characteristics and patterns of the original data for a specific purpose.\n- 27 -\n2."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\n- 26 -\nIn addition, appropriately generated synthetic data32) is attracting attention as a practical alternative that can maintain model performance without the legal restrictions required for personal information. When generating synthetic data, it is important to find a balance by maintaining the structural information of real data as much as possible to ensure usability while ensuring that individuals included in the original data are not identified. Reference Synthetic data Considerations when creating and utilizing ※｢Synthetic Data Creation and Utilization Guide｣(December 2024)▪ (Safety standard setting) Considering the trade-off between the usefulness and safety of synthetic data, set safety standards first according to the purpose of use, etc.▪ (Original data preprocessing) Based on analysis of the original data, it is recommended to delete areas unnecessary for synthesis and perform data purification.▪ (Safety verification) In preparation for situations where the generated synthetic data contains personally identifiable information. Conduct quantitative and qualitative safety verification ▪ (Safe management) Prepare and implement a management plan in preparation for the possibility of residual risks such as re-identification when synthetic data is disclosed to the public\n(Example 1: When focusing on usability) Utilization in a safe internal closed environment, Utilization for token-level AI learning of LLM in an environment with low risk of infringement (Example 2: When focusing on safety) Utilization during consignment development of IT systems, external disclosure of synthetic data\nLastly, as part of privacy-enhancing technology (PET), various learning techniques that preserve privacy, such as differential privacy and federated learning, are being researched and developed, and are being directly applied to the development of generative AI to increase the safety of AI. Example PET technology (federated learning) application case ▪ Performing federated learning for safe use of highly sensitive medical data Medical data held by each hospital is It is safely stored in a cloud environment within the hospital, and after model learning is performed locally at each hospital, only the results of the learned model are transmitted and utilized to the central server. In the medical field where securing data precision and accuracy is important, applying federated learning techniques may be useful ※ However, it is advisable to evaluate the risk of leakage and exposure for federated learned LLMs and review the need for combining additional PET techniques in addition to federated learning.\n32) Simulated or artificial data created by learning the format, structure, and statistical distribution characteristics and patterns of the original data for a specific purpose.\n- 27 -\n2."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\nmodel To compensate for privacy risks that could not be eliminated proactively at the data level and to ensure that the AI model generates safe and desirable answers, it is recommended to apply additional safety measures using techniques such as fine-tuning and alignment. In this regard, recent empirical studies have shown that various factors such as fine-tuning method, range, and location can significantly increase the risk of memorization of the model, showing the importance of safety measures in the fine-tuning stage33). The fine-tuning technique is SFT (Super). VisedFine-tuning), RLHF (Reinforcement Learning from Human Feedback), DPO (DirectPreferenceOptimization), etc. have been researched and applied, and recently Advanced learning methods such as GRPO (GroupRelativePolicyOptimization) are attracting attention. They can be used in a way that goes beyond simple response accuracy and can simultaneously improve the transparency and safety of the model judgment process."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\nmodel To compensate for privacy risks that could not be eliminated proactively at the data level and to ensure that the AI model generates safe and desirable answers, it is recommended to apply additional safety measures using techniques such as fine-tuning and alignment. In this regard, recent empirical studies have shown that various factors such as fine-tuning method, range, and location can significantly increase the risk of memorization of the model, showing the importance of safety measures in the fine-tuning stage33). The fine-tuning technique is SFT (Super). VisedFine-tuning), RLHF (Reinforcement Learning from Human Feedback), DPO (DirectPreferenceOptimization), etc. have been researched and applied, and recently Advanced learning methods such as GRPO (GroupRelativePolicyOptimization) are attracting attention. They can be used in a way that goes beyond simple response accuracy and can simultaneously improve the transparency and safety of the model judgment process."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\n33) Excerpt from the research report by Kim Byeong-pil (2025), “Privacy risk diagnosis and authentication method for general-purpose AI models,” citing the research results of Mireshghallah, Fatemehsadat, et al (2022)\nExamples of fine-tuning techniques to strengthen reference safety ※ 「Public Personal Information Processing Guide」(July 2024)「AI Privacy Risk Management Model」(December 2024) ▪ SFT (Supervised Fine-Tuning) - A process of fine-tuning generative AI based on unsupervised learning through supervised learning, additionally learning pre-purified or labeled data to generate desirable answers ※ (Example) Answering a prompt asking about personal privacy Learn answers for rejected content ▪ RLHF (Reinforcement Learning with Human Feedback) - Reward Model Creation: A person (labeler) assigns a score or rank to the output generated by an AI model, and trains a reward model based on this ※ (Example) In response to a prompt asking about an individual's privacy, a reward of (-1) is provided for an answer that includes privacy, and a reward of (+1) is provided for an answer that avoids - Policy optimization (Policy) Optimization): This is the step of optimizing the policy of the AI model using a reward model. Fine-tuning is mainly done using PPO (Proximal Policy Optimization), a policy gradient reinforcement learning algorithm.\n- 28 -\nResponsiveness to adversarial attacks that extract personal information contained in learning data from generative AI models is also an important issue that must be addressed at the model level. Adversarial attacks targeting AI models are already an existing threat, and recently, In an experiment targeting a material-specialized LLM, serious vulnerabilities in terms of privacy were confirmed, such as the probability of accessing sensitive information by bypassing the model security device reaching 80%, and the probability of original information being exposed through model responses exceeding 20%34). Reference generation Case of privacy attack against AI model35)\nImage extracted from the Stable Diffusion model36)  A case where an image was regenerated just by entering the name (Ann Graham Lotz) into an AI model that learned a dataset containing photos and captions. Personal information extracted from LLM37)  A case where GPT2 output personal information such as address and email included in the learning data when a specific sentence (“East Stroudsburg Stroudsburg...”) was entered. 34) Kim, Minsu, et al."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\n33) Excerpt from the research report by Kim Byeong-pil (2025), “Privacy risk diagnosis and authentication method for general-purpose AI models,” citing the research results of Mireshghallah, Fatemehsadat, et al (2022)\nExamples of fine-tuning techniques to strengthen reference safety ※ 「Public Personal Information Processing Guide」(July 2024)「AI Privacy Risk Management Model」(December 2024) ▪ SFT (Supervised Fine-Tuning) - A process of fine-tuning generative AI based on unsupervised learning through supervised learning, additionally learning pre-purified or labeled data to generate desirable answers ※ (Example) Answering a prompt asking about personal privacy Learn answers for rejected content ▪ RLHF (Reinforcement Learning with Human Feedback) - Reward Model Creation: A person (labeler) assigns a score or rank to the output generated by an AI model, and trains a reward model based on this ※ (Example) In response to a prompt asking about an individual's privacy, a reward of (-1) is provided for an answer that includes privacy, and a reward of (+1) is provided for an answer that avoids - Policy optimization (Policy) Optimization): This is the step of optimizing the policy of the AI model using a reward model. Fine-tuning is mainly done using PPO (Proximal Policy Optimization), a policy gradient reinforcement learning algorithm.\n- 28 -\nResponsiveness to adversarial attacks that extract personal information contained in learning data from generative AI models is also an important issue that must be addressed at the model level. Adversarial attacks targeting AI models are already an existing threat, and recently, In an experiment targeting a material-specialized LLM, serious vulnerabilities in terms of privacy were confirmed, such as the probability of accessing sensitive information by bypassing the model security device reaching 80%, and the probability of original information being exposed through model responses exceeding 20%34). Reference generation Case of privacy attack against AI model35)\nImage extracted from the Stable Diffusion model36)  A case where an image was regenerated just by entering the name (Ann Graham Lotz) into an AI model that learned a dataset containing photos and captions. Personal information extracted from LLM37)  A case where GPT2 output personal information such as address and email included in the learning data when a specific sentence (“East Stroudsburg Stroudsburg...”) was entered. 34) Kim, Minsu, et al."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\nVol.1)36) Nicholas Carlini et al., Extracting Training Data from Diffusion Models, USENIX Security, 202337) Nicholas Carlini et al., Extracting Training Data from Large Language Models, USENIX Security, 2021\n▪ DPO (Direct Preference Optimization) - One of the preference-based fine-tuning techniques proposed to overcome the shortcomings of traditional RLHF, which directly reflects the relative preference for user response pairs in model parameters ※ (Example) When there are response pairs A and B, if the person preferred A more, update the model parameters so that the probability of generating response A is higher than the probability of generating response B ▪ GRPO (Group Relative Policy) Optimization) - This is a technique to improve the limitations of DPO. Instead of comparing individual response pairs, multiple actions are grouped and relative performance is evaluated to update the policy. ※ Noise that may occur when comparing individual response pairs is alleviated through group-based learning.\n- 29 -\nThese results suggest that reinforcing safeguards at the model level is necessary to ensure the safety of generative AI. The most widely reviewed method is Differentially Private Stochastic Gradient Descent (DP-SGD), which enhances safety by adjusting the learning process and weights of the AI model. Reference Differential Privacy (DP)-based Gradient descent method38) ▪ Clipping the gradient that occurs during the data learning process to a certain range and adding random noise to it, which reduces the risk of memorizing the original data and is resistant to external model attacks ▪ As the model becomes larger (e.g. high-performance image classification model), there is a decrease in accuracy, which limits the application of the technique. However, recent research is underway to show that applying DP-SGD can be effective if the gradient is well normalized.\nIn addition, various studies are being conducted, such as ways to apply knowledge distillation technology, which is mainly used for the purpose of lightening models and enhancing performance, to the privacy field and use it as a type of PET technology. Reference Research trends on knowledge distillation technology as a PET technology39) ▪ Knowledge distillation is a technique mainly used for the purpose of lightweighting models while maintaining the performance of pre-learning models ▪ Recently A method of limiting the learning or output of personal information by utilizing the technical characteristics of knowledge distillation (learning and inference methods can be precisely adjusted) is being studied as a type of PET technique - In particular, research combining differential privacy and synthetic data with knowledge distillation is active ※ Flemings, James, et al."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\nVol.1)36) Nicholas Carlini et al., Extracting Training Data from Diffusion Models, USENIX Security, 202337) Nicholas Carlini et al., Extracting Training Data from Large Language Models, USENIX Security, 2021\n▪ DPO (Direct Preference Optimization) - One of the preference-based fine-tuning techniques proposed to overcome the shortcomings of traditional RLHF, which directly reflects the relative preference for user response pairs in model parameters ※ (Example) When there are response pairs A and B, if the person preferred A more, update the model parameters so that the probability of generating response A is higher than the probability of generating response B ▪ GRPO (Group Relative Policy) Optimization) - This is a technique to improve the limitations of DPO. Instead of comparing individual response pairs, multiple actions are grouped and relative performance is evaluated to update the policy. ※ Noise that may occur when comparing individual response pairs is alleviated through group-based learning.\n- 29 -\nThese results suggest that reinforcing safeguards at the model level is necessary to ensure the safety of generative AI. The most widely reviewed method is Differentially Private Stochastic Gradient Descent (DP-SGD), which enhances safety by adjusting the learning process and weights of the AI model. Reference Differential Privacy (DP)-based Gradient descent method38) ▪ Clipping the gradient that occurs during the data learning process to a certain range and adding random noise to it, which reduces the risk of memorizing the original data and is resistant to external model attacks ▪ As the model becomes larger (e.g. high-performance image classification model), there is a decrease in accuracy, which limits the application of the technique. However, recent research is underway to show that applying DP-SGD can be effective if the gradient is well normalized.\nIn addition, various studies are being conducted, such as ways to apply knowledge distillation technology, which is mainly used for the purpose of lightening models and enhancing performance, to the privacy field and use it as a type of PET technology. Reference Research trends on knowledge distillation technology as a PET technology39) ▪ Knowledge distillation is a technique mainly used for the purpose of lightweighting models while maintaining the performance of pre-learning models ▪ Recently A method of limiting the learning or output of personal information by utilizing the technical characteristics of knowledge distillation (learning and inference methods can be precisely adjusted) is being studied as a type of PET technique - In particular, research combining differential privacy and synthetic data with knowledge distillation is active ※ Flemings, James, et al."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\nDifferentially Private Knowledge Distillation via Synthetic Text Generation (2024) - A technique to unlearn only personal information from an AI model using knowledge distillation has also emerged ※ Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models (2024)\n38) Refer to ‘Generative AI-related privacy risk reduction technology evaluation study’ (’24.5.~’10., JC Radar and Kyungpook National University) included as an appendix in ｢AI Privacy Risk Management Model｣ (’24.12.), Google DeepMind Study (Unlocking High-Accuracy Differentially Private Image Classification through Scale, ’22.6.) 39) “Overseas trends in resolving personal information protection issues using knowledge distillation - 3rd Personal Information Technology Forum Workshop” (May 2025, Law Firm DLG Hwang Gyu-ho)\n- 30 -\n3."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\nDifferentially Private Knowledge Distillation via Synthetic Text Generation (2024) - A technique to unlearn only personal information from an AI model using knowledge distillation has also emerged ※ Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models (2024)\n38) Refer to ‘Generative AI-related privacy risk reduction technology evaluation study’ (’24.5.~’10., JC Radar and Kyungpook National University) included as an appendix in ｢AI Privacy Risk Management Model｣ (’24.12.), Google DeepMind Study (Unlocking High-Accuracy Differentially Private Image Classification through Scale, ’22.6.) 39) “Overseas trends in resolving personal information protection issues using knowledge distillation - 3rd Personal Information Technology Forum Workshop” (May 2025, Law Firm DLG Hwang Gyu-ho)\n- 30 -\n3."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\nsystem Even if privacy protection measures are implemented at the data and model level, vulnerabilities in the system operating environment where generative AI is deployed and operated may still become a path for personal information leakage or exposure. Therefore, in order to ensure the safety of the learning and inference results of the AI system, it is necessary to establish a protection system at the system level. When an AI system is linked to the outside world through an API, access control can be a key defense for privacy protection. For example, API call permissions can be granted only to pre-authenticated subjects, and granular access control can be applied to effectively block malicious requests and unauthorized access. Additionally, , Safety devices can be strengthened by applying filters to the input and output stages of a generative AI system. The input filter plays a role in analyzing and blocking whether the prompt provided by the user is malicious or leads to the creation of sensitive information. For example, text in the form of unique identification information of a specific person. If an input requesting an input is detected, the AI system can be designed to remove the corresponding input value. Additionally, if personal information included in the AI output value is detected through an output filter, it can be controlled by removing or replacing it or inserting a warning message40).Case Filter example for inappropriate user prompts41)\n40) ｢Llama Developer Use Guide: AI Protections｣ (Meta, '25."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\nsystem Even if privacy protection measures are implemented at the data and model level, vulnerabilities in the system operating environment where generative AI is deployed and operated may still become a path for personal information leakage or exposure. Therefore, in order to ensure the safety of the learning and inference results of the AI system, it is necessary to establish a protection system at the system level. When an AI system is linked to the outside world through an API, access control can be a key defense for privacy protection. For example, API call permissions can be granted only to pre-authenticated subjects, and granular access control can be applied to effectively block malicious requests and unauthorized access. Additionally, , Safety devices can be strengthened by applying filters to the input and output stages of a generative AI system. The input filter plays a role in analyzing and blocking whether the prompt provided by the user is malicious or leads to the creation of sensitive information. For example, text in the form of unique identification information of a specific person. If an input requesting an input is detected, the AI system can be designed to remove the corresponding input value. Additionally, if personal information included in the AI output value is detected through an output filter, it can be controlled by removing or replacing it or inserting a warning message40).Case Filter example for inappropriate user prompts41)\n40) ｢Llama Developer Use Guide: AI Protections｣ (Meta, '25."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\n4.), see “Mitigating risks at the input level” and “Develop potential mitigation methods at output”\n- 31 -\nRecently, the Retrieval-Augmented Generation (RAG) technique, which searches an external knowledge base and combines it with input values, has been widely used to improve the performance of generative AI. Along the same line, agents are also developing in the direction of continuously searching, retrieving, and creating knowledge bases in various environments through autonomous and goal-oriented exploration. Through this, the AI model is able to provide responses based on past knowledge. The accuracy, freshness, and reliability of output results can be improved while compensating for generation limitations and reducing hallucinations. However, RAG or AI agents search for sentences in an external knowledge base and provide prompts for them. In the process of combining and creating, the risk of retaining and exposing personal information contained in the sentence may increase relatively. Therefore, it is recommended to consider the application of safety measures such as data preprocessing and filtering discussed earlier during their implementation process."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\n4.), see “Mitigating risks at the input level” and “Develop potential mitigation methods at output”\n- 31 -\nRecently, the Retrieval-Augmented Generation (RAG) technique, which searches an external knowledge base and combines it with input values, has been widely used to improve the performance of generative AI. Along the same line, agents are also developing in the direction of continuously searching, retrieving, and creating knowledge bases in various environments through autonomous and goal-oriented exploration. Through this, the AI model is able to provide responses based on past knowledge. The accuracy, freshness, and reliability of output results can be improved while compensating for generation limitations and reducing hallucinations. However, RAG or AI agents search for sentences in an external knowledge base and provide prompts for them. In the process of combining and creating, the risk of retaining and exposing personal information contained in the sentence may increase relatively. Therefore, it is recommended to consider the application of safety measures such as data preprocessing and filtering discussed earlier during their implementation process."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\n41) This case shows the result of the filter operating successfully, but additional safety measures may be necessary if a prompt containing a new type of jailbreak attack that bypasses safety measures is entered.\nReferenceLLM-based agent services and privacy infringement risks42) ▪ As LLM-based agents are becoming more sophisticated with a high level of autonomy and adaptability, personal information processing methods are becoming complex and privacy risks are also diversifying. ▪ LLM-based agent types are divided into 3 types according to the flow of personal information processing ➊ Search-type agent · In the process of retrieving information from an external DB, risk of personal information infringement is raised through searching and combining personal information disclosed · When using DB using the RAG method, etc., unidentified documents or sensitive information Since it can be output in response to user inquiries, appropriate safety measures such as external data access control, user authentication, response filtering, and record management are required ➋ Memory-type agent · Provides continuous learning and personalized services based on short-term and long-term memory, but there is a risk of long-term tracking and profiling of user behavior, preferences, psychological state, etc. · Asymmetric information structure and technological complexity make it difficult to exercise control rights such as consent of the information subject → Requires structural risk assessment and proactive control devices\n- 32 -\ncontinuous evaluation It is difficult to ensure safety and reliability of system generative AI through one-time learning and distribution. Therefore, in the development process of generative AI, a feedback loop that continuously checks and supplements safety by designing learning and evaluation in parallel must be internalized. Factors such as privacy protection, bias, and output reliability are subject to changes in learning data or model structure. Since they are severely affected, it is recommended to periodically perform evaluation work based on benchmark tests, etc. Recently, benchmarks and frameworks to evaluate the safety of LLMs, etc., such as the ability to block abnormal requests that bypass security measures ('jailbreak resistance'), are also being continuously researched. Companies and institutions can use these tools to establish a quantitative evaluation system. Examples AI safety-specific benchmark cases including privacy protection ▪ Future of Life Institute's AI Safe Index43)  AI safety evaluation system including risk assessment, framework for safety, strategy, governance, and accountability ▪ HarmBench open source evaluation framework44)  Evaluation framework that provides automated redteaming tools ▪ JailBreakBench open source evaluation benchmark45)  Benchmark dataset that measures resistance to jailbreak for language models such as LLM▪ MIT AI Risk Repository46)  A database that reviews and covers risks related to AI, including privacy and security risks42) “AI Privacy Risks & Mitigations - Large Language Models”, (EDPB) and Byungpil Kim (2025), excerpt from research report on “Privacy Risk Diagnosis and Certification Method for General-purpose AI Models”\n➌ Multi-agent · The main agent that receives user input forwards it to various external sub-agents, and each agent interacts with external applications or calls external services. Personal information may be shared with multiple systems. · If the actions of individual agents are not properly controlled, the possibility of information aggregation and re-identification accumulates and attribution of responsibility becomes ambiguous → Requires structural risk assessment and proactive control devices.\n- 33 -\n4 System application and Management ❏ How do I apply the system once development is completed? After the AI system development is completed, it goes through a final inspection and is distributed and applied to the usage environment. The final inspection process includes a procedure to check whether the system achieves its intended purpose in the real environment. After deployment and application, the system performance is Maintenance and management are carried out by continuously checking performance and safety. At this time, efforts must be made to ensure the rights of information subjects, such as continuously monitoring for violations of information subject rights47) and transparently informing of the personal information processing process. ❏What are the privacy factors to be considered before application and during operation?"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\n41) This case shows the result of the filter operating successfully, but additional safety measures may be necessary if a prompt containing a new type of jailbreak attack that bypasses safety measures is entered.\nReferenceLLM-based agent services and privacy infringement risks42) ▪ As LLM-based agents are becoming more sophisticated with a high level of autonomy and adaptability, personal information processing methods are becoming complex and privacy risks are also diversifying. ▪ LLM-based agent types are divided into 3 types according to the flow of personal information processing ➊ Search-type agent · In the process of retrieving information from an external DB, risk of personal information infringement is raised through searching and combining personal information disclosed · When using DB using the RAG method, etc., unidentified documents or sensitive information Since it can be output in response to user inquiries, appropriate safety measures such as external data access control, user authentication, response filtering, and record management are required ➋ Memory-type agent · Provides continuous learning and personalized services based on short-term and long-term memory, but there is a risk of long-term tracking and profiling of user behavior, preferences, psychological state, etc. · Asymmetric information structure and technological complexity make it difficult to exercise control rights such as consent of the information subject → Requires structural risk assessment and proactive control devices\n- 32 -\ncontinuous evaluation It is difficult to ensure safety and reliability of system generative AI through one-time learning and distribution. Therefore, in the development process of generative AI, a feedback loop that continuously checks and supplements safety by designing learning and evaluation in parallel must be internalized. Factors such as privacy protection, bias, and output reliability are subject to changes in learning data or model structure. Since they are severely affected, it is recommended to periodically perform evaluation work based on benchmark tests, etc. Recently, benchmarks and frameworks to evaluate the safety of LLMs, etc., such as the ability to block abnormal requests that bypass security measures ('jailbreak resistance'), are also being continuously researched. Companies and institutions can use these tools to establish a quantitative evaluation system. Examples AI safety-specific benchmark cases including privacy protection ▪ Future of Life Institute's AI Safe Index43)  AI safety evaluation system including risk assessment, framework for safety, strategy, governance, and accountability ▪ HarmBench open source evaluation framework44)  Evaluation framework that provides automated redteaming tools ▪ JailBreakBench open source evaluation benchmark45)  Benchmark dataset that measures resistance to jailbreak for language models such as LLM▪ MIT AI Risk Repository46)  A database that reviews and covers risks related to AI, including privacy and security risks42) “AI Privacy Risks & Mitigations - Large Language Models”, (EDPB) and Byungpil Kim (2025), excerpt from research report on “Privacy Risk Diagnosis and Certification Method for General-purpose AI Models”\n➌ Multi-agent · The main agent that receives user input forwards it to various external sub-agents, and each agent interacts with external applications or calls external services. Personal information may be shared with multiple systems. · If the actions of individual agents are not properly controlled, the possibility of information aggregation and re-identification accumulates and attribution of responsibility becomes ambiguous → Requires structural risk assessment and proactive control devices.\n- 33 -\n4 System application and Management ❏ How do I apply the system once development is completed? After the AI system development is completed, it goes through a final inspection and is distributed and applied to the usage environment. The final inspection process includes a procedure to check whether the system achieves its intended purpose in the real environment. After deployment and application, the system performance is Maintenance and management are carried out by continuously checking performance and safety. At this time, efforts must be made to ensure the rights of information subjects, such as continuously monitoring for violations of information subject rights47) and transparently informing of the personal information processing process. ❏What are the privacy factors to be considered before application and during operation?"}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\nPre-deployment inspection Before deployment, a final check must be made to ensure that the AI operates safely as intended. AI models, source code, learning data, etc. must be comprehensively reviewed and analyzed to evaluate accuracy and stability. If companies or organizations cannot directly access the analysis target, such as when using an external model or ordering a system, input a query to the external model to evaluate the output or test the product. You can consider using alternative inspection methods, such as providing evaluation data to the system supplier48). In this way, you can check privacy risks such as accuracy of AI, resistance to attempts to bypass security measures, and possibility of learning data leakage/exposure in the actual operating environment before final distribution and manage the results as documents to increase the safety of the system before distribution.43) https://futureoflife.org/index, FLI AI Safety Index 2024 (2024. 12. 11.)44) https://github.com/centerforaisafety/HarmBench45) https://jailbreakbench.github.io46) https://airisk.mit.edu47)GDS, ｢AI Playbook for the UK Government｣, “Principle 4: You have meaningful human control at the right stages” Reference 48) OMB (Office of Management and Budget) Memorandum (M-25-21), ｢Accelerating Federal Use of AI through Innovation, Governance, and Public Trust｣, Section 4(b) i."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\nPre-deployment inspection Before deployment, a final check must be made to ensure that the AI operates safely as intended. AI models, source code, learning data, etc. must be comprehensively reviewed and analyzed to evaluate accuracy and stability. If companies or organizations cannot directly access the analysis target, such as when using an external model or ordering a system, input a query to the external model to evaluate the output or test the product. You can consider using alternative inspection methods, such as providing evaluation data to the system supplier48). In this way, you can check privacy risks such as accuracy of AI, resistance to attempts to bypass security measures, and possibility of learning data leakage/exposure in the actual operating environment before final distribution and manage the results as documents to increase the safety of the system before distribution.43) https://futureoflife.org/index, FLI AI Safety Index 2024 (2024. 12. 11.)44) https://github.com/centerforaisafety/HarmBench45) https://jailbreakbench.github.io46) https://airisk.mit.edu47)GDS, ｢AI Playbook for the UK Government｣, “Principle 4: You have meaningful human control at the right stages” Reference 48) OMB (Office of Management and Budget) Memorandum (M-25-21), ｢Accelerating Federal Use of AI through Innovation, Governance, and Public Trust｣, Section 4(b) i."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\nSee “Conduct Pre-Deployment Testing”\n- 34 -\nCompanies and institutions can write and disclose the purpose of use and prohibited activities of generative AI in an ‘acceptable use policy’ (AUP), etc., taking into account the privacy risks finally confirmed in the pre-distribution testing stage. At this time, the contents reviewed in the strategy establishment stage can be referred to. PbD principles, personal privacy An AUP can be created taking into account the privacy risks and foreseeable misuse and abuse of generative AI identified through a proactive approach such as security impact assessment. The AUP created and made public can be After generative AI is distributed, it can be used as a basis to prevent misuse and abuse, and to take measures such as service restrictions or account blocking for use that violates this. Examples Examples of Acceptable Use Policy ▪ Part of Naver CLOVA Malicious use includes the following acts and acts with similar purposes, but is not limited to the examples below. 1."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\nSee “Conduct Pre-Deployment Testing”\n- 34 -\nCompanies and institutions can write and disclose the purpose of use and prohibited activities of generative AI in an ‘acceptable use policy’ (AUP), etc., taking into account the privacy risks finally confirmed in the pre-distribution testing stage. At this time, the contents reviewed in the strategy establishment stage can be referred to. PbD principles, personal privacy An AUP can be created taking into account the privacy risks and foreseeable misuse and abuse of generative AI identified through a proactive approach such as security impact assessment. The AUP created and made public can be After generative AI is distributed, it can be used as a basis to prevent misuse and abuse, and to take measures such as service restrictions or account blocking for use that violates this. Examples Examples of Acceptable Use Policy ▪ Part of Naver CLOVA Malicious use includes the following acts and acts with similar purposes, but is not limited to the examples below. 1."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\nCreating content about illegal or criminal and harmful behavior • Content relating to child sexual abuse or exploitation • Content promoting/promoting the sale of illegal drugs (such as drugs) or goods (such as weapons) or methods for manufacturing them • … 6. Creation of malicious code, hacking, attack, service abusing code, etc. • Intrusion by accessing information processing devices, etc. without permission… 8. Entering personal information such as sensitive information or unique identification information of oneself or others, or inducing conversations or creating content that may cause infringement of personal information and privacy.\nReference Overseas AI system pre-deployment test case ▪ Joint pre-deployment test for Anthropic model by US CAISI and UK AISI (24.11.) - Joint test conducted by US AI Standards and Innovation Center (US CAISI) and UK AI Safety Institute (UK AISI) on Anthropic's ‘Sonnet 3.5 (new)’ model - Based on safety measure (safeguard) effectiveness* along with problem-solving ability in professional fields including software development field Perform tests *Apply several jailbreak attack techniques such as Criminal Activity, AgentHarm, and the publicly known HarmBench developed by UK AISI and strategically evaluate the response of the target model\n- 35 -\n▪ Part of the OpenAI Business usage policy (as of May 2025) ※ Written for users, etc. 3.3."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\nCreating content about illegal or criminal and harmful behavior • Content relating to child sexual abuse or exploitation • Content promoting/promoting the sale of illegal drugs (such as drugs) or goods (such as weapons) or methods for manufacturing them • … 6. Creation of malicious code, hacking, attack, service abusing code, etc. • Intrusion by accessing information processing devices, etc. without permission… 8. Entering personal information such as sensitive information or unique identification information of oneself or others, or inducing conversations or creating content that may cause infringement of personal information and privacy.\nReference Overseas AI system pre-deployment test case ▪ Joint pre-deployment test for Anthropic model by US CAISI and UK AISI (24.11.) - Joint test conducted by US AI Standards and Innovation Center (US CAISI) and UK AI Safety Institute (UK AISI) on Anthropic's ‘Sonnet 3.5 (new)’ model - Based on safety measure (safeguard) effectiveness* along with problem-solving ability in professional fields including software development field Perform tests *Apply several jailbreak attack techniques such as Criminal Activity, AgentHarm, and the publicly known HarmBench developed by UK AISI and strategically evaluate the response of the target model\n- 35 -\n▪ Part of the OpenAI Business usage policy (as of May 2025) ※ Written for users, etc. 3.3."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\nCustomer will not, and will not permit End Users to: (a) use the Services or Customer Content in a way that violates applicable laws or OpenAI Policies; (b) use the Services or Customer Content in a way that violates third parties’ rights; (c) allow minors to use OpenAI Services without consent from their parent or guardian; (omitted) (h) interfere with or disrupt the Services, including circumvent any rate limits or restrictions or bypass any protective measures or safety mitigations for the Services; (Omitted below) Monitoring, guarantee of data subject rights, transparency Secured companies and institutions must install reporting and opinion submission functions for information subjects in the system to respond to inappropriate results such as bias, discrimination, or rights infringement. Afterwards, the relevant window must be monitored on a regular basis and the opinions received must be actively reflected in operational policies and system improvement49).Information If there is a request for viewing, correction, deletion, etc. from the subject, it is necessary to prepare a plan to guarantee the rights of the information subject to the extent reasonably feasible in terms of time, cost, and technology. The Personal Information Protection Act stipulates that, in principle, requests for viewing, correction, deletion, and suspension of processing must be responded to within 10 days. However, if it is difficult to guarantee the exercise of traditional rights considering the size of the learning data set, composition method, system, etc., it is recommended to clearly inform the information subject of the reason and respond to the request as faithfully as possible through alternative means. For example, apply output filtering first to prevent personal information from being disclosed. You can take urgent action and consider excluding personal information from the learning dataset in the future. Meanwhile, 'machine learning' technology, which deletes specific items in the model instead of retraining the model, which is expensive, has limitations as the technology is not yet sufficiently mature, but it can be a useful means to guarantee the rights of information subjects in the future. 49) ｢Llama Developer Use Guide: AI Protections｣ (Meta, '25."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\nCustomer will not, and will not permit End Users to: (a) use the Services or Customer Content in a way that violates applicable laws or OpenAI Policies; (b) use the Services or Customer Content in a way that violates third parties’ rights; (c) allow minors to use OpenAI Services without consent from their parent or guardian; (omitted) (h) interfere with or disrupt the Services, including circumvent any rate limits or restrictions or bypass any protective measures or safety mitigations for the Services; (Omitted below) Monitoring, guarantee of data subject rights, transparency Secured companies and institutions must install reporting and opinion submission functions for information subjects in the system to respond to inappropriate results such as bias, discrimination, or rights infringement. Afterwards, the relevant window must be monitored on a regular basis and the opinions received must be actively reflected in operational policies and system improvement49).Information If there is a request for viewing, correction, deletion, etc. from the subject, it is necessary to prepare a plan to guarantee the rights of the information subject to the extent reasonably feasible in terms of time, cost, and technology. The Personal Information Protection Act stipulates that, in principle, requests for viewing, correction, deletion, and suspension of processing must be responded to within 10 days. However, if it is difficult to guarantee the exercise of traditional rights considering the size of the learning data set, composition method, system, etc., it is recommended to clearly inform the information subject of the reason and respond to the request as faithfully as possible through alternative means. For example, apply output filtering first to prevent personal information from being disclosed. You can take urgent action and consider excluding personal information from the learning dataset in the future. Meanwhile, 'machine learning' technology, which deletes specific items in the model instead of retraining the model, which is expensive, has limitations as the technology is not yet sufficiently mature, but it can be a useful means to guarantee the rights of information subjects in the future. 49) ｢Llama Developer Use Guide: AI Protections｣ (Meta, '25."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\n(omitted below)\n- 37 -\nIn order to sufficiently guarantee the rights of data subjects as discussed so far, it is necessary to clearly inform data subjects of whether their personal information is included in AI learning data and how personal information is processed by AI. Companies and institutions must provide AI systems with the facts of data set collection, main sources, processing purposes, etc. It is recommended to support information subjects in exercising their rights by transparently disclosing the personal information processing process in personal information processing policies, technical documents, FAQs, etc. In addition, if there are technical limitations in responding to requests for viewing, correction, or deletion, informing the information subject of such matters in advance is also a way to protect the information subject's rights. It can be helpful. AI agents, which are emerging recently, are AI systems that possess memory capabilities and perform tasks autonomously. There is increasing concern about excessive user information collection and profiling during the operation process. Therefore, companies and institutions that provide AI agent services must enter the prompt. It is recommended to clearly notify the user's conversation history, ▲ whether learning data is used, ▲ whether to provide it to a third party, ▲ storage and destruction policy, ▲ filtering standards, etc. In this case, it is desirable to provide information for a sufficient period of time before and after learning to ensure the information subject's practical right to choose (opt-out, etc.). Reference generation type Things to keep in mind when establishing an AI-related personal information processing policy※ 「Guidelines for preparing personal information processing policy」(April 2025)‣ For personal information processors who develop AI and provide services, it is recommended to set and write down the standards for collection and use of learning data in advance, taking into account minimum collection and clarification of purpose. Included‣ When changing the personal information processing policy, continuously disclose the change and implementation period and changed contents, and if there is a previous personal information processing policy, record the history of changes * If major changes are notified separately, it is recommended to notify the information subject in a way that can be easily confirmed through a web page pop-up window, notice, etc.‣ In the case of an automated decision, the standards, procedures, and the way personal information is processed are recorded so that the information subject can easily confirm\n- 38 -\nExamples of measures to strengthen the protection of rights of information subjects※ Decision of the Personal Information Protection Commission No. 2025-011-031 (private)※ Decision of the Personal Information Protection Commission No. 2024-006-169∼174 (public) ▪ In the case of AI learning of the data entered by the user, this fact is clearly announced and the information subject's practical choice is guaranteed - (example."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\n(omitted below)\n- 37 -\nIn order to sufficiently guarantee the rights of data subjects as discussed so far, it is necessary to clearly inform data subjects of whether their personal information is included in AI learning data and how personal information is processed by AI. Companies and institutions must provide AI systems with the facts of data set collection, main sources, processing purposes, etc. It is recommended to support information subjects in exercising their rights by transparently disclosing the personal information processing process in personal information processing policies, technical documents, FAQs, etc. In addition, if there are technical limitations in responding to requests for viewing, correction, or deletion, informing the information subject of such matters in advance is also a way to protect the information subject's rights. It can be helpful. AI agents, which are emerging recently, are AI systems that possess memory capabilities and perform tasks autonomously. There is increasing concern about excessive user information collection and profiling during the operation process. Therefore, companies and institutions that provide AI agent services must enter the prompt. It is recommended to clearly notify the user's conversation history, ▲ whether learning data is used, ▲ whether to provide it to a third party, ▲ storage and destruction policy, ▲ filtering standards, etc. In this case, it is desirable to provide information for a sufficient period of time before and after learning to ensure the information subject's practical right to choose (opt-out, etc.). Reference generation type Things to keep in mind when establishing an AI-related personal information processing policy※ 「Guidelines for preparing personal information processing policy」(April 2025)‣ For personal information processors who develop AI and provide services, it is recommended to set and write down the standards for collection and use of learning data in advance, taking into account minimum collection and clarification of purpose. Included‣ When changing the personal information processing policy, continuously disclose the change and implementation period and changed contents, and if there is a previous personal information processing policy, record the history of changes * If major changes are notified separately, it is recommended to notify the information subject in a way that can be easily confirmed through a web page pop-up window, notice, etc.‣ In the case of an automated decision, the standards, procedures, and the way personal information is processed are recorded so that the information subject can easily confirm\n- 38 -\nExamples of measures to strengthen the protection of rights of information subjects※ Decision of the Personal Information Protection Commission No. 2025-011-031 (private)※ Decision of the Personal Information Protection Commission No. 2024-006-169∼174 (public) ▪ In the case of AI learning of the data entered by the user, this fact is clearly announced and the information subject's practical choice is guaranteed - (example."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\nChatbot service) At the top of the chat room input, notify the fact that ‘conversation content is collected as learning data’ and ‘how to refuse data collection (opt-out, etc.)’ ▪ New users are initially notified for a sufficient period of time, users who do not opt-out are notified additionally more than a certain number of times, and then reconsidered regularly ▪ Learning data collection, rejection, and destruction policies are transparently disclosed ※ Personal information items included in learning data, purpose of collection and use (e.g. LLM learning), Personal information filtering policy, retention period, and destruction method (e.g., stopping data collection after the user sets opt-out), etc. ▪ Providing a function to easily remove and delete data entered by users at any time, and taking measures to minimize personal information infringement, including improving accessibility to the function ▪ Modifying and redistributing LLM for vulnerabilities related to personal information infringement, opening an inquiry window, and establishing procedures to guide companies using their LLM on vulnerability measures, etc. Examples of opt-out guarantees\nInformation is provided in the prompt input field to make it easy to understand the fact that learning is being done and to opt-out (initial notification within a sufficient period of time, followed by periodic reconsideration). An intuitive setting interface is set up so that opt-out can be easily performed.\n- 39 -\n5 Creating an AI Privacy Governance System As the data processing flow of AI becomes more complex, the importance of risk management is increasing. In response to these environmental changes, each company or institution needs to establish and operate an internal management system centered on a personal information protection officer (CPO) who oversees compliance with personal information-related laws and regulations and risk management."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\nChatbot service) At the top of the chat room input, notify the fact that ‘conversation content is collected as learning data’ and ‘how to refuse data collection (opt-out, etc.)’ ▪ New users are initially notified for a sufficient period of time, users who do not opt-out are notified additionally more than a certain number of times, and then reconsidered regularly ▪ Learning data collection, rejection, and destruction policies are transparently disclosed ※ Personal information items included in learning data, purpose of collection and use (e.g. LLM learning), Personal information filtering policy, retention period, and destruction method (e.g., stopping data collection after the user sets opt-out), etc. ▪ Providing a function to easily remove and delete data entered by users at any time, and taking measures to minimize personal information infringement, including improving accessibility to the function ▪ Modifying and redistributing LLM for vulnerabilities related to personal information infringement, opening an inquiry window, and establishing procedures to guide companies using their LLM on vulnerability measures, etc. Examples of opt-out guarantees\nInformation is provided in the prompt input field to make it easy to understand the fact that learning is being done and to opt-out (initial notification within a sufficient period of time, followed by periodic reconsideration). An intuitive setting interface is set up so that opt-out can be easily performed.\n- 39 -\n5 Creating an AI Privacy Governance System As the data processing flow of AI becomes more complex, the importance of risk management is increasing. In response to these environmental changes, each company or institution needs to establish and operate an internal management system centered on a personal information protection officer (CPO) who oversees compliance with personal information-related laws and regulations and risk management."}
{"meta": {"section_type": "guideline", "article_ref": "제31조", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical] [Article: Article 31]\n\n51) It is a type of ‘mock attack test’ that checks in advance what risks an AI system may cause, and is a tool/activity that identifies problems such as the creation of harmful content, biased answers, and security bypass (jailbreak) by intentionally testing the AI model with humans or other AI.\nReference: CPO system under the Personal Information Protection Act (Article 31 of the Act) The Personal Information Protection Act stipulates the CPO system to promote personal information protection activities and impose responsibilities on personal information processors, such as compliance with personal information-related laws and prevention of misuse and abuse - (Definition of CPO) A person who is overall responsible for work related to personal information processing - (Obligation to designate a CPO) Personal information processors, excluding small business owners, must designate a CPO.\nReference Examples of major AI companies' red teaming methodologies, etc. ▪ OpenAI: External expert red teaming and automated red teaming in parallel52)  Professional red teaming is carried out in the following steps: ① selecting key test areas and ② selecting target models, ③ providing test interfaces and documentation guidelines, and ④ post-evaluating and reviewing red teaming results.  Automated red teaming is efficient in creating and testing large-scale attack cases, and is Reflect and supplement important scenarios identified in the red teaming stage\n- 40 -\nIn addition, the CPO maintains a close cooperation system with the Chief Artificial Intelligence Officer (CAIO), Chief Information Security Officer (CISO), etc., and is responsible for processing personal information within the organization. They must be guaranteed the authority and role to actively participate in the development and utilization of generative AI. In particular, they must intervene from the early stages of AI planning and development to secure sufficient information on personal information processing and provide timely feedback to relevant departments so that the personal information by design (PbD) perspective can be internalized in generative AI services."}
{"meta": {"section_type": "guideline", "article_ref": "제31조", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit] [Article: Article 31]\n\n51) It is a type of ‘mock attack test’ that checks in advance what risks an AI system may cause, and is a tool/activity that identifies problems such as the creation of harmful content, biased answers, and security bypass (jailbreak) by intentionally testing the AI model with humans or other AI.\nReference: CPO system under the Personal Information Protection Act (Article 31 of the Act) The Personal Information Protection Act stipulates the CPO system to promote personal information protection activities and impose responsibilities on personal information processors, such as compliance with personal information-related laws and prevention of misuse and abuse - (Definition of CPO) A person who is overall responsible for work related to personal information processing - (Obligation to designate a CPO) Personal information processors, excluding small business owners, must designate a CPO.\nReference Examples of major AI companies' red teaming methodologies, etc. ▪ OpenAI: External expert red teaming and automated red teaming in parallel52)  Professional red teaming is carried out in the following steps: ① selecting key test areas and ② selecting target models, ③ providing test interfaces and documentation guidelines, and ④ post-evaluating and reviewing red teaming results.  Automated red teaming is efficient in creating and testing large-scale attack cases, and is Reflect and supplement important scenarios identified in the red teaming stage\n- 40 -\nIn addition, the CPO maintains a close cooperation system with the Chief Artificial Intelligence Officer (CAIO), Chief Information Security Officer (CISO), etc., and is responsible for processing personal information within the organization. They must be guaranteed the authority and role to actively participate in the development and utilization of generative AI. In particular, they must intervene from the early stages of AI planning and development to secure sufficient information on personal information processing and provide timely feedback to relevant departments so that the personal information by design (PbD) perspective can be internalized in generative AI services."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "medical treatment", "guideline_source": "PIPC"}, "text": "[Domain: Medical]\n\nSee “Blueprint for GenAI Red Teaming”\n▪Microsoft: Repeat in the order of ①preparation, ②test performance, and ③checking the results53) Preparation: Recruit experts in various fields to perform red teaming, select test subjects (target model, AI system interface, etc.) and select risk areas to check Test performance: Provide and support test procedures and access rights to red team personnel at all times, check the situation Result check: Test results Report regularly to major stakeholders (list of identified major issues, review of future red teaming plans and related materials, etc.) Examples of inspection items by model, development, system, and operation area during red teaming 54) ▪ Model: Inspection of inference attacks (parameters, learning data induction, etc.), extraction attacks (model policies, system prompts, etc.), model adjustments (jailbreak attacks, prompt injection, safety policy bypass, etc.), etc. ▪ Development: Data contamination (external vector type DB contamination, etc.) Search enhancement generation result manipulation, cache contamination, etc.), proxy/firewall policy bypass, content filter bypass (multilingual attack, context bypass attack, etc.), access control (session management, API access rights, RBAC policy, token management policy, etc.) inspection ▪ System: Inspection of remote code execution vulnerabilities (execution of resulting code, system command execution, etc.), supply chain vulnerability (dependency integrity, update system, distribution pipeline, etc.) Operation: Limiting individual request frequency, unauthorized data access, etc. Check for missing input/output filtering, security audit, and whether log record information is sufficient.\n- 41 -\nCase Privacy and Security Cooperation Internal Management System and AI Privacy Red Team Operation Case 55) ▪ Use systematic testing methodology  Utilize standard security and privacy diagnosis frameworks such as OWASP (OpenWorldwideApplicationSecurityProject)  Develop standardized attack scenarios and test cases ▪ Attacker scenario-based testing  Membership Inference Attack, Model Inversion Attack  Extract sensitive information Testing, prompt injection attacks, etc. ▪ Continuous testing  Testing is conducted in parallel from the beginning of development, and tests must be performed when new features are added  Establishing a regular (quarterly, semi-annual) testing schedule ▪ Documentation and reporting  Detailed documentation of discovered vulnerabilities, including severity and scope of impact assessment ▪ Monitoring the latest trends  Identifying new AI vulnerabilities and attack techniques  Monitoring changes in related laws and regulations ▪ Capacity building and training  Mock hacking training and Workshop holding, etc.\n55) ｢Llama Developer Use Guide: AI Protections｣ (Meta, '25."}
{"meta": {"section_type": "guideline", "article_ref": "", "domain": "credit", "guideline_source": "PIPC"}, "text": "[Domain: Credit]\n\nSee “Blueprint for GenAI Red Teaming”\n▪Microsoft: Repeat in the order of ①preparation, ②test performance, and ③checking the results53) Preparation: Recruit experts in various fields to perform red teaming, select test subjects (target model, AI system interface, etc.) and select risk areas to check Test performance: Provide and support test procedures and access rights to red team personnel at all times, check the situation Result check: Test results Report regularly to major stakeholders (list of identified major issues, review of future red teaming plans and related materials, etc.) Examples of inspection items by model, development, system, and operation area during red teaming 54) ▪ Model: Inspection of inference attacks (parameters, learning data induction, etc.), extraction attacks (model policies, system prompts, etc.), model adjustments (jailbreak attacks, prompt injection, safety policy bypass, etc.), etc. ▪ Development: Data contamination (external vector type DB contamination, etc.) Search enhancement generation result manipulation, cache contamination, etc.), proxy/firewall policy bypass, content filter bypass (multilingual attack, context bypass attack, etc.), access control (session management, API access rights, RBAC policy, token management policy, etc.) inspection ▪ System: Inspection of remote code execution vulnerabilities (execution of resulting code, system command execution, etc.), supply chain vulnerability (dependency integrity, update system, distribution pipeline, etc.) Operation: Limiting individual request frequency, unauthorized data access, etc. Check for missing input/output filtering, security audit, and whether log record information is sufficient.\n- 41 -\nCase Privacy and Security Cooperation Internal Management System and AI Privacy Red Team Operation Case 55) ▪ Use systematic testing methodology  Utilize standard security and privacy diagnosis frameworks such as OWASP (OpenWorldwideApplicationSecurityProject)  Develop standardized attack scenarios and test cases ▪ Attacker scenario-based testing  Membership Inference Attack, Model Inversion Attack  Extract sensitive information Testing, prompt injection attacks, etc. ▪ Continuous testing  Testing is conducted in parallel from the beginning of development, and tests must be performed when new features are added  Establishing a regular (quarterly, semi-annual) testing schedule ▪ Documentation and reporting  Detailed documentation of discovered vulnerabilities, including severity and scope of impact assessment ▪ Monitoring the latest trends  Identifying new AI vulnerabilities and attack techniques  Monitoring changes in related laws and regulations ▪ Capacity building and training  Mock hacking training and Workshop holding, etc.\n55) ｢Llama Developer Use Guide: AI Protections｣ (Meta, '25."}
